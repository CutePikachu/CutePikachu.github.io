{"pages":[{"title":"Projects","text":"My journey to the Computer Science World starts in July 2018. The followings are the projects I have at least made a fair amount of contribution. Some of them might not be open sourced, but they will shine and work as expected in the future. 2020.7 - Now awesome-unswLink: https://github.com/UNSWEEB/awesome-unswJust to collect class notes of UNSW computer science courses for students. Looking for more notes. We are welcome and seeking for contribution. 2020.7 - Now XYZSASStudent Afair System using Aliyun table storage and function compute. 2020.7 - Now AauthWeb link: https://aauth.link/Repo link: https://github.com/yzITI/Aauth“Auth with Anything: Fourth Party Login Service.” The goal is to make it is easier for other web developers to use third party login service. I have only contributed to front-end page yet. 2020.4 - Now AcewordsLink: https://acewords.topAn web app to help Chinese memorize foreign language words.This one is actually version2.0. I have made more than half of its backend(written in NodeJs) and about one third of its front-end(still VueJs).My journey to the backend world starts now. 2019.12 - 2020.2 YZSA-feStudent Affair system for Yangzhou Highschool of Jiangsu Province front-end.I have paticipated in this project which is currently using by Yangzhou Highschool of Jiangsu Province. This project is not open sourced and I won’t provide the link to the website because it requires students or staff account to login. 2019.9 - Now YZZX-techWebsite link: https://yzzx.techRepo link(deprecated): https://github.com/CutePikachu/yzzx-tech-deprecated-Repo link(in use): https://github.com/yzITI/yzzx-techThis is the web page for ITI(website language in Chinese).The deprecated version is not fully written by me, though I have made it suitable to view in phone. In the current version, I have written some pages(in about early 2020). 2019.7 Little littersLink: https://github.com/dsksi/cse-2019-hackathonThis was made during UNSW(university of New South Wales) cse hackathon, and our group got 3rd prize(3/40). The project itself was not complicated but it was impressive because we made it in 24 hours and only part of our team actually had some web programming experience. I wrote few components or functions in react and this was my first time coding using ReactJs. 2019.6 Building MazeLink: https://github.com/CutePikachu/BuildingMaze-FEIt is a maze game built using vue.This project was somewhat incomplete and can be inproved. However, I don’t want to change it and have decided to leave it like that.This is the project opening the door of front-end development for me. Guided by Phantomlsh.","link":"/projects/index.html"}],"posts":[{"title":"Mac装spin和ispin","text":"翻了很久也没找到Mac的安装教程(可能是太简单了)， 安装spin这个就很简单了，感谢brew, 1brew install spin 安装ispin去https://github.com/nimble-code/Spin下载，可以直接打包或者git clone。其实ispin只是一个文件，在 ‘optional_gui’这个folder里（不确定只有这个文件是不是就可以直接运行）。然后我们进入ispin文件所在的目录。 1cd Spin/optional_gui 然后在命令行输入 1wish -f ispin.tcl 就可以运行了。","link":"/2020/06/01/Mac%E8%A3%85spin%E5%92%8Cispin/"},{"title":"【双剑合璧】Git和Github使用教程","text":"是否有遇到过写着写着想回到之前版本，却又不记得具体实现；又或是想和队友共享代码，每次修改发送文件；抑或是想换个电脑写却不想用email等搬运全部文件这类烦不胜烦类似的问题，伟大的程序员们自然早就为我们造好了轮子，那就是版本控制的利器——Git以及cloud based Github。本教程也主要讲Git与Github的使用。 1. 背景介绍本来想粗暴写一下安装使用教程，想了想还是先写一点背景介绍，不感兴趣可以直接跳过。 版本控制首先我们需要了解一个概念——Version Control，也就是版本控制。当我们写代码的时候总会有意无意制造出一些bug，有时候我们会想返回前一次没有问题的时候，因此这就是我们为什么需要版本控制。 简单来说，版本控制就是在不同时间节点保存你的程序，然后你可以通过它回看甚至回到之前保存的版本。 什么是GitGit是在2005年的时候初次开发出来的版本控制利器，并风靡全球。Git是一个安装并管理本地系统的工具而且可以给你提供现有文件的你保存的不同版本。因为是本地的，下载之后就不再需要网络也可以使用。 什么是GithubGithub有一点像可视化的Git，并且是在线的服务。让你可以在线管理你的Git仓库（这个具体会在后面讲）。通过Github，你可以分享你的程序，让其他合作者一起进行编辑。它不仅保存了Git的全部功能，还进行了扩充，它可以让你在任意电脑任意地区访问，只要你有权限。它最大的优点就是它是一个很庞大的数据库，你可以搜索，阅读甚至使用别人写好的程序。当然Github还有很多替代物如Gitlab之类，本文就不赘述了。 Git vs. Github简单的说，Git是一个帮助你管理和追踪本地源码历史的版本控制工具，而Github则是一个基于云端运用Git技术的让你管理Git仓库的服务。 2. Git首先，我们来康康Git的使用。 注意：所有和Git相关的命令都是git开头噢。 安装如果已经安装过，可以跳过。 Linux 如果是Fedora或其他类似的 RPM-based distribution, 譬如RHEL 和 CentOS： 1$ sudo dnf install git-all 或Debian-based distribution像Ubuntu 1$ sudo apt install git-all macOS 可以先试 1$ git --version 如果没有安装，应该会弹出安装请求，也可以去https://git-scm.com/download/mac 下载。 Windows 这个稍微有点烦，见https://git-scm.com/book/en/v2/Getting-Started-Installing-Git 。 新建一个Git仓库安装好之后我们就可以开始用了，Git的一个仓库（Repo）就是你的一个项目。我们首先新建一个文件夹 12$ mkdir firstRepo$ cd firstRepo 初始化一个仓库 1$ git init 这时候会显示这么一行 Initialized empty Git repository in /Users/tina/Desktop/firstRepo/.git/, 后面一部分是你当前仓库的路径，会根据路径不同进行变化。通过.git我们可以知道Git其实是创建了一个隐藏文件夹，所以我们运行ls这个命令，并不会看到有关Git的信息。 添加文件我们依然先在当前目录建文件，文件类型无所谓。这里就用txt文件了。 123$ touch first.txt$ lsfirst.txt 此时我们可以看见文件里有一个文件叫first.txt，但是这个文件并不在我们Git仓库里（划重点），为了看仓库里有什么，我们使用 1234567891011$ git statusOn branch masterInitial commitUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) first.txtnothing added to commit but untracked files present (use \"git add\" to track) 这个命令我们之后具体讲，现在我们看到untracked files这里，这个的意思就是说这些个文件在当前目录下但是没有保存到我们仓库里，所以Git不会对它的改变进行追踪。把文件添加到仓库里，我们使用 1$ git add first.txt 此时first.txt已经被加进去了。如果我们想添加多个文件呢，可以把想加入的文件用空格隔开，写在后面，像这样 1$ git add file1 file2 不过我们还有一个更简单的方法，那就是. 1$ git add . 这样可以把当前目录下全部的没有track的文件都加进来（是不是很方便！） 删除文件提到添加就不得不说删除，如果一个文件不想被跟踪了怎么办呢，那就删掉啦。 1$ git rm file 注意：这个操作不会在当前目录删除文件 一个比较重要的flag --force 顾名思义, 这个flag是用来强制删除文件的。一般来说，如果你的文件没有被commit（下面就讲这个），是不可以被删除的，但是加上这个flag就可以删除了。 “截图”咳，实话实话，我也不知道commit怎么准确翻译成中文。现在我们来看Git非常非常重要的一个命令，commit。它其实就是类似一个截图，存下来你当前的项目完成情况并保存下来，以后可以回顾甚至回到当前节点。至于怎么回到我们后面再讲，现在就来进行“截图”。这个截图只是对于Git追踪的文件，所以它与add是不可分割的。 1234$ git commit -m \"Your message about the commit\"[master (root-commit) b345d9a] This is my first commit! 1 file changed, 1 insertion(+) create mode 100644 first.txt 这样就建了一个新的commit啦，看到这个b345d9a东西了么，这个是一个commit id。但是如果你对文件进行了修改，又运行了这个语句，你会看到这么一行Already Up to date. Bug？Nonono， 这就是需要我们add来参与了。因为在Git心里，你的文件还是上一次add来的，他一看，文件和上一次commit的没有变化呀，不截图不截图。所以我们要重新add一遍，这个时候的add就是一个更新的操作了，我们每次commit之前都要先add再commit。 不过，这两步某些情况下是可以合并的，变成 1$ git commit -am \"Your message about the commit\" 这样做就是更新我跟踪的所有文件并进行现有成果截图，但是如果你新建的文件还是要用单独的add来进行添加哦。 注意：这个语句千万不要瞎写哦，以后你可能会用到 分支虽然我一直刻意没有提到上面出现过几次的一个词master，它是什么呢，它是我们的主分支。想象一棵树，它就是我们的树干。分支的作用是当你想修改某个部分的代码，添加新功能，但却不想影响之前写好的代码，就可以分出一个branch，在上面进行。不同分支之间不会相互干扰，除非你进行合并等操作。 我们初始化一个Git仓库时，主分支就已经存在，在此基础上，我们可以新建自己的branch。使用 1234$ git branch branchName$ git branch* master branchName 第一个命令是新建，第二个是列出全部分支。*表示当前所在分支。切换分支我们使用 1234$ git checkout branchName$ git branch master* branchName 通常来说，我们新建一个分支就是为了切换到新分支，所以我们可以把上两步合二为一 1$ git checkout -b &lt;my branch name&gt; 注意：我们新建分支的内容是和你建分支时所在的一致，要注意新建分支时所在的branch哦，通常情况我们都是在master上加分支。 在进行写代码，修改之后，我们想把分支上内容合并至master，使用 12$ git checkout master$ git merge branchName 这里稍微有一些饶人，我们需要在master上进行merge操作来使得master与分支一致。现在我们来简单讲解下merge。 假设在master上我们有几次commit后，新建一个分支A，几次commit之后，我们想把A合并到master上。合并之前， common base—— — commit —— commit (master) ​ ｜— commit —— commit (A) 合并后 common base—— — commit —— commit (master) —— new merge commit ​ ｜— commit —— commit (A) ——｜ 这个时候master上就有了A上面的内容啦。 merge分为两种，一种是fast forward, 另外一种是3-way merge。 第一种很直接，合并前 common base—— (master) ​ ｜— commit —— commit (A) 合并后 common base—— —— new merge commit ​ ｜— commit —— commit (A) ——｜ 就不细讲Git原理，康图。 第二种其实就是第一个那个图。小朋友，你是否有很多问号，为什么就这样合并起来，而没有冲突。merge的过程很容易产生的问题就是冲突！尽管Git的merge已经很nb了，但依然不可避免产生冲突，但这些我们可以很容易地解决。 冲突这是Git里经常遇到并且要解决的问题！当你merge不同分支时，很容易遇到。为了让大家直观感受，我们创造一个冲突。 1234567891011121314151617$ git checkout master$ echo 'this is conflicted text from master' &gt; first.txt$ git commit -am 'added one line'[master 8cc7111] added one line 1 file changed, 1 insertion(+)$ git checkout branchNameSwitched to branch 'branchName'$ echo 'this is conflicted text from feature branch' &gt; first.txt$ git commit -am 'added one line'[a 23f5790] added one line 1 file changed, 1 insertion(+)$ git checkout masterSwitched to branch 'master'$ git merge branchNameAuto-merging first.txtCONFLICT (content): Merge conflict in first.txtAutomatic merge failed; fix conflicts and then commit the result. YES!!冲突出现了，现在我们要解决它。这个时候打开此文件 1$ vim first.txt vim是最好的text editor！ 我们会看到这个 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADthis is conflicted text from master=======this is conflicted text from feature branch&gt;&gt;&gt;&gt;&gt;&gt;&gt; branchName &lt;&lt;&lt;&lt;&lt;&lt;&lt; 和=======是告诉我们哪里有冲突，并且是哪个branch。就这个而言前第二行是master上内容而第四行则是branchName上的内容，我们选取需要内容后记得删除135行哦！ Vim使用i进行编辑，完成后esc然后打:q推出。这个时候冲突已经解决完啦，我们需要进行一次commit来结束这次的合并操作。 1$ git commit -am 'solve conflict' 其他git status其实算是看当前分支的一个状态吧，可以看到是否有未被追踪的文件，有没有变化没有被commit。使用 1$ git status git log简单粗暴，显示commit记录，使用 1$ git log 会显示出一串commit记录，每个包括id，作者，时间，分支，以及commit的时候的那个message。 切换到某次commit1$ git checkout specific-commit-id 这个commit id就需要我们去git log里面找来，嗯就是那一长串看不懂的hash。至于你具体想切换到哪个，就看你commit的时间以及你自己写的commit信息啦（滑稽）瞎写的话现在就作茧自缚了嘻嘻。 结语Git大致就讲这么多啦，想更了解具体的Git，阔以参考document。https://git-scm.com/doc 3. Github不得不说Github是真的好用，造好的轮子随便用，甚至还可以找到作业答案，当然要自己写作业了！，而且Github推出的Github desktop是真香。使用很简单，无师自通。言归正传，我们来康康Github怎么用。 注册及安装想注册的话你需要准备的东西有：一个或多个邮箱。一个账号可以绑定N个邮箱，然后去https://github.com/ 进行注册。如果是学生且想免费申请Github pro可以访问https://education.github.com/pack 。 申请完之后我们打开命令行 12$ git config --global user.name \"&lt;your_name_here&gt;\"$ git config --global user.email \"&lt;your_email@email.com&gt;\" 注意：写名字时去除括号但保留引号，邮箱使用申请时的邮箱。以及，如果只是想在当前仓库用的话，去掉global就ok了 现在我们连接ssh到Github，这样就可以通过ssh进行clone操作而不是http。先复制ssh key 1$ pbcopy &lt; ~/.ssh/id_rsa.pub 然后去Github网页，点击头像，出现下拉菜单，点击Settings，在用户设置的菜单栏选择SSH and GPG keys，点击New SSH key，在title栏写上你设备的名字，方便你辨认，把刚刚复制好的key复制到key那里。然后点击添加，如果出现输密码框就输入密码。 新建仓库先去Github上建立一个新仓库，点击Repositories旁New按钮，填写Repo的名字后就可以确认建立。新建完后自动跳转到这个初始页面，在这里我们可以看到这个仓库的https地址，点击旁边的复制按钮备用。 连接仓库现在，之前Git里的操作都可以正常用了，不过效果依然存在本地，想要和云端连接，我们需要学一些新操作。首先和remote连接 1$ git remote add origin remote repository URL 这里的URL就是上一步操作复制得到的。然后 1$ git push -u origin master 运行完之后，我们就可以在Github上看到啦。每当我们想要发布一个新的分支的时候，都需要运行这个命令，要将master改成分支的名字即可。 更新当remote和本地的版本不一致时，我们会需要进行更新，可能是本地快于remote也可能是remote快于本地。我们使用git pull来拉取remote的更新，使用前要记得先commit本地的版本哦。如果要更新remote版本，我们直接用git push就可以做到了。 克隆在Github上打开你要克隆的仓库主页，点击Clone or Download，然后可以选择用https还是ssh进行克隆，复制链接，打开命令行 1$ git clone url 如果不是你的仓库，只能使用https哦，或者也可以选择下载。 Github网页既然他是有网页版的，那自然不能忽略网页版提供的服务。 搜索我要说的搜索是在页面顶部Navigation bar里的那个搜索框，在哪里可以进行关键词搜索，能检索到有相关信息的公开仓库。在搜索结果的页面，我们可以选取特定语言（这个项目使用的语言），结果的排序方法等等。还有其他一些信息，就自己去看啦。 每个仓库都会显示stars，一般来说stars越多，认可度越高，也就越好。 关于Repo因为要素过多有很多内容，我就选取部分说明，其他的功能自己康康就好啦。 文件 点到自己的任意一个仓库，我们可以直接在网上编辑文件。点开文件A，然后点击Edit就可以进行修改，修改完成后使用下方的commit进行保存。也可以直接新建文件，上传文件，删除文件等。不过比较神奇的操作是新建文件夹。具体如下： 点击Create New File，在文件名的地方输入文件夹名称并加上/, 神奇的事情发生了，文件夹就建好了。不过不可以建空文件夹哦，所以它会强行让你写一个文件名。 合作 如果想和组员共同编辑一个仓库，我们点击settings，然后Manage Access，这个时候会让你输入一下密码。进去后可以修改当前仓库权限以及邀请合作者，提供输入合作人的Github名字来进行查找和添加。 其他怎么fetch一个云端的分支（不在本地的）？ 1$ git checkout --track origin/daves_branch 如果不想用命令行怎么办？ Github Desktop你值得拥有：https://desktop.github.com/ 4. 写在最后感谢大家看完我的废话教程，这大概是本人多年一年的使用经常用到的部分，希望可以给你们带来帮助。","link":"/2020/03/21/%E3%80%90%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7%E3%80%91Git%E5%92%8CGithub%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"},{"title":"Foundation of Concurrency","text":"Gain more insight about concurrency. Semantics#mermaid-1597736524786 .label{font-family:'trebuchet ms', verdana, arial;color:#333}#mermaid-1597736524786 .node rect,#mermaid-1597736524786 .node circle,#mermaid-1597736524786 .node ellipse,#mermaid-1597736524786 .node polygon{fill:#ECECFF;stroke:#9370db;stroke-width:1px}#mermaid-1597736524786 .node.clickable{cursor:pointer}#mermaid-1597736524786 .arrowheadPath{fill:#333}#mermaid-1597736524786 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1597736524786 .edgeLabel{background-color:#e8e8e8}#mermaid-1597736524786 .cluster rect{fill:#ffffde !important;stroke:#aa3 !important;stroke-width:1px !important}#mermaid-1597736524786 .cluster text{fill:#333}#mermaid-1597736524786 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:'trebuchet ms', verdana, arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1597736524786 .actor{stroke:#ccf;fill:#ECECFF}#mermaid-1597736524786 text.actor{fill:#000;stroke:none}#mermaid-1597736524786 .actor-line{stroke:grey}#mermaid-1597736524786 .messageLine0{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1597736524786 .messageLine1{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1597736524786 #arrowhead{fill:#333}#mermaid-1597736524786 #crosshead path{fill:#333 !important;stroke:#333 !important}#mermaid-1597736524786 .messageText{fill:#333;stroke:none}#mermaid-1597736524786 .labelBox{stroke:#ccf;fill:#ECECFF}#mermaid-1597736524786 .labelText{fill:#000;stroke:none}#mermaid-1597736524786 .loopText{fill:#000;stroke:none}#mermaid-1597736524786 .loopLine{stroke-width:2;stroke-dasharray:'2 2';stroke:#ccf}#mermaid-1597736524786 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1597736524786 .noteText{fill:black;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:14px}#mermaid-1597736524786 .activation0{fill:#f4f4f4;stroke:#666}#mermaid-1597736524786 .activation1{fill:#f4f4f4;stroke:#666}#mermaid-1597736524786 .activation2{fill:#f4f4f4;stroke:#666}#mermaid-1597736524786 .section{stroke:none;opacity:0.2}#mermaid-1597736524786 .section0{fill:rgba(102,102,255,0.49)}#mermaid-1597736524786 .section2{fill:#fff400}#mermaid-1597736524786 .section1,#mermaid-1597736524786 .section3{fill:#fff;opacity:0.2}#mermaid-1597736524786 .sectionTitle0{fill:#333}#mermaid-1597736524786 .sectionTitle1{fill:#333}#mermaid-1597736524786 .sectionTitle2{fill:#333}#mermaid-1597736524786 .sectionTitle3{fill:#333}#mermaid-1597736524786 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1597736524786 .grid .tick{stroke:#d3d3d3;opacity:0.3;shape-rendering:crispEdges}#mermaid-1597736524786 .grid path{stroke-width:0}#mermaid-1597736524786 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1597736524786 .task{stroke-width:2}#mermaid-1597736524786 .taskText{text-anchor:middle;font-size:11px}#mermaid-1597736524786 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1597736524786 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1597736524786 .taskText0,#mermaid-1597736524786 .taskText1,#mermaid-1597736524786 .taskText2,#mermaid-1597736524786 .taskText3{fill:#fff}#mermaid-1597736524786 .task0,#mermaid-1597736524786 .task1,#mermaid-1597736524786 .task2,#mermaid-1597736524786 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1597736524786 .taskTextOutside0,#mermaid-1597736524786 .taskTextOutside2{fill:#000}#mermaid-1597736524786 .taskTextOutside1,#mermaid-1597736524786 .taskTextOutside3{fill:#000}#mermaid-1597736524786 .active0,#mermaid-1597736524786 .active1,#mermaid-1597736524786 .active2,#mermaid-1597736524786 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1597736524786 .activeText0,#mermaid-1597736524786 .activeText1,#mermaid-1597736524786 .activeText2,#mermaid-1597736524786 .activeText3{fill:#000 !important}#mermaid-1597736524786 .done0,#mermaid-1597736524786 .done1,#mermaid-1597736524786 .done2,#mermaid-1597736524786 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1597736524786 .doneText0,#mermaid-1597736524786 .doneText1,#mermaid-1597736524786 .doneText2,#mermaid-1597736524786 .doneText3{fill:#000 !important}#mermaid-1597736524786 .crit0,#mermaid-1597736524786 .crit1,#mermaid-1597736524786 .crit2,#mermaid-1597736524786 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1597736524786 .activeCrit0,#mermaid-1597736524786 .activeCrit1,#mermaid-1597736524786 .activeCrit2,#mermaid-1597736524786 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1597736524786 .doneCrit0,#mermaid-1597736524786 .doneCrit1,#mermaid-1597736524786 .doneCrit2,#mermaid-1597736524786 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1597736524786 .doneCritText0,#mermaid-1597736524786 .doneCritText1,#mermaid-1597736524786 .doneCritText2,#mermaid-1597736524786 .doneCritText3{fill:#000 !important}#mermaid-1597736524786 .activeCritText0,#mermaid-1597736524786 .activeCritText1,#mermaid-1597736524786 .activeCritText2,#mermaid-1597736524786 .activeCritText3{fill:#000 !important}#mermaid-1597736524786 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1597736524786 g.classGroup text{fill:#9370db;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:10px}#mermaid-1597736524786 g.classGroup rect{fill:#ECECFF;stroke:#9370db}#mermaid-1597736524786 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1597736524786 .classLabel .box{stroke:none;stroke-width:0;fill:#ECECFF;opacity:0.5}#mermaid-1597736524786 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1597736524786 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1597736524786 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #compositionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #aggregationStart{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #aggregationEnd{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #dependencyStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #dependencyEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 #extensionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597736524786 .commit-id,#mermaid-1597736524786 .commit-msg,#mermaid-1597736524786 .branch-label{fill:lightgrey;color:lightgrey} #mermaid-1597736524786 { color: rgb(0, 0, 0); font: normal normal 400 normal 16px / normal \"Times New Roman\"; }PseudocodeCCSLTSKSbehavioursLTL Notes Notes made by Liam. To use as a guide for revision. Bridging the GapWe have seen several semantic models for concurrent systems in this course. Initially, when we introduced Linear Temporal Logic, we viewed concurrent processes as sets of behaviours, where behaviours were infinite sequences of states. This model is very nice for specifying the semantics of LTL, which we did in Week 1. Later on, though, we used different semantic models, such as Transition Diagrams and Ben-Ari’s pseudocode. We introduced the Calculus of Communicating Systems (CCS), a type of process algebra, to construct Labelled Transition Systems, analogous to our transition diagrams. This was intended to give us a formal language that is semantically connected to our diagrammatic notation. There is a missing link, however, between these two semantic notions: The world of CCS and labelled transition systems on one hand, and the world of behaviours and LTL properties on the other. We examined the two CCS processes $a ⋅ (b + c)$ and $a ⋅ b + a ⋅ c$ and concluded that LTL would not be able to distinguish these two processes, even though they are not considered equal (bisimilar) in CCS. If we were to try to add the equation $a⋅(P + Q) = a ⋅ P + a ⋅ Q$ into CCS, then we would identify those two processes above, but we would also identify $a ⋅ b + a$ with $a ⋅ b$, and clearly one of these satisfies $◊b$ and the other doesn’t! This is because the semantic equivalence we get by adding that equation is called partial trace equivalence. As mentioned in lectures, partial trace equivalence is like looking at all the possible sequences of actions (traces) in the process, including those sequences that do not take available transitions. So, the traces of $a⋅b+a$ are $ϵ$ (the empty trace), $a$, and $ab$ – exactly the same as the partial traces of $a⋅b$. Because partial trace equivalence includes these incomplete traces, using it is basically giving up on the vital progress assumption. Progress is the assumption that a process will take an action if one is available. To get the set of behaviours we expect for our LTL semantics, we need some way to choose which traces we want to keep and which ones we don’t. This is the definition of what counts as a completed trace. There are many different completeness criteria. Progress is the weakest, and the most common. Other completeness criteria include weak and strong fairness, which we have seen already, and justness which I will describe below. ProgressA simple way to view progress as a completeness criteria is that it says a path is complete iff it is infinite or if it ends in a state with no outgoing transitions. This rules out all paths which end in states where they could take an outgoing transition but don’t. Weak FairnessWeak and strong fairness are defined in terms of tasks. A task is a set of transitions. Weak fairness as a property of paths can be expressed by the LTL formulas$$◻(◻(enabled(t))⇒◊(taken(t)))$$for each task tt. Here enabled(t)enabled(t) is an atomic proposition that is true if a transition in tt is enabled. Likewise taken(t)taken(t) holds if tt is taken in that state. Given a process $P=a⋅P+b$, weak fairness would state that eventually $b$ must occur, as we would not be able to loop infinitely around $a$, never taking the (forever enabled) $b$ transition. Viewed as a completeness criterion, weak fairness rules out all traces which do not obey the property above. Note that it implies progress, because any trace that will eventually take a forever enabled transition will surely not be able to sit in a state without taking an available transition. Strong FairnessStrong fairness is similar to weak fairness, except that the property has one extra operator:$$◻(◻◊(enabled(t))⇒◊(taken(t))))$$This is saying that our task tt does not have to be forever enabled, just enabled infinitely often. This means a process can go away and come back. For example, the process $P=a⋅a⋅P+b$ would not eventually do $b$ under weak fairness, as after one $a$ the $b$ transition is no longer enabled. However because the infinite aa path has $b$ available infinitely often, strong fairness would require that $b$ is eventually taken. Viewed as a completeness criterion, this rules out all properties that don’t obey the property above. It can be proven in LTL that strong fairness implies weak fairness. JustnessJustness is a slightly harder concept to formalise. It is weaker than both strong and weak fairness, but stronger than progress. Essentially, it is a criterion that says that individual components should be able to make progress on their own. For example, imagine a system that consists of three components, one which always does aa, another which always does bb, and another which synchronises with aa: $A=a⋅A$ $B=b⋅B$ $C=\\bar a⋅A$ System$=A|B|C$ Now, a valid trace of this system would be simply $bbbbb⋯$ forever. This would satisfy progress, as a transition is always being taken. However, the actions $a$ and $\\bar a$ that could occur, and the communication $τ$ transition that could between $A$ and $C$ are never taken in this trace. Justness says that each component of the system will always make progress if their resources are available. In other words, $B$ doing unrelated $b$ moves should never prevent $A$ and $C$ from communicating. Viewed as a completeness criterion, this is stronger than progress, as it is essentially the same as progress except applied locally, rather than globally. It is weaker than weak fairness, as justness would not require that $A=a⋅A+b$ eventually does a $b$, as all transitions are from the same component ($A$). Kripke StructuresThese traces that we have examined are still sequences of actions, not states. Behaviours, however, are sequences of states. Normally, to convert our labelled transition systems into something we can reason about in LTL, we first translate them into an automata called a Kripke Structure. A Kripke structure is a 4-tuple $(S,I,↦,L)$ which contains a set of states $S$, an initial state $I$, a transition relation $↦$ which, unlike a labelled transition system, does not have any action labels on the transitions, and a labelling function $L$ which associates to every state $S$ a (set of) atomic propositions – these are the atomic propositions we use in our LTL formulae. A Kripke structure for an OS process behaviour was actually shown in the lecture on temporal logic in Week 1, I just never told you it was a Kripke structure. Kripke Structures deal with states, not transition actions. This means, to translate from a labelled transition system to a Kripke structure, we need a way to move labels from transitions to states. The simplest translation is due to de Nicola and Vaandrager, where each transition $s_i\\xrightarrow a s_j$ in the LTS is split into two in the Kripke Structure: $s_i→X$ and $X→s_j$, where $X$ is a new state that is labelled with $a$. Because this converts existing LTS locations into blank, unlabelled states in the Kripke structure, this introduces problems with the next state operator in LTL. For this reason (and others) we usually consider LTL without the next state operator in this field. Normally with LTL, we require that Kripke structures have no deadlock states, that is, states with no outgoing transitions. The usual solution here is to add a self loop to all terminal states. We can extract our normal notion of a behaviour by using the progress completeness criterion. Because of the restriction above, the progress criterion is equivalent to examining only the infinite runs of the automata. Putting it all togetherNow we can: Translate Pseudocode to CCS or transition diagrams Translate transition systems to Kripke structures Extract behaviours from Kripke structures using various completeness criteria Specify LTL properties about those behaviours Now we have connected all the semantic models used in the course. PseudocodeCalculus of Communicating Systems(CCS)The Calculus of Communicating Systems: Is a process algebra, a simple formal language to describe concurrent systems. Is given semantics in terms of labelled transition systems. Was developed by Turing-award winner Robin Milner in the 1980s Has an abstract view of synchronization that applies well to message passing. Processes Processes in CCS are defined by equations: The equation:$$\\textbf{CLOCK} = \\text{tick}$$defines a process CLOCK that simply executes the action “tick” and then terminates. This process corresponds to the first location in this labelled transition system (LTS):$$\\bullet\\xrightarrow{\\text{tick}}\\bullet$$An LTS is like a transition diagram, save that our transitions are just abstract actions and we have no initial or final location. Action PrefixingIf a is an action and $P$ is a process, then $x.P$ is a process that executes $x$ before $P$. This brackets to the right, so:$$x.y.z.P = x.(y.(z.P))$$Example$$\\textbf{CLOCK}_2 = \\text{tick.tock}$$defines a process called CLOCK$_2$ that executes the action “tick” then the action “tock” and then terminates$$\\bullet\\xrightarrow{\\text{tick}}\\bullet\\xrightarrow{\\text{tock}}\\bullet$$The process:$$\\textbf{CLOCK}_3 = \\text{tock.tick}$$has the same actions as CLOCK$_2$ but arranges them in another order. StoppingMore precisely, we should write:$$\\textbf{CLOCK}_2 = \\text{tick.tock.}\\textbf{STOP}$$where STOP is the trivial process with no transitions. LoopsUp to now, all processes make a finite number of transitions and then terminate. Processes that can make a infinite number of transitions can be pictured by allowing loops: CLOCK$_4 =$ tick.CLOCK$_4$ CLOCK$5_ =$ tick.tick,CLOCK$_5$ We accomplish loops in CCS using recursion. Equality of ProcessesThese two processes(CLOCK$_4$ and CLOCK$_5$) are physically different: But they both have the same behaviour — an infinite sequence of “tick” transitions. (Informal definition) We consider two process to be equal if an external observer cannot distinguish them by their actions. We will refine this definition later. Choice If $P$ and $Q$ are processes then $P + Q$ is a process which can either behave as the process $P$ or the process $Q$. Choice Equalities$$\\begin{align*}&amp;P + (Q + R) &amp;= \\space&amp;(P + Q) + R &amp;(\\text{associativity})\\\\&amp;P + Q &amp;= \\space&amp;Q + P &amp;(\\text{commutativity})\\\\&amp;P + STOP &amp;= \\space&amp;P &amp;(\\text{neutral element})\\\\&amp;P + P &amp;= \\space&amp;P &amp;(\\text{idempotence})\\end{align*}$$ What about the equation:$$a.(P + Q)=^?(a.P) + (a.Q)$$ Branching TimeExmaple: $\\textbf{VM}_1 = \\text{in}50¢.(\\text{outCoke + outPepsi})$ $ \\textbf{VM}_2 = (\\text{in}50¢.\\text{outCoke}) + (\\text{in}50¢.\\text{outPepsi})$ Reactive Systems VM$_1$ allows the customer to choose which drink to vend after inserting 50¢. In VM$_2$ however, the machine makes the choice when the customer inserts a coin. They different in this reactive view, but they have the same behaviours! EquivalencesThe equation$$a.(P + Q) = (a.P) + (a.Q)$$is usually not admitted for this reason. If we do admit it, then our notion of equality is very coarse (it is called partial trace equivalence). This is enough if we want to prove safety properties, but progress is not guaranteed. Our notion of equality without this equation is called (strong) bisimulation equivalence or (strong) bisimilarity. Parallel Composition If $P$ and $Q$ are processes then $P | Q$ is the parallel composition of their processes — i.e. the non-deterministic interleaving of their actions $$\\textbf{ACLOCK}=\\text{tick.beep | tock}$$ SynchronizationIn CCS, every action a has an opposing coaction $\\bar a$ (and $\\bar{\\bar a}$ = a): It is a convention to think of an action as an output event and a coaction as an input event. If a system can execute both an action and its coaction, it may execute them both simultaneously by taking an internal transition marked by the special action $τ$ . Expansion TheoremLet $P$ and $Q$ be processes. By expanding recursive definitions and using our existing equations for choice we can express $P$ and $Q$ as n-ary choices of action prefixes:$$P =\\sum_{i∈I}\\alpha_i. P_i \\text{ and } Q =\\sum_{j∈J}β_j. Q_j$$Then, the parallel composition can be expressed as follows:$$P | Q =\\sum_{i∈I}αi.(P_i| Q) +\\sum_{j∈J}β_j.(P | Q_j) + \\sum_{i∈I, j∈J, α_i=\\barβ_j}τ.(P_i| Q_j)$$From this, many useful equations are derivable:$$\\begin{align}&amp;P|Q &amp;= \\space&amp;Q|P\\\\&amp;P|(Q|R) &amp;=\\space &amp;(P|Q)|R\\\\&amp;P|\\text{STOP} &amp;=\\space &amp;P\\\\\\end{align}$$ Restriction If $P$ is a process and a is an action (not $τ$ ), then $P \\ a$ is the same as the process $P$ except that the actions a and a may not be executed. We have $(a.P)$ \\ $ b = a.(P $\\ $ b) \\text{ if } a \\not\\in {b, \\bar b}$ Example:$$\\begin{align}&amp;\\textbf{CLOCK}_4 &amp;= \\space&amp;\\text{tick}\\textbf{.CLOCK}_4\\\\&amp;\\textbf{MAN} &amp;=\\space &amp;\\bar {\\text{tick}}\\text{.eat.}\\textbf{MAN} \\\\&amp;\\textbf{EXAMPLE} &amp;=\\space &amp;(\\textbf{MAN|CLOCK}_4)\\text{\\ tick}\\\\\\end{align}$$ SemanticsUp until now, our semantics were given informally in terms of pictures. Now we will formalise our semantic intuitions. Our set of locations in our labelled transition system will be the set of all CCS processes. Locations can now be labelled with what process they are: We will now define what transitions exist in our LTS by means of a set of inference rules. This technique is called operational semantics. Inference RulesIn logic we often write:$$\\frac{A_1, A_2, … A_n}{C}$$To indicate that C can be proved by proving all assumptions A1 through An. For example, the classical logical rule of modus ponens is written as follows:$$\\frac{A\\Rightarrow B\\qquad A}{B}\\text{Modus Ponens}$$ Operational Semantics$$\\frac{}{a.P\\xrightarrow{a} P}\\text{ACT}\\qquad\\frac{P\\xrightarrow{a} P’}{P+Q\\xrightarrow{a} P’}\\text{CHOICE}_1\\qquad\\frac{Q\\xrightarrow{a} Q’}{P+Q\\xrightarrow{a} Q’}\\text{CHOICE}_2$$ $$\\frac{P\\xrightarrow{a} P’}{P|Q\\xrightarrow{a} P’|Q}\\text{PAR}_1\\qquad\\frac{Q\\xrightarrow{a} Q’}{P|Q\\xrightarrow{a} P|Q’}\\text{PAR}_2\\qquad\\frac{P\\xrightarrow{a} P’\\quad Q\\xrightarrow{a} Q’}{P|Q\\xrightarrow{τ} P’|Q’}\\text{SYNC}$$ $$\\frac{P\\xrightarrow{a} P’\\quad a\\not\\in{b,\\bar b}}{P\\text{ \\ }b\\xrightarrow{a} P’\\text{ \\ }b}\\text{RESTRICT}$$ Bisimulation Equivalence Two processes (or locations) P and Q are bisimilar iff they can do the same actions and those actions themselves lead to bisimilar processes. All of our previous equalities can be proven by induction on the semantics here. Proof TreesThe advantages of this rule presentation is that they can be “stacked” to give a neat tree like derivation of proofs. Value PassingWe introduce synchronous channels into CCS by allowing actions and coactions to take parameters.$$\\begin{align}&amp;\\text{Actions:}&amp;a(3)\\qquad &amp;c(15) &amp;x(True)…\\&amp;\\text{Coactions:}&amp;\\bar a(x)\\qquad &amp;\\bar c(y) &amp;\\bar c(z)…\\end{align}$$The parameter of an action is the value to be sent, and the parameter of a coaction is the variable in which the received value is stored. Example A one-cell sized buffer is implemented as:$$\\textbf{BUFF}=\\bar{\\text{in}}(x).\\text{out}(x).\\textbf{BUFF}$$Larger buffers can be made by stitching multiple BUFF processes together! This is how we model asynchronous communication in CCS. Merge and GuardsGuard: If $P$ is a value-passing CCS process and $ϕ$ is a formula about the variables in scope, then $[ϕ]P$ is a process that executes just like $P$ if $ϕ$ is holds for the current state and like STOP otherwise We can define an if statement like so:$$\\textbf{if } ϕ \\textbf{ then } P \\textbf{ else } Q≡ ([ϕ].P) + ([¬ϕ].Q)$$ Assignment If $P$ is a process and $x$ is a variable in the state, and $e$ is an expression, then $[\\![x := e]\\!]$ $P$ is is the same as $P$ except that it first updates the variable $x$ to have the value $e$ before making a transition. Some presentations of value passing CCS also include assignment to update variables in the state. With this, our value-passing CCS is now just as expressive as Ben-Ari’s pseudocode. Moreover, the connection between CCS and transition diagrams is formalised, enabling us to reason symbolically about processes rather than semantically. Process AlgebraThis is an example of a process algebra. There are many such algebras and they have been very influential on the design of concurrent programming languages. Kripke Structures(KS)Linear Temporal Logic(LTL)Logic A logic is a formal language designed to express logical reasoning. Like any formal languages, logics have a syntax and semantics(meaning of the value). Example: Proposition Logic Syntax A set of atomic propositions $P = {a, b, c, …}$ An inductively defined set of formulae: Each $p \\in P$ is a formula If $P$ and $Q$ are formulae, then $P \\land Q$ is a formula If $P$ is a formula, then $\\neg P$ is a formula (Other connectives(like or) are just sugar for these, so we omit them) Sematics Semantics are a mathematical representation of the meaning of a piece of syntax. There are many ways of giving a logic semantics, but we will use models. Example (Propositional Logic Semantics) A model for propositional logic is a valuation $V ⊆ P$, a set of “true” atomic propositions. We can extend a valuation over an entire formula, giving us a satisfaction relation:$$\\begin{align} V ⊨ p &amp;⇔ p \\in V\\\\ V ⊨ \\varphi \\land \\psi &amp;⇔ V ⊨ \\varphi\\text{ and }V ⊨ \\psi\\\\ V ⊨ \\neg \\varphi &amp;⇔ V |\\ne \\varphi\\end{align}$$We read $V ⊨ φ$ as $V$ “satisfies” $φ$. LTL Linear temporal logic (LTL) is a logic designed to describe linear time properties. Linear temporal logic syntax We have normal propositional operators: $p ∈ P$ is an LTL formula. If $ϕ$, $ψ$ are LTL formulae, then $ϕ ∧ ψ$ is an LTL formula. If $ϕ$ is an LTL formula, $¬ϕ$ is an LTL formula. We also have modal or temporal operators: If $ϕ$ is an LTL formula, then $\\circ ϕ$ is an LTL formula. The circle is read as ‘next’ If $ϕ$, $ψ$ are LTL formulae, then $ϕ U ψ$ is an LTL formula. The U is read as until. LTL Semanticslet $\\sigma = \\sigma_0\\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5…$ be a behavior. Then define the notation: $\\sigma|_0 = \\sigma$ $\\sigma|_1 = \\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5$ $\\sigma|_{n+1} = (\\sigma|_1)|_n$ Semantics The models of LTL are behaviours. For atomic propositions, we just look at the first state. We often identify states with the set of atomic propositions they satisfy.$$\\begin{align}&amp;\\sigma \\vDash p &amp;\\Leftrightarrow\\space &amp;p\\in \\sigma_0\\\\&amp;\\sigma \\vDash \\varphi\\land\\psi &amp;\\Leftrightarrow \\space&amp;\\sigma \\vDash \\varphi\\space \\text{and} \\space \\sigma\\vDash\\psi\\\\&amp;\\sigma \\vDash \\neg\\varphi &amp;\\Leftrightarrow \\space&amp;\\sigma \\not\\vDash \\varphi\\\\&amp;\\sigma \\vDash \\bigcirc\\varphi &amp;\\Leftrightarrow\\space &amp;\\sigma_1 \\vDash \\varphi\\\\&amp;\\sigma \\vDash \\varphi U\\psi &amp;\\Leftrightarrow\\space &amp;\\text{There exists an $i$ such that} \\text{$\\sigma_i\\vDash\\varphi$ and for all $j&lt; i$, $\\sigma|_j\\vDash\\varphi$}\\end{align}$$We say $P\\vDash\\varphi$ iff $\\forall\\sigma\\in[\\![P]\\!]$. $\\sigma\\vDash\\varphi$. Derived Operators The operator $\\Diamond\\varphi$(“finally” or “eventually”) says that $\\varphi$ will be true at some point. The operator$\\square\\varphi$(“global” or “always”) says that $\\varphi$ is always true from now on. Fairness The fairness assumption means that if a process can always make a move, it will eventually be scheduled to make that move. Expressing Fairness in LTLWeak fairness for action $π$ is then expressible as:$$\\square(\\square \\text{enabled}(π) ⇒ \\Diamond\\text{taken}(π))$$Strong fairness for action π is then expressible as:$$\\square(\\square\\Diamond\\text{enabled}(π) ⇒ \\Diamond\\text{taken}(π))$$ Critical SectionsDesiderata We want to ensure two main properties and two secondary ones: Mutual Exclusion No two processes are in their critical section at the same time. Eventual Entry (or starvation-freedom) Once it enters its pre-protocol, a process will eventually be able to execute its critical section. Absence of Deadlock The system will never reach a state where no actions can be taken from any process. Absence of Unneccessary Delay If only one process is attempting to enter its critical section, it is not prevented from doing so. Eventual Entry is liveness, the rest are safety. Dekker’s algorithm Dekker’s algorithm works well except if the scheduler pathologically tries to run the loop at q3 · · · q7 when turn = 2 over and over rather than run the process p (or vice versa). With fairness assumption, Dekker’s algorithm is correct. Tie-Breaker (Peterson’s) Algorithmfor 2 Processes for n Processes Properties of the Tie-Breaker AlgorithmDo we satisfy: Eventual entry? Yes Linear waiting? No Linear Waiting Linear waiting is the property that says the number of times a process is “overtaken” (bypassed) in the preprotocol is bounded by $n$ (the number of processes). Bakery Algorithm Mutual ExclusionThe following are invariants$$\\begin{align}np = 0&amp;\\equiv p1..2\\\\nq = 0&amp;\\equiv q1..2\\\\p4 &amp;\\Rightarrow nq= 0\\lor np\\le nq\\\\q4 &amp;\\Rightarrow np= 0\\lor nq\\lt nq\\end{align}$$and hence also $¬(p4 ∧ q4)$. Other Safety PropertiesDeadlock freedom: the disjunction $nq = 0 \\lor np\\le nq \\lor np=0\\lor np\\lt np$ of the conditions on the await statements at $p3/q3$ is equivalent to $T$. Hence it is not possible for both processes to be blocked there. Absence of unnecessary delay: Even if one process prefers to stay in its non-critical section, no deadlock will occur by the first two invariants(1) and (2) Eventual EntryFor $p$ to fail to reach its CS despite wanting to, it needs to be stuck at p3, where it will evaluate the condition infinitely often by weak fairness. To remain stuck, each of these evaluations must yield false. in LTL:$$\\square\\Diamond\\neg(nq=0\\lor np\\le nq)$$Which implies$$(5)\\space \\square\\Diamond nq\\ne0, \\text{and}\\(6)\\qquad \\square \\Diamond nq\\lt np$$Because there is no deadlock, (5) implies that process $q$ goes through infinitely many iterations of the main loop without getting lost in the non-critical section. But thrn it must set $nq$ to the constant $np+1$. From then onwards it is no longer possible to fail the test ($nq=0\\lor np\\le nq$), contradiction. N processes once again relying on atomicity of non-LCR lines of Ben-Ari pseudo-code; \u001c $&lt;\\!&lt;$breaks ties using PIDs. An Implementable Algorithm Properties of Lamport’s bakery algorithmCons: $O(n)$ pre-protocol; unbounded ticket numbers Assertion 1: If $p_k 1..2 ∧ p_i5..9$ and $k$ then reaches $p_5..9$ while $i$ is still there, then number[i] &lt; number[k]. Assertion 2: p_i8..9 ∧ p_k 5..9 ∧ i $\\ne$ k ⇒ (number[i], i) \u001c $&lt;\\!&lt;$ (number[k], k) Fast Algorithm sacrifice eventual entry Szymanski’s Algorithm enforces linear wait requires at most $4p − \\lceil{\\frac{p}{n}}\\rceil$ writes for p CS entries by n competing processes can be made immune to process failures and restarts as well as read errors occurring during writes Phases of the pre-protocol announce intention to enter CS enter waiting room through door 1; wait there for other processes last to enter the waiting room closes door 1 n the order of PIDs, leave waiting room through door 2 to enter CS Shared variablesEach process $i$ exclusively writes a variable called flag, which is read by all the other processes. It assumes one of five values: $0$ denoting that i is in its non-CS, $1$ declares i’s intention to enter the CS $2$ shows that i waits for other processes to enter the waiting room $3$ denotes that i has just entered the waiting room $4$ indicates that i left the waiting room Hardware-Assisted Critical SectionMachine InstructionsExchange test and set LocksThe variable common is called a lock (or mutex). A lock is the most common means of concurrency control in a programming language implementation. Typically it is abstracted into an abstract data type, with two operations: Taking the lock – the first exchange (step $p_2$/ $q_2$) Releasing the lock – the second exchange (step $p_5$ / $q_5$) Architectural ProblemsIn a mulitprocessor execution environment, reads and writes to variables initially only read from/write to cache. Writes to shared variables must eventually trigger a write-back to main memory over the bus. These writes cause the shared variable to be cache invalidated. Each processor must now consult main memory when reading in order to get an up-to-date value. The problem: Bus traffic is limited by hardware. With these instructions… The processes spin while waiting, writing to shared variables on each spin. This quickly causes the bus to become jammed, and can delay processes from releasing the lock and violating eventual entry.","link":"/2020/08/09/FoundationofConcurreny/"},{"title":"Operating Systems","text":"DEFINING LIVENESS Bowen ALPERN and Fred B. SCHNEIDERThis paper provides a formal definition of liveness property and has shown that no less restrictive definition would be correct. It also proves some theorems based on the definition. After the formalization of safety property, this paper provides some insight on liveness properties and shows that all properties are the intersection of safety and liveness properties. This research is important because it converts the definition of liveness property from “someting good will happen” to an expression which helps understand the meaning and characteristics of liveness properties. As mentioned in the conclusion, ‘good things’ and ‘bad things’ are not well-defined concepts, the topological definition given in the paper indeed gives correct intuition about liveness properties. Though it might be unnecessary, when temporal logic been mentioned the first time, it should be given some explanations. In general, this paper clearly states the definition the liveness properties and explains in an understandble way.","link":"/2020/08/11/paperReview/"},{"title":"Haskell and functional programming","text":"Enjoy learning haskell but not only haskell. Haskell IntroductionIn this course we use Haskell, because it is the most widespread language with good support for mathematically structured programming. 12f :: Int -&gt; Boolf x = (x &gt; 0) First $x$ is the input and the RHS of the equation is the Output. CurryingIn mathematics, we treat $log_{10}(x)$ and $log_2 (x)$ and ln(x) as separate functions. In Haskell, we have a single function logBase that, given a number n, produces a function for $log_n(x)$ 12345678910log10 :: Double -&gt; Doublelog10 = logBase 10log2 :: Double -&gt; Doublelog2 = logBase 2ln :: Double -&gt; Doubleln = logBase 2.71828logBase :: Double -&gt; Double -&gt; Double Function application associates to the left in Haskell, so: logBase 2 64 ≡ (logBase 2) 64 Functions of more than one argument are usually written this way in Haskell, but it is possible to use tuples instead… TuplesTuples are another way to take multiple inputs or produce multiple outputs: 1234toCartesian :: (Double, Double) -&gt; (Double, Double)toCartesian (r, theta) = (x, y) where x = r * cos theta y = r * sin theta N.B: The order of bindings doesn’t matter. Haskell functions have no side effects, they just return a result. There’s no notion of time(no notion of sth happen before sth else). Higher Order FunctionsIn addition to returning functions, functions can take other functions as arguments: 12345678twice :: (a -&gt; a) -&gt; (a -&gt; a)twice f a = f (f a)double :: Int -&gt; Intdouble x = x * 2quadruple :: Int -&gt; Intquadruple = twice double Haskell concrete types are written in upper case like Int, Bool, and in lower case if they stand for any type. 12345678{- twice twice double 3 == (twice twice double) 3 == (twice (twice double)) 3 == (twice quadruple) 3 == quadrauple (quadruple 3) == 48-} ListsHaskell makes extensive use of lists, constructed using square brackets. Each list element must be of the same type. 1234[True, False, True] :: [Bool][3, 2, 5+1] :: [Int][sin, cos] :: [Double -&gt; Double][ (3,’a’),(4,’b’) ] :: [(Int, Char)] MapA useful function is map, which, given a function, applies it to each element of a list: 123map not [True, False, True] = [False, True, False]map negate [3, -2, 4] = [-3, 2, -4]map (\\x -&gt; x + 1) [1, 2, 3] = [2, 3, 4] The last example here uses a lambda expression to define a one-use function without giving it a name. What’s the type of map? 1map :: (a -&gt; b) -&gt; [a] -&gt; [b] StringsThe type String in Haskell is just a list of characters: 1type String = [Char] This is a type synonym, like a typedef in C. Thus: &quot;hi!&quot; == ['h', 'i', '!'] Practice Word Frequencies Given a number $n$ and a string $s$, generate a report (in String form) that lists the $n$ most common words in the string $s$. We must: Break the input string into words. Convert the words to lowercase. Sort the words. Count adjacent runs of the same word. Sort by size of the run. Take the first $n$ runs in the sorted list. Generate a report. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import Data.Char(toLower)import Data.List(group,sort,sortBy)breakIntoWords :: String -&gt; [String]breakIntoWords = wordsconvertIntoLowercase :: [[Char]] -&gt; [String]convertIntoLowercase = map (map toLower)sortWords :: [String] -&gt; [String]sortWords = sorttype Run = (Int, String)countAdjacentRuns :: [String] -&gt; [Run]countAdjacentRuns = convertToRuns . groupAdjacentRuns -- [\"hello\",\"hello\",\"world\"] --&gt; [[\"hello\",\"hello\"],[\"world\"]]groupAdjacentRuns :: [String] -&gt; [[String]]groupAdjacentRuns = group-- head :: [a] -&gt; aconvertToRuns :: [[String]] -&gt; [Run]convertToRuns = map (\\ls-&gt; (length ls, head ls))sortByRunSize :: [Run] -&gt; [Run]sortByRunSize = sortBy (\\(l1, w1) (l2, w2) -&gt; compare l2 l1)takeFirst :: Int -&gt; [Run] -&gt; [Run]takeFirst = take generateReport :: [Run] -&gt; StringgenerateReport = unlines . map (\\(l,w) -&gt; w ++ \":\" ++ show l ) -- (\\x -&gt; f x) == fmostCommonWords :: Int -&gt; (String -&gt; String)mostCommonWords n = generateReport . takeFirst n . sortByRunSize . countAdjacentRuns . sortWords . convertIntoLowercase . breakIntoWords Functional CompositionWe used function composition to combine our functions together. The mathematical $(f ◦ g)(x)$ is written $(f . g) x$ in Haskell. In Haskell, operators like function composition are themselves functions. You can define your own! 1234-- Vector addition(.+) :: (Int, Int) -&gt; (Int, Int) -&gt; (Int, Int)(x1, y1) .+ (x2, y2) = (x1 + x2, y1 + y2)(2,3) .+ (1,1) == (3,4) You could even have defined function composition yourself if it didn’t already exist: 12(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)(f . g) x = f (g x) ListsHow were all of those list functions we just used implemented? Lists are singly-linked lists in Haskell. The empty list is written as [] and a list node is written as x : xs. The value x is called the head and the rest of the list xs is called the tail. Thus: 12\"hi!\" == ['h', 'i', '!'] == 'h':('i':('!':[])) == 'h' : 'i' : '!' : [] When we define recursive functions on lists, we use the last form for pattern matching: 123map :: (a -&gt; b) -&gt; [a] -&gt; [b]map f [] = []map f (x:xs) = f x : map f xs We can evaluate programs equationally: 12345678910111213{-map toUpper \"hi!\" ≡ map toUpper (’h’:\"i!\") ≡ toUpper ’h’ : map toUpper \"i!\" ≡ ’H’ : map toUpper \"i!\" ≡ ’H’ : map toUpper (’i’:\"!\") ≡ ’H’ : toUpper ’i’ : map toUpper \"!\" ≡ ’H’ : ’I’ : map toUpper \"!\" ≡ ’H’ : ’I’ : map toUpper (’!’:\"\") ≡ ’H’ : ’I’ : ’!’ : map toUpper \"\" ≡ ’H’ : ’I’ : ’!’ : map toUpper [] ≡ ’H’ : ’I’ : ’!’ : [] ≡ \"HI!\"-} List Functions1234567891011121314151617181920212223242526272829303132333435-- in maths: f(g(x)) == (f o g)(x)myMap :: (a -&gt; b) -&gt; [a] -&gt; [b]myMap f [] = []myMap f (x:xs) = (f x) : (myMap f xs)-- 1 : 2 : 3 : []-- 1 + 2 + 3 + 0sum' :: [Int] -&gt; Intsum' [] = 0sum' (x:xs) = x + sum xs-- [\"hello\",\"world\",\"!\"] -&gt; \"helloworld!\"-- \"hello\":\"world\":\"!\":[]-- \"hello\"++\"world\"++\"!\"++[]concat' :: [[a]] -&gt; [a]concat' [] = []concat' (xs:xss) = xs ++ concat xssfoldr' :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; bfoldr' f z [] = zfoldr' f z (x:xs) = x `f` (foldr' f z xs)sum'' = foldr' (+) 0concat'' = foldr' (++) []filter' :: (a -&gt; Bool) -&gt; [a] -&gt; [a]filter' p [] = []-- filter' p (x:xs) = if p x then x : filter' p xs -- else filter' p xsfilter' p (x:xs) | p x = x : filter' p xs | otherwise = filter' p xs InductionSuppose we want to prove that a property $P(n)$ holds for all natural numbers $n$. Remember that the set of natural numbers $N$ can be defined as follows: Definition of Natural Numbers Inductive definition of Natural numbers 0 is a natural number. For any natural number $n, n + 1$ is also a natural number Therefore, to show $P(n)$ for all $n$, it suffices to show: $P(0)$ (the base case), and assuming $P(k)$ (the inductive hypothesis), $⇒ P(k + 1)$ (the inductive case). Induction on Lists Haskell lists can be defined similarly to natural numbers Definition of Haskell Lists [] is a list. For any list xs, x:xs is also a list (for any item x). This means, if we want to prove that a property P(ls) holds for all lists ls, it suffices to show: P([]) (the base case) P(x:xs) for all items x, assuming the inductive hypothesis P(xs) Data TypesSo far, we have seen type synonyms using the type keyword. For a graphics library, we might define: 123456type Point = (Float, Float)type Vector = (Float, Float)type Line = (Point, Point)type Colour = (Int, Int, Int, Int) -- RGBAmovePoint :: Point -&gt; Vector -&gt; PointmovePoint (x,y) (dx,dy) = (x + dx, y + dy) But these definitions allow Points and Vectors to be used interchangeably, increasing the likelihood of errors. We can define our own compound types using the data keyword: 12data Point = Point Float Floatderiving (Show, Eq) First Point is the type name Second Point is Constructor name Floats are Constructor argument types 12345data Vector = Vector Float Floatderiving (Show, Eq)movePoint :: Point -&gt; Vector -&gt; PointmovePoint (Point x y) (Vector dx dy)= Point (x + dx) (y + dy) RecordsWe could define Colour similarly: 1data Colour = Colour Int Int Int Int But this has so many parameters, it’s hard to tell which is which. Haskell lets us declare these types as records, which is identical to the declaration style on the previous slide, but also gives us projection functions and record syntax: 12345data Colour = Colour { redC :: Int , greenC :: Int , blueC :: Int , opacityC :: Int } deriving (Show, Eq) Here, the code redC (Colour 255 128 0 255) gives 255. Enumeration TypesSimilar to enums in C and Java, we can define types to have one of a set of predefined values: 123456data LineStyle = Solid | Dashed | Dotted deriving (Show, Eq)data FillStyle = SolidFill | NoFill deriving (Show, Eq) Types with more than one constructor are called sum types Algebraic Data TypesJust as the Point constructor took two Float arguments, constructors for sum types can take parameters too, allowing us to model different kinds of shape: 12345678data PictureObject = Path [Point] Colour LineStyle | Circle Point Float Colour LineStyle FillStyle | Polygon [Point] Colour LineStyle FillStyle | Ellipse Point Float Float Float Colour LineStyle FillStyle deriving (Show, Eq)type Picture = [PictureObject] Recursive and Parametric TypesData types can also be defined with parameters, such as the well known Maybe type, defined in the standard library: 1data Maybe a = Just a | Nothing Types can also be recursive. If lists weren’t already defined in the standard library, we could define them ourselves: 1data List a = Nil | Cons a (List a) We can even define natural numbers, where 2 is encoded as Succ(Succ Zero): 1data Natural = Zero | Succ Natural Types in Design Make illegal states unrepresentable. ​ – Yaron Minsky (of Jane Street) Choose types that constrain your implementation as much as possible. Then failure scenarios are eliminated automatically. Partial Functions A partial function is a function not defined for all possible inputs. Partial functions are to be avoided, because they cause your program to crash if undefined cases are encountered. To eliminate partiality, we must either: enlarge the codomain, usually with a Maybe type: 123safeHead :: [a] -&gt; Maybe a safeHead (x:xs) = Just xsafeHead [] = Nothing constrain the domain to be more specific: 1234safeHead' :: NonEmpty a -&gt; asafeHead' (One a) = asafeHead' (Cons a _) = adata NonEmpty a = One a | Cons a (NonEmpty a) Type ClassesYou have already seen functions such as: compare, (==), (+), (show) that work on multiple types, and their corresponding constraints on type variables Ord, Eq, Num and Show. These constraints are called type classes, and can be thought of as a set of types for which certain operations are implemented. Show The Show type class is a set of types that can be converted to strings. 12class Show a where -- nothing to do with OOP show :: a -&gt; String Types are added to the type class as an instance like so: 123instance Show Bool where show True = \"True\" show False = \"False\" We can also define instances that depend on other instances: 123instance Show a =&gt; Show (Maybe a) where show (Just x) = \"Just \" ++ show x show Nothing = \"Nothing\" Fortunately for us, Haskell supports automatically deriving instances for some classes, including Show. Read Type classes can also overload based on the type returned. 12class Read a where read :: String -&gt; a Semigroup A semigroup is a pair of a set S and an operation • : S → S → S where the operation s associative. Associativity is defined as, for all a, b, c: (a • (b • c)) = ((a • b) • c) Haskell has a type class for semigroups! The associativity law is enforced only by programmer discipline: 123class Semigroup s where(&lt;&gt;) :: s -&gt; s -&gt; s-- Law: (&lt;&gt;) must be associative. What instances can you think of? Lists &amp; ++, numbers and +, numbers and * Example: Lets implement additive colour mixing: 12345678instance Semigroup Colour whereColour r1 g1 b1 a1 &lt;&gt; Colour r2 g2 b2 a2 = Colour (mix r1 r2) (mix g1 g2) (mix b1 b2) (mix a1 a2)where mix x1 x2 = min 255 (x1 + x2) Observe that associativity is satisfied. Moniod A monoid is a semigroup (S, •) equipped with a special identity element z : S such that x • z = x and z • y = y for all x, y. 12345class (Semigroup a) =&gt; Monoid a where mempty :: aFor colours, the identity element is transparent black:instance Monoid Colour where mempty = Colour 0 0 0 0 For each of the semigroups discussed previously(lists, num and +, num and *): Are they monoids? Yes If so, what is the identity element? [], 0, 1 Are there any semigroups that are not monoids? Maximum NewtypesThere are multiple possible monoid instances for numeric types like Integer: The operation (+) is associative, with identity element 0 The operation (*) is associative, with identity element 1 Haskell doesn’t use any of these, because there can be only one instance per type per class in the entire program (including all dependencies and libraries used). A common technique is to define a separate type that is represented identically to the original type, but can have its own, different type class instances. In Haskell, this is done with the newtype keyword. A newtype declaration is much like a data declaration except that there can be only one constructor and it must take exactly one argument: 12345newtype Score = S Integerinstance Semigroup Score where S x &lt;&gt; S y = S (x + y)instance Monoid Score where mempty = S 0 Here, Score is represented identically to Integer, and thus no performance penalty is incurred to convert between them. In general, newtypes are a great way to prevent mistakes. Use them frequently! Ord Ord is a type class for inequality comparison 12class Ord a where (&lt;=) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x &lt;= x Transitivity: If x &lt;= y and y &lt;= z then x &lt;= z Antisymmetry: If x &lt;= y and y &lt;= x then x == y. Totality: Either x &lt;= y or y &lt;= x Relations that satisfy these four properties are called total orders(most are total orders). Without the fourth (totality), they are called partial orders(e.g. division). Eq Eq is a type class for equality or equivalence 12class Eq a where (==) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x == x. Transitivity: If x == y and y == z then x == z. Symmetry: If x == y then y == x. Relations that satisfy these are called equivalence relations. Some argue that the Eq class should be only for equality, requiring stricter laws like: If x == y then f x == f y for all functions f But this is debated. Funtors Types and ValuesHaskell is actually comprised of two languages. The value-level language, consisting of expressions such as if, let, 3 etc. The type-level language, consisting of types Int, Bool, synonyms like String, and type constructors like Maybe, (-&gt;), [ ] etc This type level language itself has a type system! KindsJust as terms in the value level language are given types, terms in the type level language are given kinds. The most basic kind is written as *. Types such as Int and Bool have kind *. Seeing as Maybe is parameterised by one argument, Maybe has kind * -&gt; *: given a type (e.g. Int), it will return a type (Maybe Int). ListsSuppose we have a function: 1toString :: Int -&gt; String And we also have a function to give us some numbers: 1getNumbers :: Seed -&gt; [Int] How can I compose toString with getNumbers to get a function f of type Seed -&gt; [String]? we use map: f = map toString . getNumbers. What about return a Maybe Int? we can use maybe map 1234567maybeMap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe bmaybeMap f Nothing = NothingmaybeMap f (Just x) = Just (f x)maybeMap f mx = case mx of Nothing -&gt; Nothing Just x -&gt; Just (f x) We can generalise this using functor. FunctorAll of these functions are in the interface of a single type class, called Functor. 12class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Unlike previous type classes we’ve seen like Ord and Semigroup, Functor is over types of kind * -&gt; *. 1234567891011121314151617181920-- Instance for tuples-- type level:-- (,) :: * -&gt; (* -&gt; *)-- (,) x :: * -&gt; *instance Functor ((,) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (,) x a -&gt; (,) x b-- fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x, b) fmap f (x,a) = (x, f a) -- instance for functions-- type level:-- (-&gt;) :: * -&gt; (* -&gt; *)-- (-&gt;) x :: * -&gt; *instance Functor ((-&gt;) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (-&gt;) x a -&gt; (-&gt;) x b-- fmap :: (a -&gt; b) -&gt; (x -&gt; a) -&gt; (x -&gt; b) fmap = (.) Functor LawsThe functor type class must obey two laws: fmap id == id(indentity law) fmap f . fmap g == fmap (f . g)(composition law, haskell use this law to do optimisation) In Haskell’s type system it’s impossible to make a total fmap function that satisfies the first law but violates the second. This is due to parametricity. Property Based TestingFree PropertiesHaskell already ensures certain properties automatically with its language design and type system. Memory is accessed where and when it is safe and permitted to be accessed (memory safety). Values of a certain static type will actually have that type at run time. Programs that are well-typed will not lead to undefined behaviour (type safety). All functions are pure: Programs won’t have side effects not declared in the type. (purely functional programming) ⇒ Most of our properties focus on the logic of our program. Logical PropertiesWe have already seen a few examples of logical properties. Example: reverse is an involution: reverse (reverse xs) == xs right identity for (++): xs ++ [] == xs transitivity of (&gt;): (a &gt; b) ∧ (b &gt; c) ⇒ (a &gt; c) The set of properties that capture all of our requirements for our program is called the functional correctness specification of our software. This defines what it means for software to be correct. ProofsLast week we saw some proof methods for Haskell programs. We could prove that our implementation meets its functional correctness specification. Such proofs certainly offer a high degree of assurance, but: Proofs must make some assumptions about the environment and the semantics of the software. Proof complexity grows with implementation complexity, sometimes drastically. If software is incorrect, a proof attempt might simply become stuck: we do not always get constructive negative feedback. Proofs can be labour and time intensive ($$$), or require highly specialised knowledge ($$$). TestingCompared to proofs: Tests typically run the actual program, so requires fewer assumptions about the language semantics or operating environment. Test complexity does not grow with implementation complexity, so long as the specification is unchanged. Incorrect software when tested leads to immediate, debuggable counter examples. Testing is typically cheaper and faster than proving. Tests care about efficiency and computability, unlike proofs(e.g. termination is provable but not computable). We lose some assurance, but gain some convenience ($$$). Property Based Testing Key idea: Generate random input values, and test properties by running them. Example(QuickCheck Property) 1234567891011121314151617181920import Test.QuickCheckimport Data.Charimport Data.List-- Testable -- Arbitrary Testable-- Arbitrary Testableprop_reverseApp :: [Int] -&gt; ([Int] -&gt; Bool)prop_reverseApp xs ys = reverse (xs ++ ys) == reverse ys ++ reverse xsdivisible :: Int -&gt; Int -&gt; Booldivisible x y = x `mod` y == 0-- or select different generators with modifier newtypes.prop_refl :: Positive Int -&gt; Bool prop_refl (Positive x) = divisible x x-- Encode pre-conditions with the (==&gt;) operator:prop_unwordsWords s = unwords (words s) == sprop_wordsUnwords l = all (\\w -&gt; all (not . isSpace) w &amp;&amp; w /= []) l ==&gt; words (unwords l) == l PBT vs. Unit Testing Properties are more compact than unit tests, and describe more cases. ⇒ Less testing code Property-based testing heavily depends on test data generation: Random inputs may not be as informative as hand-crafted inputs ⇒ use shrinking(When a test fails, it finds the smallest test case still falls) Random inputs may not cover all necessary corner cases: ⇒ use a coverage checker Random inputs must be generated for user-defined types: ⇒ QuickCheck includes functions to build custom generators By increasing the number of random inputs, we improve code coverage in PBT. Test Data GenerationData which can be generated randomly is represented by the following type class: 123class Arbitrary a where arbitrary :: Gen a -- more on this later shrink :: a -&gt; [a] Most of the types we have seen so far implement Arbitrary. Shrinking The shrink function is for when test cases fail. If a given input x fails, QuickCheck will try all inputs in shrink x; repeating the process until the smallest possible input is found Testable TypesThe type of the quickCheck function is: 12-- more on IO laterquickCheck :: (Testable a) =&gt; a -&gt; IO () The Testable type class is the class of things that can be converted into properties. This includes: Bool values QuickCheck’s built-in Property type Any function from an Arbitrary input to a Testable output: 12instance (Arbitrary i, Testable o) =&gt; Testable (i -&gt; o) ... Thus the type [Int] -&gt; [Int] -&gt; Bool (as used earlier) is Testable. Examples12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455split :: [a] -&gt; ([a],[a])split [] = ([],[])split [a] = ([a],[])split (x:y:xs) = let (l,r) = split xs in (x:l,y:r)prop_splitPerm xs = let (l,r) = split (xs :: [Int]) in permutation xs (l ++ r)permutation :: (Ord a) =&gt; [a] -&gt; [a] -&gt; Boolpermutation xs ys = sort xs == sort yspermutation' :: (Eq a) =&gt; [a] -&gt; [a] -&gt; (a -&gt; Bool)permutation' xs ys = \\x -&gt; count x xs == count x ys where count x l = length (filter (== x) l)merge :: (Ord a) =&gt; [a] -&gt; [a] -&gt; [a]merge [] ys = ysmerge xs [] = xsmerge (x:xs) (y:ys) | x &lt;= y = x : merge xs (y:ys) | otherwise = y : merge (x:xs) ysprop_mergePerm xs ys = permutation (xs ++ (ys :: [Int] )) (merge xs ys)prop_mergeSorted (Ordered xs) (Ordered ys) = sorted (merge (xs :: [Int]) ys)sorted :: Ord a =&gt; [a] -&gt; Boolsorted [] = True sorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)mergeSort :: (Ord a) =&gt; [a] -&gt; [a]mergeSort [] = []mergeSort [x] = [x]mergeSort xs = let (l,r) = split xs in merge (mergeSort l) (mergeSort r)prop_mergeSortSorts xs = sorted (mergeSort (xs :: [Int])) prop_mergeSortPerm xs = permutation xs (mergeSort (xs :: [Int]))prop_mergeSortExtra xs = mergeSort (xs :: [Int]) == sort xsprop_mergeSortUnit = mergeSort [3,2,1] == [1,2,3]main = do quickCheck prop_mergeSortUnit quickCheck prop_mergeSortSorts quickCheck prop_mergeSortPerm Redundant PropertiesSome properties are technically redundant (i.e. implied by other properties in the specification), but there is some value in testing them anyway: They may be more efficient than full functional correctness tests, consuming less computing resources to test. They may be more fine-grained to give better test coverage than random inputs for full functional correctness tests. They provide a good sanity check to the full functional correctness properties. Sometimes full functional correctness is not easily computable but tests of weaker properties are. These redundant properties include unit tests. We can (and should) combine both approaches! Lazy Evaluation It never evaluate anything unless it has to 123sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n This crashes when given a large number. Why? Because of the growing stack frame. 1234567891011121314sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = sumTo' (a+n) (n-1)sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n-- sumTo' 0 5-- sumTo' (0+5) (5-1)-- sumTo' (0+5) 4-- sumTo' (0+5+4) (4-1)-- sumTo' (0+5+4) 3-- sumTo' (0+5+4+3) (3-1)-- sumTo' (0+5+4+3) 2 -&gt; never evaluate the first argument-- .. This still crashes when given a large number. Why? This is called a space leak, and is one of the main drawbacks of Haskell’s lazy evaluation method. Haskell is lazily evaluated, also called call-by-need. This means that expressions are only evaluated when they are needed to compute a result for the user. We can force the previous program to evaluate its accumulator by using a bang pattern, or the primitive operation seq: 1234567{-# LANGUAGE BangPatterns #-}sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' !a 0 = asumTo' !a n = sumTo' (a+n) (n-1)sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = let a' = a + n in a' `seq` sumTo' a' (n-1) AdvantagesLazy Evaluation has many advantages: It enables equational reasoning even in the presence of partial functions and non-termination It allows functions to be decomposed without sacrificing efficiency, for example: minimum = head . sort is, depending on sorting algorithm, possibly O(n). It allows for circular programming and infinite data structures, which allow us to express more things as pure functions. Infinite Data StructuresLaziness lets us define data structures that extend infinitely. Lists are a common example, but it also applies to trees or any user-defined data type: 1ones = 1 : ones Many functions such as take, drop, head, tail, filter and map work fine on infinite lists. 12345naturals = 0 : map (1+) naturals--ornaturals = map sum (inits ones)-- fibonacci numbersfibs = 1:1:zipWith (+) fibs (tail fibs) Data Invariants and ADTsStructure of a ModuleA Haskell program will usually be made up of many modules, each of which exports one or more data types. Typically a module for a data type X will also provide a set of functions, called operations, on X. to construct the data type: c :: · · · → X to query information from the data type: q :: X → · · · to update the data type: u :: · · · X → X A lot of software can be designed with this structure. Example: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869module Dictionary ( Word , Definition , Dict , emptyDict , insertWord , lookup ) whereimport Prelude hiding (Word, lookup)import Test.QuickCheckimport Test.QuickCheck.Modifiers-- lookup :: [(a,b)] -&gt; a -&gt; Maybe btype Word = Stringtype Definition = Stringnewtype Dict = D [DictEntry] deriving (Show, Eq)emptyDict :: DictemptyDict = D []insertWord :: Word -&gt; Definition -&gt; Dict -&gt; DictinsertWord w def (D defs) = D (insertEntry (Entry w def) defs) where insertEntry wd (x:xs) = case compare (word wd) (word x) of GT -&gt; x : (insertEntry wd xs) EQ -&gt; wd : xs LT -&gt; wd : x : xs insertEntry wd [] = [wd]lookup :: Word -&gt; Dict -&gt; Maybe Definitionlookup w (D es) = search w es where search w [] = Nothing search w (e:es) = case compare w (word e) of LT -&gt; Nothing EQ -&gt; Just (defn e) GT -&gt; search w essorted :: (Ord a) =&gt; [a] -&gt; Boolsorted [] = Truesorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)wellformed :: Dict -&gt; Boolwellformed (D es) = sorted esprop_insert_wf dict w d = wellformed dict ==&gt; wellformed (insertWord w d dict)data DictEntry = Entry { word :: Word , defn :: Definition } deriving (Eq, Show)instance Ord DictEntry where Entry w1 d1 &lt;= Entry w2 d2 = w1 &lt;= w2instance Arbitrary DictEntry where arbitrary = Entry &lt;$&gt; arbitrary &lt;*&gt; arbitraryinstance Arbitrary Dict where arbitrary = do Ordered ds &lt;- arbitrary pure (D ds)prop_arbitrary_wf dict = wellformed dict Data Invariants Data invariants are properties that pertain to a particular data type. Whenever we use operations on that data type, we want to know that our data invariants are maintained. For a given data type X, we define a wellformedness predicate $$\\text{wf :: X → Bool}$$For a given value x :: X, wf x returns true iff our data invariants hold for the value x For each operation, if all input values of type X satisfy wf, all output values will satisfy wf. In other words, for each constructor operation c :: · · · → X, we must show wf (c · · ·), and for each update operation u :: X → X we must show wf x =⇒ wf(u x) Abstract Data Types An abstract data type (ADT) is a data type where the implementation details of the type and its associated operations are hidden. 123456newtype Dicttype Word = Stringtype Definition = StringemptyDict :: DictinsertWord :: Word -&gt; Definition -&gt; Dict -&gt; Dictlookup :: Word -&gt; Dict -&gt; Maybe Definition If we don’t have access to the implementation of Dict, then we can only access it via the provided operations, which we know preserve our data invariants. Thus, our data invariants cannot be violated if this module is correct. In general, abstraction is the process of eliminating detail. The inverse of abstraction is called refinement. Abstract data types like the dictionary above are abstract in the sense that their implementation details are hidden, and we no longer have to reason about them on the level of implementation. ValidationSuppose we had a sendEmail function 123sendEmail :: String -- email address -&gt; String -- message -&gt; IO () -- action (more in 2 wks) It is possible to mix the two String arguments, and even if we get the order right, it’s possible that the given email address is not valid. We could define a tiny ADT for validated email addresses, where the data invariant is that the contained email address is valid: 1234567module EmailADT(Email, checkEmail, sendEmail) newtype Email = Email String checkEmail :: String -&gt; Maybe Email checkEmail str | '@' `elem` str = Just (Email str) | otherwise = Nothing-- Then, change the type of sendEmail: sendEmail :: Email -&gt; String -&gt; IO() The only way (outside of the EmailADT module) to create a value of type Email is to use checkEmail. checkEmail is an example of what we call a smart constructor: a constructor that enforces data invariants. Data RefinementReasoning about ADTsConsider the following, more traditional example of an ADT interface, the unbounded queue: 123456data QueueemptyQueue :: Queueenqueue :: Int -&gt; Queue -&gt; Queuefront :: Queue -&gt; Int -- partialdequeue :: Queue -&gt; Queue -- partialsize :: Queue -&gt; Int We could try to come up with properties that relate these functions to each other without reference to their implementation, such as: dequeue (enqueue x emptyQueue) == emptyQueue However these do not capture functional correctness (usually). Models for ADTsWe could imagine a simple implementation for queues, just in terms of lists: 12345emptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = length But this implementation is O(n) to enqueue! Unacceptable! However!(This is out mental model) This is a dead simple implementation, and trivial to see that it is correct. If we make a better queue implementation, it should always give the same results as this simple one. Therefore: This implementation serves as a functional correctness specification for our Queue type! Refinement Relations The typical approach to connect our model queue to our Queue type is to define a relation, called a refinement relation, that relates a Queue to a list and tells us if the two structures represent the same queue conceptually: 1234rel :: Queue -&gt; [Int] -&gt; Boolprop_empty_r = rel emptyQueue emptyQueueLprop_size_r fq lq = rel fq lq ==&gt; size fq == sizeL lqprop_enq_ref fq lq x = rel fq lq ==&gt; rel (enqueue x fq) (enqueueL x lq) Abstraction FunctionsThese refinement relations are very difficult to use with QuickCheck because the rel fq lq preconditions are very hard to satisfy with randomly generated inputs. For this example, it’s a lot easier if we define an abstraction function that computes the corresponding abstract list from the concrete Queue. 1toAbstract :: Queue → [Int] Conceptually, our refinement relation is then just: 1\\fq lq → absfun fq == lq However, we can re-express our properties in a much more QC-friendly format 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import Test.QuickCheckemptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = lengthtoAbstract :: Queue -&gt; [Int]toAbstract (Q f sf r sr) = f ++ reverse rprop_empty_ref = toAbstract emptyQueue == emptyQueueLprop_enqueue_ref fq x = toAbstract (enqueue x fq) == enqueueL x (toAbstract fq)prop_size_ref fq = size fq == sizeL (toAbstract fq)prop_front_ref fq = size fq &gt; 0 ==&gt; front fq == frontL (toAbstract fq)prop_deq_ref fq = size fq &gt; 0 ==&gt; toAbstract (dequeue fq) == dequeueL (toAbstract fq)prop_wf_empty = wellformed emptyQueueprop_wf_enq x q = wellformed q ==&gt; wellformed (enqueue x q)prop_wf_deq x q = wellformed q &amp;&amp; size q &gt; 0 ==&gt; wellformed (dequeue q)data Queue = Q [Int] -- front of the queue Int -- size of the front [Int] -- rear of the queue Int -- size of the rear deriving (Show, Eq)wellformed :: Queue -&gt; Boolwellformed (Q f sf r sr) = length f == sf &amp;&amp; length r == sr &amp;&amp; sf &gt;= srinstance Arbitrary Queue where arbitrary = do NonNegative sf' &lt;- arbitrary NonNegative sr &lt;- arbitrary let sf = sf' + sr f &lt;- vectorOf sf arbitrary r &lt;- vectorOf sr arbitrary pure (Q f sf r sr)inv3 :: Queue -&gt; Queueinv3 (Q f sf r sr) | sf &lt; sr = Q (f ++ reverse r) (sf + sr) [] 0 | otherwise = Q f sf r sremptyQueue :: QueueemptyQueue = Q [] 0 [] 0enqueue :: Int -&gt; Queue -&gt; Queueenqueue x (Q f sf r sr) = inv3 (Q f sf (x:r) (sr+1))front :: Queue -&gt; Int -- partialfront (Q (x:f) sf r sr) = xdequeue :: Queue -&gt; Queue -- partialdequeue (Q (x:f) sf r sr) = inv3 (Q f (sf -1) r sr)size :: Queue -&gt; Intsize (Q f sf r sr) = sf + sr Data RefinementThese kinds of properties establish what is known as a data refinement from the abstract, slow, list model to the fast, concrete Queue implementation. Refinement and Specifications In general, all functional correctness specifications can be expressed as: all data invariants are maintained, and the implementation is a refinement of an abstract correctness model. There is a limit to the amount of abstraction we can do before they become useless for testing (but not necessarily for proving). Effects Effects are observable phenomena from the execution of a program. Internal vs. External EffectsExternal Observability An external effect is an effect that is observable outside the function. Internal effects are not observable from outside. Example Console, file and network I/O; termination and non-termination; non-local control flow; etc. Are memory effects external or internal? Depends on the scope of the memory being accessed. Global variable accesses are external. PurityA function with no external effects is called a pure function. A pure function is the mathematical notion of a function. That is, a function of type a -&gt; b is fully specified by a mapping from all elements of the domain type a to the codomain type b. Consequences: Two invocations with the same arguments result in the same value. No observable trace is left beyond the result of the function. No implicit notion of time or order of execution. Haskell FunctionsHaskell functions are technically not pure. They can loop infinitely. They can throw exceptions (partial functions) They can force evaluation of unevaluated expressions. Caveat Purity only applies to a particular level of abstraction. Even ignoring the above, assembly instructions produced by GHC aren’t really pure. Despite the impurity of Haskell functions, we can often reason as though they are pure. Hence we call Haskell a purely functional language. The Danger of Implicit Side Effects They introduce (often subtle) requirements on the evaluation order. They are not visible from the type signature of the function. They introduce non-local dependencies which is bad for software design, increasing coupling. They interfere badly with strong typing, for example mutable arrays in Java, or reference types in ML. We can’t, in general, reason equationally about effectful programs! Can we program with pure functions?Typically, a computation involving some state of type s and returning a result of type a can be expressed as a function: 1s -&gt; (s, a) Rather than change the state, we return a new copy of the state. All that copying might seem expensive, but by using tree data structures, we can usually reduce the cost to an O(log n) overhead. StateState Passing12345678910111213data Tree a = Branch a (Tree a) (Tree a) | Leaf-- Given a tree, label each node with an ascending number in infix order:label :: Tree () -&gt; Tree Intlable t = snd (go t 1) where go :: Tree() -&gt; Int -&gt; (Int, Tree Int) go Leaf c = (c, Leaf) go (Branch () l r) c = let (c', l') = go l c v = c' (c'', r') = go r (c'+1) in (c'', Branch v l' r')-- it works but not pretty Let’s use a data type to simplify this! State12345678910111213141516newtype State s a = A procedure that, manipulating some state of type s, returns a-- State Operationsget :: State s sput :: s -&gt; State s ()pure :: a -&gt; State s aevalState :: State s a -&gt; s -&gt; a-- Sequential Composition-- Do one state action after another with do blocks:do put 42 desugars put 42 &gt;&gt; put Truepure True(&gt;&gt;) :: State s a -&gt; State s b -&gt; State s b-- Bind-- The 2nd step can depend on the first with bind:do x &lt;- get desugars get &gt;&gt;= \\x -&gt; pure (x + 1)pure (x+1)(&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b Example 12345678910111213141516171819202122232425262728293031323334353637383940{-modify :: (s -&gt; s) -&gt; State s ()modify f = do s &lt;- get put (f s)-}label' :: Tree () -&gt; Tree Intlabel' t = evalState (go t) 1 where go :: Tree () -&gt; State Int (Tree Int) go Leaf = pure Leaf go (Branch () l r) = do l' &lt;- go l v &lt;- get put (v + 1) r' &lt;- go r pure (Branch v l' r')newtype State' s a = State (s -&gt; (s, a))get' :: State' s sget' = (State $ \\s -&gt; (s, s)) put' :: s -&gt; State' s ()put' s = State $ \\_ -&gt; (s,())pure' :: a -&gt; State' s apure' a = State $ \\s -&gt; (s, a)evalState' :: State' s a -&gt; s -&gt; aevalState (State f) s = snd (f s) (&gt;&gt;=!) :: State' s a -&gt; (a -&gt; State' s b) -&gt; State' s b(State c) &gt;&gt;=! f = State $ \\s -&gt; let (s', a) = c s (State c') = f a in c' s'(&gt;&gt;!) :: State' s a -&gt; State' s b -&gt; State' s b(&gt;&gt;!) a b = a &gt;&gt;=! \\_ -&gt; b IO A procedure that performs some side effects, returning a result of type a is written as IO a. IO a is an abstract type. But we can think of it as a function: RealWorld -&gt; (RealWorld, a) (that’s how it’s implemented in GHC) 123456(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO bpure :: a -&gt; IO agetChar :: IO CharreadLine :: IO StringputStrLn :: String -&gt; IO () -- return a procedure Infectious IOWe can convert pure values to impure procedures with pure: 1pure :: a -&gt; IO a But we can’t convert impure procedures to pure values The only function that gets an a from an IO a is &gt;&gt;=: 1(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b But it returns an IO procedure as well. If a function makes use of IO effects directly or indirectly, it will have IO in its type! Haskell Design StrategyWe ultimately “run” IO procedures by calling them from main: 1main :: IO () Example 12345678-- Given an input number n, print a triangle of * characters of base width n.printTriangle :: Int -&gt; IO ()printTriangle 0 = pure ()printTriangle n = do putStrLn (replicate n '*') printTriangle (n - 1)main = printTriangle 9 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108{-Design a game that reads in a n × n maze from a file. The player starts at position(0, 0) and must reach position (n − 1, n − 1) to win. The game accepts keyboard inputto move the player around the maze.-}import Data.List import System.IOmazeSize :: Int mazeSize = 10data Tile = Wall | Floor deriving (Show, Eq)type Point = (Int, Int)lookupMap :: [Tile] -&gt; Point -&gt; TilelookupMap ts (x,y) = ts !! (y * mazeSize + x)addX :: Int -&gt; Point -&gt; PointaddX dx (x,y) = (x + dx, y)addY :: Int -&gt; Point -&gt; PointaddY dy (x,y) = (x, y + dy)data Game = G { player :: Point , map :: [Tile] }invariant :: Game -&gt; Bool invariant (G (x,y) ts) = x &gt;= 0 &amp;&amp; x &lt; mazeSize &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; mazeSize &amp;&amp; lookupMap ts (x,y) /= WallmoveLeft :: Game -&gt; Game moveLeft (G p m) = let g' = G (addX (-1) p) m in if invariant g' then g' else G p mmoveRight :: Game -&gt; GamemoveRight (G p m) = let g' = G (addX 1 p) m in if invariant g' then g' else G p mmoveUp :: Game -&gt; GamemoveUp (G p m) = let g' = G (addY (-1) p) m in if invariant g' then g' else G p mmoveDown :: Game -&gt; GamemoveDown (G p m) = let g' = G (addY 1 p) m in if invariant g' then g' else G p mwon :: Game -&gt; Bool won (G p m) = p == (mazeSize-1,mazeSize-1)main :: IO () main = do str &lt;- readFile \"input.txt\" let initial = G (0,0) (stringToMap str) gameLoop initial where gameLoop :: Game -&gt; IO () gameLoop state | won state = putStrLn \"You win!\" | otherwise = do display state c &lt;- getChar' case c of 'w' -&gt; gameLoop (moveUp state) 'a' -&gt; gameLoop (moveLeft state) 's' -&gt; gameLoop (moveDown state) 'd' -&gt; gameLoop (moveRight state) 'q' -&gt; pure () _ -&gt; gameLoop state stringToMap :: String -&gt; [Tile]stringToMap [] = []stringToMap ('#':xs) = Wall : stringToMap xs stringToMap (' ':xs) = Floor : stringToMap xs stringToMap (c:xs) = stringToMap xsdisplay :: Game -&gt; IO () display (G (px,py) m) = printer (0,0) m where printer (x,y) (t:ts) = do if (x,y) == (px,py) then putChar '@' else if t == Wall then putChar '#' else putChar ' ' if (x == mazeSize - 1) then do putChar '\\n' printer (0,y+1) ts else printer (x+1,y) ts printer (x,y) [] = putChar '\\n'getChar' :: IO Char getChar' = do b &lt;- hGetBuffering stdin e &lt;- hGetEcho stdin hSetBuffering stdin NoBuffering hSetEcho stdin False x &lt;- getChar hSetBuffering stdin b hSetEcho stdin e pure x Benefits of an IO Type Absence of effects makes type system more informative: A type signatures captures entire interface of the function All dependencies are explicit in the form of data dependencies. All dependencies are typed It is easier to reason about pure code and it is easier to test Testing is local, doesn’t require complex set-up and tear-down. Reasoning is local, doesn’t require state invariants Type checking leads to strong guarantees. Mutable VariablesWe can have honest-to-goodness mutability in Haskell, if we really need it, using IORef. 1234data IORef anewIORef :: a -&gt; IO (IORef a)readIORef :: IORef a -&gt; IO awriteIORef :: IORef a -&gt; a -&gt; IO () Example 1234567891011121314151617181920212223242526import Data.IORef import Test.QuickCheck.Monadic import Test.QuickCheckaverageListIO :: [Int] -&gt; IO IntaverageListIO ls = do sum &lt;- newIORef 0 count &lt;- newIORef 0 let loop :: [Int] -&gt; IO () loop [] = pure () loop (x:xs) = do s &lt;- readIORef sum writeIORef sum (s + x) c &lt;- readIORef count writeIORef count (c + 1) loop xs loop ls s &lt;- readIORef sum c &lt;- readIORef count pure (s `div` c) prop_average :: [Int] -&gt; Propertyprop_average ls = monadicIO $ do pre (length ls &gt; 0) avg &lt;- run (averageListIO ls) assert (avg == (sum ls `div` length ls)) Mutable Variables, LocallySomething like averaging a list of numbers doesn’t require external effects, even if we use mutation internally. 12345data STRef s anewSTRef :: a -&gt; ST (STRef s a)readSTRef :: STRef s a -&gt; ST s awriteSTRef :: STRef s a -&gt; a -&gt; ST s ()runST :: (forall s. ST s a) -&gt; a The extra s parameter is called a state thread, that ensures that mutable variables don’t leak outside of the ST computation. The ST type is not assessable in this course, but it is useful sometimes in Haskell programming. QuickChecking EffectsQuickCheck lets us test IO (and ST) using this special property monad interface: 1234monadicIO :: PropertyM IO () -&gt; Propertypre :: Bool -&gt; PropertyM IO ()assert :: Bool -&gt; PropertyM IO ()run :: IO a -&gt; PropertyM IO a Do notation and similar can be used for PropertyM IO procedures just as with State s and IO procedures. Example 12345678910111213-- GNU Factorimport Test.QuickCheck import Test.QuickCheck.Modifiersimport Test.QuickCheck.Monadic import System.Process-- readProcess :: FilePath -&gt; [String] -&gt; String -&gt; IO Stringtest_gnuFactor :: Positive Integer -&gt; Propertytest_gnuFactor (Positive n) = monadicIO $ do str &lt;- run (readProcess \"gfactor\" [show n] \"\") let factors = map read (tail (words str)) assert (product factors == n) FunctorRecall the type class defined over type constructors called Functor. We’ve seen instances for lists, Maybe, tuples and functions. Other instances include: IO (how?) States (how?) Gen 123456789101112131415ioMap :: (a -&gt; b) -&gt; IO a -&gt; IO bioMap f act = do a &lt;- act pure (f a) stateMap :: (a -&gt; b) -&gt; State s a -&gt; State s bstateMap f act = do a &lt;- act pure (f a)-- more generalmonadMap :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m bmonadMap f act = do a &lt;- act pure (f a) QuickCheck Generators12345678910{-class Arbitrary a where arbitray :: Gen a shrink :: a -&gt; [a]-- Gen a ~=~ random generator of values type a.-}sortedLists :: (Arbitrary a, Ord a) =&gt; Gen [a]sortedLists = fmap sort arbitrary :: Gen a-- listOf :: Gen a -&gt; Gen [a] Applicative FunctorsBinary FunctionsSuppose we want to look up a student’s zID and program code using these functions: 1234lookupID :: Name -&gt; Maybe ZIDlookupProgram :: Name -&gt; Maybe Program-- we had a function:makeRecord :: ZID -&gt; Program -&gt; StudentRecord How can we combine these functions to get a function of type Name -&gt; Maybe StudentRecord? 1234lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in ? Binary Map? We could imagine a binary version of the maybeMap function: 12maybeMap2 :: (a -&gt; b -&gt; c) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c But then, we might need a trinary version. 12maybeMap3 :: (a -&gt; b -&gt; c -&gt; d) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c -&gt; Maybe d Or even a 4-ary version, 5-ary, 6-ary. . . this would quickly become impractical! Using Functor? Using fmap gets us part of the way there: 12345lookupRecord' :: Name -&gt; Maybe (Program -&gt; StudentRecord)lookupRecord' n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid -- what about program? But, now we have a function inside a Maybe. Applicative This is encapsulated by a subclass of Functor called Applicative. 123class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b Maybe is an instance, so we can use this for lookupRecord: 12345lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid &lt;*&gt; program -- or pure makeRecord &lt;*&gt; zid &lt;*&gt; program Using ApplicativeIn general, we can take a regular function application:$$\\text{f a b c d}$$And apply that function to Maybe (or other Applicative) arguments using this pattern (where &lt;*&gt; is left-associative): $$\\text{pure f &lt;&gt; ma &lt;&gt; mb &lt;&gt; mc &lt;&gt; }$$ Relationship to FunctorAll law-abiding instances of Applicative are also instances of Functor, by defining: 1fmap f x = pure f &lt;*&gt; x Sometimes this is written as an infix operator, &lt;$&gt;, which allows us to write: 123pure f &lt;*&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md-- as:f &lt;$&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md Applicative laws12345678-- Identitypure id &lt;*&gt; v = v-- Homomorphismpure f &lt;*&gt; pure x = pure (f x)-- Interchangeu &lt;*&gt; pure y = pure ($ y) &lt;*&gt; u-- Compositionpure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w) Example 1234567891011121314151617181920212223242526272829type Name = Stringtype ZID = Intdata Program = COMP | SENG | BINF | CENG deriving (Show, Eq)type StudentRecord = (Name, ZID, Program)lookupID :: Name -&gt; Maybe ZIDlookupID \"Liam\" = Just 3253158lookupID \"Unlucky\" = Just 4444444lookupID \"Prosperous\" = Just 8888888lookupID _ = NothinglookupProgram :: Name -&gt; Maybe ProgramlookupProgram \"Liam\" = Just COMPlookupProgram \"Unlucky\" = Just SENGlookupProgram \"Prosperous\" = Just CENGlookupProgram _ = NothingmakeRecord :: ZID -&gt; Program -&gt; Name -&gt; StudentRecordmakeRecord zid pr name = (name,zid,pr)liam :: Maybe StudentRecordliam = let mzid = lookupID \"Liam\" mprg = lookupProgram \"Liam\" in pure makeRecord &lt;*&gt; mzid &lt;*&gt; mprg &lt;*&gt; pure \"Liam\"-- pure :: a -&gt; Maybe a-- fmap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b Functor Laws for ApplicativeThese are proofs not Haskell code: 123456789101112131415fmap f x = pure f &lt;*&gt; x-- The two functor laws are:1. fmap id x == x2. fmap f (fmap g x) == fmap (f.g) x-- Proof:1) pure id &lt;*&gt; x == x -- true by Identity law2) pure f &lt;*&gt; (pure g &lt;*&gt; x) == pure (.) &lt;*&gt; pure f &lt;*&gt; pure g &lt;*&gt; x --Composition == pure ((.) f) &lt;*&gt; pure g &lt;*&gt; x --Homomorphism == pure (f.g) &lt;*&gt; x --Homomorphism Applicative ListsThere are two ways to implement Applicative for lists: 1(&lt;*&gt;) :: [a -&gt; b] -&gt; [a] -&gt; [b] Apply each of the given functions to each of the given arguments, concatenating all the results Apply each function in the list of functions to the corresponding value in the list of arguments The second one is put behind a newtype (ZipList) in the Haskell standard library. 1234567891011121314pureZ :: a -&gt; [a]pureZ a = a:pureZ aapplyListsZ :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsZ (f:fs) (x:xs) = f x : applyListsZ fs xsapplyListsZ [] _ = []applyListsZ _ [] = []pureC :: a -&gt; [a]pureC a = [a] applyListsC :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsC (f:fs) args = map f args ++ applyListsC fs argsapplyListsC [] args = [] Other instancesQuickCheck generators: Gen1234data Concrete = C [Char] [Char] deriving (Show, Eq)instance Arbitrary Concrete where arbitrary = C &lt;$&gt; arbitrary &lt;*&gt; arbitrary Functions: ((-&gt;) x The Applicative instance for functions lets us pass the same argument into multiple functions without repeating ourselves. 1234567891011instance Applicative ((-&gt;) x) where pure :: a -&gt; x -&gt; a pure a x = a (&lt;*&gt;) :: (x -&gt; (a -&gt; b)) -&gt; (x -&gt; a) -&gt; (x -&gt; b) (&lt;*&gt;) xab xa x = xab x (xa x)-- f (g x) (h x) (i x)---- Can be written as: -- (pure f &lt;*&gt; g &lt;*&gt; h &lt;*&gt; i) x Tuples: ((,) x)We can’t implement pure without an extra constraint! The tuple instance for Applicative lets us combine secondary outputs from functions into one secondary output without manually combining them. 1234567891011instance Functor ((,) x) where fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x,b) fmap f (x,a) = (x,f a)-- monoid has an identity elementinstance Monoid x =&gt; Applicative ((,) x) where pure :: a -&gt; (x,a) pure a = (mempty ,a) (&lt;*&gt;) :: (x,a -&gt; b) -&gt; (x, a) -&gt; (x, b) (&lt;*&gt;) (x, f) (x',a) = (x &lt;&gt; x', f a) It requires Monoid here to combine the values, and to provide a default value for pure. 12345678910111213f :: A -&gt; (Log, B)g :: X -&gt; (Log, Y)a :: Ax :: Xcombine :: B -&gt; Y -&gt; Z-- combine the logs silentlytest :: (Log, Z)test = combine &lt;$&gt; f a &lt;*&gt; g x-- instead of -- let (l1, b) = f a-- (l2, y) = g x-- in (l1 &lt;&gt; l2, combine b y) IO and State s123456789instance Applicative IO where pure :: a -&gt; IO a pure a = pure a (&lt;*&gt;) :: IO (a -&gt; b) -&gt; IO a -&gt; IO b pf &lt;*&gt; pa = do f &lt;- pf a &lt;- pa pure (f a) Monads Monads are types m where we can sequentially compose functions of the form a -&gt; m b 12class Applicative m =&gt; Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b Sometimes in old documentation the function return is included here, but it is just an alias for pure. It has nothing to do with return as in C/Java/Python etc. Example 123456789101112131415161718192021222324-- Maybe Monad(&gt;&gt;=) :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b(&gt;&gt;=) Nothing f = Nothing(&gt;&gt;=) (Just a) f = f a-- List Monad(&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b](&gt;&gt;=) as f = concatMap f as-- function(&gt;&gt;=) :: (x -&gt; a) -&gt; (a -&gt; x -&gt;b) -&gt; b(&gt;&gt;=) xa axb x = axb (xa x) x-- function monad example(reader monad)f :: A -&gt; Config -&gt; Bf :: X -&gt; Config -&gt; Yf :: B -&gt; Config -&gt; Ccombine :: (A, X) -&gt; Config -&gt; (Y, C)combine (a, x) = do b &lt;- f a y &lt;- g x c &lt;- h b pure(y, c) Monad LawWe can define a composition operator with (&gt;&gt;=): 12(&lt;=&lt;) :: (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) (f &lt;=&lt; g) x = g x &gt;&gt;= f Monad Laws 123f &lt;=&lt; (g &lt;=&lt; x) == (f &lt;=&lt; g) &lt;=&lt; x -- associativitypure &lt;=&lt; f == f -- left identityf &lt;=&lt; pure == f -- right identity These are similar to the monoid laws, generalised for multiple types inside the monad. This sort of structure is called a category in mathematics. Relationship to ApplicativeAll Monad instances give rise to an Applicative instance, because we can define &lt;*&gt; in terms of &gt;&gt;=. 1mf &lt;*&gt; mx = mf &gt;&gt;= \\f -&gt; mx &gt;&gt;= \\x -&gt; pure (f x) This implementation is already provided for Monads as the ap function in Control.Monad Do notationWorking directly with the monad functions can be unpleasant. As we’ve seen, Haskell has some notation to increase niceness: 1234567do x &lt;- y becomes y &gt;&gt;= \\x -&gt; do z zdo x becomes x &gt;&gt;= \\_ -&gt; do y y Examples 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071{- (Dice Rolls) Roll two 6-sided dice, if the difference is &lt; 2, reroll the second die. Final score is the difference of the two die. What score is most common?-}roll :: [Int]roll = [1, 2, 3, 4, 5, 6]diceGame = do d1 &lt;- roll d2 &lt;- roll if (abs (d1 - d2) &lt; 2) then do d2' &lt;- roll pure (abs (d1 - d2')) else pure (abs (d1 - d2)) {- Partial Functions We have a list of student names in a database of type [(ZID, Name)]. Given a list of zID’s, return a Maybe [Name], where Nothing indicates that a zID could not be found-}db :: [(ZID, Name)]db = [(3253158, \"Liam\"), (8888888, \"Rich\"), (4444444, \"Mort\")]studentNames :: [ZID] -&gt; Maybe [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- lookup z db ns &lt;- studentNames zs pure (n:ns)-- briefer but less clear with applicative notation:-- studentNames (z:zs) = (:) &lt;$&gt; lookup z db &lt;*&gt; studentNames zs{- Arbitrary Instances Define a Tree type and a generator for search trees: searchTrees :: Int -&gt; Int -&gt; Generator Tree-}data Tree a = Leaf | Branch a (Tree a) (Tree a) deriving (Show, Eq)instance Arbitrary (Tree Int) where arbitrary = do mn &lt;- (arbitrary :: Gen Int) Positive delta &lt;- arbitrary let mx = mn + delta searchTree mn mx where searchTree :: Int -&gt; Int -&gt; Gen (Tree Int) searchTree mn mx | mn &gt;= mx = pure Leaf | otherwise = do v &lt;- choose (mn,mx) l &lt;- searchTree mn v r &lt;- searchTree (v+1) mx pure (Branch v l r)-- The Either MonadstudentNames :: [ZID] -&gt; Either ZID [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- case lookup z db of Just v -&gt; Right v Nothing -&gt; Left z ns &lt;- studentNames zs pure (n:ns) Static Assurance with TypesStatic AssureanceMethods of Assurance Static means of assurance analyse a program without running it. Static vs. Dynamic Static checks can be exhaustive. Exhaustivity An exhaustive check is a check that is able to analyse all possible executions of a program. However, some properties cannot be checked statically in general (halting problem), or are intractable to feasibly check statically (state space explosion). Dynamic checks cannot be exhaustive, but can be used to check some properties where static methods are unsuitable. Compiler IntegrationMost static and all dynamic methods of assurance are not integrated into the compilation process. You can compile and run your program even if it fails tests You can change your program to diverge from your model checker model. Your proofs can diverge from your implementation. Types Because types are integrated into the compiler, they cannot diverge from the source code. This means that type signatures are a kind of machine-checked documentation for your code. Types are the most widely used kind of formal verification in programming today. They are checked automatically by the compiler. They can be extended to encompass properties and proof systems with very high expressivity (covered next week). They are an exhaustive analysis Phantom Types A type parameter is phantom if it does not appear in the right hand side of the type definition. 1newtype Size x = S Int Lets examine each one of the following use cases: We can use this parameter to track what data invariants have been established about a value. We can use this parameter to track information about the representation (e.g. units of measure). We can use this parameter to enforce an ordering of operations performed on these values (type state). ValidationSuppose we have 123data UG -- empty typedata PGdata StudentID x = SID Int We can define a smart constructor that specialises the type parameter: 12sid :: Int -&gt; Either (StudentID UG) (StudentID PG) Define functions: 12enrolInCOMP3141 :: StudentID UG -&gt; IO ()lookupTranscript :: StudentID x -&gt; IO String Units of Measure123456789data Kilometresdata Milesdata Value x = U IntsydneyToMelbourne = (U 877 :: Value Kilometres)losAngelesToSanFran = (U 383 :: Value Miles)-- Note the arguments to area must have the same unitdata Square aarea :: Value m -&gt; Value m -&gt; Value (Square m)area (U x) (U y) = U (x * y) Type State123456789101112{- A Socket can either be ready to recieve data, or busy. If the socket is busy, the user must first use the wait operation, which blocks until the socket is ready. If the socket is ready, the user can use the send operation to send string data, which will make the socket busy again.-}data Busydata Readynewtype Socket s = Socket ...wait :: Socket Busy -&gt; IO (Socket Ready)send :: Socket Ready -&gt; String -&gt; IO (Socket Busy)-- assumption: use the socket in a linear way(only use once) Linearity and Type StateThe previous code assumed that we didn’t re-use old Sockets: 12345678910send2 :: Socket Ready -&gt; String -&gt; String -&gt; IO (Socket Busy)send2 s x y = do s' &lt;- send s x s'' &lt;- wait s' s''' &lt;- send s'' y pure s'''-- But we can just re-use old values to send without waiting:send2' s x y = do _ &lt;- send s x s' &lt;- send s y pure s' Linear type systems can solve this, but not in Haskell (yet) Datatype Promotion123data UGdata PGdata StudentID x = SID Int Defining empty data types for our tags is untyped. We can have StudentID UG, but also StudentID String. The DataKinds language extension lets us use data types as kinds: 1234567891011121314{-# LANGUAGE DataKinds, KindSignatures #-}data Stream = UG | PGdata StudentID (x :: Stream) = SID Intpostgrad :: [Int]postgrad = [3253158]makeStudentID :: Int -&gt; Either (StudentID UG) (StudentID PG)makeStudentID i | i `elem` postgrad = Right (SID i) | otherwise = Left (SID i)enrollInCOMP3141 :: StudentID UG -&gt; IO ()enrollInCOMP3141 (SID x) = putStrLn (show x ++ \" enrolled in COMP3141!\") GADTsUntyped Evaluator 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950data Expr t = BConst Bool | IConst Int | Times (Expr Int) (Expr Int) | Less (Expr Int) (Expr Int) | And (Expr Bool) (Expr Bool) | If (Expr Bool) (Expr t) (Expr t) deriving (Show, Eq)data Value = BVal Bool | IVal Int deriving (Show, Eq)eval :: Expr -&gt; Valueeval (BConst b) = BVal beval (IConst i) = IVal ieval (Times e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; IVal (i1 * i2)eval (Less e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; BVal (i1 &lt; i2) eval (And e1 e2) = case (eval e1, eval e2) of (BVal b1, BVal b2) -&gt; BVal (b1 &amp;&amp; b2) eval (If ec et ee) = case eval ec of BVal True -&gt; eval et BVal False -&gt; eval ee-- partial functioneval :: Expr -&gt; Maybe Valueeval (BConst b) = pure (BVal b)eval (IConst i) = pure (IVal i)eval (Times e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (IVal (v1' * v2')) _ -&gt; Nothingeval (Less e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (BVal (v1' &lt; v2')) _ -&gt; Nothingeval (And e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (BVal v1', BVal v2') -&gt; pure (BVal (v1' &amp;&amp; v2')) _ -&gt; Nothingeval (If ec et ee) = do v1 &lt;- eval ec case v1 of (BVal True) -&gt; eval et (BVal False) -&gt; eval ee GADTs Generalised Algebraic Datatypes (GADTs) is an extension to Haskell that, among other things, allows data types to be specified by writing the types of their constructors. 1234567{-# LANGUAGE GADTs, KindSignatures #-}-- Unary natural numbers, e.g. 3 is S (S (S Z))data Nat = Z | S Nat-- is the same asdata Nat :: * whereZ :: NatS :: Nat -&gt; Nat When combined with the type indexing trick of phantom types, this becomes very powerful! Typed Evaluator There is now only one set of precisely-typed constructors. 12345678910111213141516{-# LANGUAGE GADTs, KindSignatures #-}data Expr :: * -&gt; * where BConst :: Bool -&gt; Expr Bool IConst :: Int -&gt; Expr Int Times :: Expr Int -&gt; Expr Int -&gt; Expr Int Less :: Expr Int -&gt; Expr Int -&gt; Expr Bool And :: Expr Bool -&gt; Expr Bool -&gt; Expr Bool If :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr aeval :: Expr t -&gt; teval (IConst i) = ieval (BConst b) = beval (Times e1 e2) = eval e1 * eval e2eval (Less e1 e2) = eval e1 &lt; eval e2eval (And e1 e2) = eval e1 &amp;&amp; eval e2eval (If ec et ee) = if eval ec then eval et else eval ee ListsWe could define our own list type using GADT syntax as follows: 123456data List (a :: *) :: * whereNil :: List aCons :: a -&gt; List a -&gt; List a-- head (hd) and tail (tl) functions are partial hd (Cons x xs) = xtl (Cons x xs) = xs We will constrain the domain of these functions by tracking the length of the list on the type level. Vectors123456789101112131415161718192021222324252627282930313233{-# LANGUAGE GADTs, KindSignatures #-}{-# LANGUAGE DataKinds, StandaloneDeriving, TypeFamilies #-}data Nat = Z | S Natplus :: Nat -&gt; Nat -&gt; Nat plus Z n = nplus (S m) n = S (plus m n)type family Plus (m :: Nat) (n :: Nat) :: Nat where Plus Z n = n Plus (S m) n = S (Plus m n)data Vec (a :: *) :: Nat -&gt; * where Nil :: Vec a Z Cons :: a -&gt; Vec a n -&gt; Vec a (S n)deriving instance Show a =&gt; Show (Vec a n)appendV :: Vec a m -&gt; Vec a n -&gt; Vec a (Plus m n)appendV Nil ys = ysappendV (Cons x xs) ys = Cons x (appendV xs ys)-- 0: Z-- 1: S Z-- 2: S (S Z)hd :: Vec a (S n) -&gt; ahd (Cons x xs) = xmapVec :: (a -&gt; b) -&gt; Vec a n -&gt; Vec b nmapVec f Nil = NilmapVec f (Cons x xs) = Cons (f x) (mapVec f xs) TradeoffsThe benefits of this extra static checking are obvious, however: It can be difficult to convince the Haskell type checker that your code is correct, even when it is. Type-level encodings can make types more verbose and programs harder to understand. Sometimes excessively detailed types can make type-checking very slow, hindering productivity Pragmatism We should use type-based encodings only when the assurance advantages outweigh the clarity disadvantages. The typical use case for these richly-typed structures is to eliminate partial functions from our code base. If we never use partial list functions, length-indexed vectors are not particularly useful Theory of TypesLogic We can specify a logical system as a deductive system by providing a set of rules and axioms that describe how to prove various connectives. Natural Deduction A way we can specify logic Each connective typically has introduction and elimination rules. For example, to prove an implication A → B holds, we must show that B holds assuming A. This introduction rule is written as: More rulesImplication also has an elimination rule, that is also called modus ponens:$$\\frac{\\ulcorner \\vdash A\\rightarrow B \\space\\space\\space\\space\\space\\space\\space\\space\\ulcorner \\vdash A}{\\ulcorner \\vdash B}\\rightarrow-E$$Conjunction (and) has an introduction rule that follows our intuition:$$\\frac{\\ulcorner \\vdash A\\space\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner \\vdash B}{\\ulcorner \\vdash A\\land B}\\land-I_1$$It has two elimination rules:$$\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash A}\\land-E_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash B}\\land-E_2$$Disjunction (or) has two introduction rules:$$\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_2$$Disjunction elimination is a little unusual:$$\\frac{\\ulcorner \\vdash A\\lor B\\space\\space\\space\\space\\space\\space\\space A,\\ulcorner \\vdash P \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner \\vdash P}{\\ulcorner \\vdash P}\\lor-E$$The true literal, written T, has only an introduction:$$\\frac{}{\\ulcorner \\vdash \\top}$$And false, written ⊥, has just elimination (ex falso quodlibet):$$\\frac{\\ulcorner \\vdash \\bot}{\\ulcorner \\vdash P}$$Typically we just define：$$\\neg A \\equiv(A\\rightarrow\\bot)$$ Constructive LogicThe logic we have expressed so far does not admit the law of the excluded middle:$$P\\lor\\neg P$$Or the equivalent double negation elimination:$$(\\neg\\neg P)\\rightarrow P$$This is because it is a constructive logic that does not allow us to do proof by contradiction. Typed Lambda CalculusBoiling Haskell DownThe theoretical properties we will describe also apply to Haskell, but we need a smaller language for demonstration purposes. No user-defined types, just a small set of built-in types. No polymorphism (type variables) Just lambdas (λx.e) to define functions or bind variables. This language is a very minimal functional language, called the simply typed lambda calculus, originally due to Alonzo Church. Our small set of built-in types are intended to be enough to express most of the data types we would otherwise define. We are going to use logical inference rules to specify how expressions are given types (typing rules). Function TypesWe create values of a function type A → B using lambda expressions:$$\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$The typing rule for function application is as follows:$$\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2:: B}$$ Composite Data TypesIn addition to functions, most programming languages feature ways to compose types together to produce new types, such as: Classes, Tuples, Structs, Unions, Records… Product TypesFor simply typed lambda calculus, we will accomplish this with tuples, also called product types. (A, B) We won’t have type declarations, named fields or anything like that. More than two values can be combined by nesting products, for example a three dimensional vector:$$\\text{(Int, (Int, Int))}$$ Constructors and EliminatorsWe can construct a product type the same as Haskell tuples:$$\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}$$The only way to extract each component of the product is to use the fst and snd eliminators:$$\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}$$ Unit TypesCurrently, we have no way to express a type with just one value. This may seem useless at first, but it becomes useful in combination with other types. We’ll introduce the unit type from Haskell, written (), which has exactly one inhabitant, also written ():$$\\frac{}{\\ulcorner\\vdash ():: ()}$$ Disjunctive CompositionWe can’t, with the types we have, express a type with exactly three values. 1data TrafficLight = Red | Amber | Green In general we want to express data that can be one of multiple alternatives, that contain different bits of data. 12345type Length = Inttype Angle = Intdata Shape = Rect Length Length | Circle Length | Point | Triangle Angle Length Length Sum TypesWe’ll build in the Haskell Either type to express the possibility that data may be one of two forms.$$\\text{Either } A \\space B$$These types are also called sum types. Our TrafficLight type can be expressed (grotesquely) as a sum of units:$$\\text{TrafficLight } \\simeq \\text{Either () (Either () ())}$$ Constructors and Eliminators for SumsTo make a value of type Either A B, we invoke one of the two constructors:$$\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}$$We can branch based on which alternative is used using pattern matching:$$\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}$$Example Our traffic light type has three values as required:$$\\begin{align*}\\text{TrafficLight } &amp;\\simeq \\text{Either () (Either () ())}\\\\\\text{Red } &amp;\\simeq \\text{Left ()}\\\\\\text{Amber } &amp;\\simeq \\text{Right (Left ())}\\\\\\text{Green } &amp;\\simeq \\text{Right (Right (Left ()}\\\\\\end{align*}$$ The Empty TypeWe add another type, called Void, that has no inhabitants. Because it is empty, there is no way to construct it. We do have a way to eliminate it, however:$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}$$If I have a variable of the empty type in scope, we must be looking at an expression that will never be evaluated. Therefore, we can assign any type we like to this expression, because it will never be executed. Gathering Rules$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ():: ()}\\\\\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2::B}\\space\\space\\space\\space\\space\\space\\space\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$ Removing Terms. . .$$\\frac{\\ulcorner\\vdash\\text{Void}}{\\ulcorner\\vdash P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ()}\\\\\\frac{\\ulcorner\\vdash A}{\\ulcorner\\vdash\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash B}{\\ulcorner\\vdash\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash \\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space A,\\ulcorner\\vdash P\\space \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner\\vdash P}{\\ulcorner\\vdash P}\\\\\\frac{\\ulcorner\\vdash A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash B}{\\ulcorner\\vdash(A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash A}\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash B}\\\\\\frac{\\ulcorner\\vdash A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash A}{\\ulcorner\\vdash B}\\space\\space\\space\\space\\space\\space\\space\\frac{ A, \\ulcorner\\vdash B}{\\ulcorner\\vdash A\\rightarrow B}$$ This looks exactly like constructive logic! If we can construct a program of a certain type, we have also created a proof of a program The Curry-Howard CorrespondenceThis correspondence goes by many names, but is usually attributed to Haskell Curry and William Howard. It is a very deep result: Programming Logic Types Propositions Programs Proofs Evaluation Proof Simplification It turns out, no matter what logic you want to define, there is always a corresponding λ-calculus, and vice versa. λ-calculus Logic Typed λ-Calculus Constructive Logic Continuations Classical Logic Monads Modal Logic Linear Types, Session Types Linear Logic Region Types Separation Logic TranslatingWe can translate logical connectives to types and back: Types logical connectives Tuples Conjuction($\\land$) Either Disjunction (∨) Functions Implication () True Void False We can also translate our equational reasoning on programs into proof simplification on proofs! Proof SimplificationAssuming A $∧$ B, we want to prove B $∧$ A. We have this unpleasant proof: Translating to types, we get: Assuming x :: (A, B), we want to construct (B, A). We know that$$\\text{(snd x, snd (fst x, fst x)) = (snd x, fst x)}$$Assuming x :: (A, B), we want to construct (B, A). Back to logic: ApplicationsAs mentioned before, in dependently typed languages such as Agda and Idris, the distinction between value-level and type-level languages is removed, allowing us to refer to our program in types (i.e. propositions) and then construct programs of those types (i.e. proofs). Generally, dependent types allow us to use rich types not just for programming, but also for verification via the Curry-Howard correspondence. CaveatsAll functions we define have to be total and terminating. Otherwise we get an inconsistent logic that lets us prove false things. Most common calculi correspond to constructive logic, not classical ones, so principles like the law of excluded middle or double negation elimination do not hold. Algebraic Type IsomorphismSemiring Structure These types we have defined form an algebraic structure called a commutative semiring Laws for Either and Void: Associativity: Either (Either A B) C $\\simeq$ Either A (Either B C) Identity: Either Void A $\\simeq$ A Commutativity: Either A B $\\simeq$ Either B A Laws for tuples and 1: Associativity: ((A, B), C) $\\simeq$ (A,(B, C)) Identity: ((), A) $\\simeq$ A Commutativity: (A, B) $\\simeq$ (B, A) Combining the two: Distributivity: (A, Either B C) $\\simeq$ Either (A, B) (A, C) Absorption: (Void, A) $\\simeq$ Void What does $\\simeq$ mean here? It’s more than logical equivalence. Distinguish more things. Isomorphism Two types A and B are isomorphic, written A $\\simeq$ B, if there exists a bijection between them. This means that for each value in A we can find a unique value in B and vice versa. Example: 12data Switch = On Name Int | Off Name Can be simplified to the isomorphic (Name, Maybe Int). Generic Programming Representing data types generically as sums and products is the foundation for generic programming libraries such as GHC generics. This allows us to define algorithms that work on arbitrary data structures. Polymorphism and ParametrictyType QuantifiersConsider the type of fst: 1fst :: (a,b) -&gt; a This can be written more verbosely as: 1fst :: forall a b. (a,b) -&gt; a Or, in a more mathematical notation:$$\\text{fst}::\\forall a\\space b(a, b)\\rightarrow a$$This kind of quantification over type variables is called parametric polymorphism or just polymorphism for short. (It’s also called generics in some languages, but this terminology is bad) Curry-HowardThe type quantifier ∀ corresponds to a universal quantifier ∀, but it is not the same as the ∀ from first-order logic. What’s the difference? First-order logic quantifiers range over a set of individuals or values, for example the natural numbers:$$\\forall x. x+1 &gt;x$$These quantifiers range over propositions (types) themselves. It is analogous to second-order logic, not first-order:$$\\forall A.\\forall B\\space A\\land B \\rightarrow B\\land A\\\\\\forall A.\\forall B\\space (A, B) \\rightarrow (B, A)$$The first-order quantifier has a type-theoretic analogue too (type indices), but this is not nearly as common as polymorphism. Generality A type A is more general than a type B, often written A $\\sqsubseteq$ B, if type variables in A can be instantiated to give the type B. If we need a function of type Int → Int, a polymorphic function of type ∀a. a → a will do just fine, we can just instantiate the type variable to Int. But the reverse is not true. This gives rise to an ordering. Example$$\\text{Int}\\rightarrow\\text{Int} \\sqsupseteq \\forall z. z\\rightarrow z\\sqsupseteq \\forall x\\space y. x\\rightarrow y\\sqsupseteq\\forall a. a$$ Constraining ImplementationsHow many possible total, terminating implementations are there of a function of the following type? Many$$\\text{Int}\\rightarrow\\text{Int}$$How about this type? 1$$\\forall a. a\\rightarrow a$$ Polymorphic type signatures constrain implementations. Parametricity The principle of parametricity states that the result of polymorphic functions cannot depend on values of an abstracted type. More formally, suppose I have a polymorphic function g that is polymorphic on type a. If run any arbitrary function f :: a → a on all the a values in the input of g, that will give the same results as running g first, then f on all the a values of the output. Example:$$foo :: ∀a. [a] → [a]$$We know that every element of the output occurs in the input. The parametricity theorem we get is, for all f :$$\\text{foo }\\circ(\\text{map f}) = (\\text{map f})\\circ \\text{foo}$$ $$head :: ∀a. [a] → a\\\\\\text{ f (head l) = head (map f l)}$$ $$(++) :: ∀a. [a] → [a] → [a]\\\\\\text{map f (a ++ b) = map f a ++ map f b}$$ $$concat :: ∀a. [[a]] → [a]\\\\\\text{map f (concat ls) = concat (map (map f ) ls)}$$ Higher Order Functions$$filter :: ∀a. (a → Bool) → [a] → [a]\\\\\\text{filter p (map f ls) = map f (filter (p ◦ f ) ls)}$$ Parametricity TheoremsFollow a similar structure. In fact it can be mechanically derived, using the relational parametricity framework invented by John C. Reynolds, and popularised by Wadler in the famous paper, “Theorems for Free!”1 . Upshot: We can ask lambdabot on the Haskell IRC channel for these theorems.","link":"/2020/08/01/Haskell/"},{"title":"Operating Systems","text":"The more you know about operating system, the better you can cooperate with it. Operating System OverviewRoleRole 1 The Operating System is an Abstract Machine Extends the basic hardware with added functionality Provides high-level abstractions More programmer friendly Common core for all applications It hides the details of the hardware Makes application code portable Role 2 The Operating System is a Resource Manager Responsible for allocating resources to users and processes Must ensure No Starvation Progress Allocation is according to some desired policy First-come, first-served; Fair share; Weighted fair share; limits (quotas), etc… Overall, that the system is efficiently used Structural (Implementation) View the Operating System is the software Privileged mode. Operating System Kernel Portion of the operating system that is running in privileged mode Usually resident (stays) in main memory Contains fundamental functionality Whatever is required to implement other services Whatever is required to provide security Contains most-frequently used functions Also called the nucleus or supervisor The Operating System is Privileged Applications should not be able to interfere or bypass the operating system OS can enforce the “extended machine” OS can enforce its resource allocation policies Prevent applications from interfering with each other The Structure of a Computer System Applications interact with themselves and via function calls to library procedures OS and application interacts via System calls A Note on System Libraries System libraries are just that, libraries of support functions (procedures, subroutines) Only a subset of library functions are actually system calls, and system call functions are in the library for convenience Privilege-less OS Can implement OS functionality, but cannot enforce it. Some Embedded OSs have no privileged component All software runs together No isolation One fault potentially brings down entire system Operating System Software Fundamentally, OS functions the same way as ordinary computer software. It is machine code that is executed (same machine instructions as application). It has more privileges (extra instructions and access). Operating system relinquishes control of the processor to execute other programs, it reestablishes control after System calls and Interrupts (especially timer interrupts). Operating System Internal Structure The Monolithic Operating System Structure, Also called the “spaghetti nest” approach(Everything is tangled up with everything else. ) Processes and ThreadsMajor Requirements of an Operating System Interleave the execution of several processes to maximize processor utilization while providing reasonable response time Allocate resources to processess Support interprocess communication and user creation of processes Processes and ThreadsProcesses Also called a task or job Execution of an individual program “Owner” of resources allocated for program execution trace the usage of processes, clean up memory after finishing execution Encompasses one or more threads Threads The sequence of execution through an application Unit of execution Can be traced list the sequence of instructions that execute Belongs to a process Process provide environment, and all thread running in the process share the environment Executes within it ProcessThe Process ModelSingle process machine Multiprogramming of four programs Conceptual model of 4 independent, sequential processes(with a single thread each) Only one program active at any instant the box represents a process, and the line represents the thread and the dotted lines represent what the OS can do Process and thread models of selected OSes Single process, single thread MSDOS Signle process, multiple threads OS/161 as distributed Multiple processes, single thread Traditional UNIX Multiple processes, multiple threads Modern Unix(Linux, Solaris), Windows Process CreationPrincipal events that cause process creation System initialization Foreground processes(interactive programs) Background processes Email server, web server, print server, etc. Called a daemon(unix) or service(Windows) Execution of a process creation system call by running a process New login shell for an incoming ssh connection User request to create a new process Initiation of a bach job Note: Technically, all these cases use the same system machanism to create new processes Process TerminationConditions which terminate processes Normal exit(voluntary) Error exit(voluntary) Fatal error(involuntary) e.g. segmentation fault Killed by another process(involuntary) Implementation of Processes A processes’ information is stored in a process control block(PCB) The PCBs form a process table Reality can be complex(hashing, chaining, allocation bitmaps,…) Size can be dynamic Example fields of a process table entry Process Management Memory management File management Registers Pointer to text segment Root directory Program counter Pointer to data segment working directory Program status word Pointer to stack segement File descriptors Stack pointer User ID Process state Group ID Priority Scheduling parameters Process ID Parent process process group Signals time when process started CPU time used Children’s CPU time Time of next alarm Process/Thread StatesThree states process model(the minial model the OSes implement) In the image the numbers prepresent: Process blocks for input Scheduler picks another process Scheculer picks this process Input becomes available Possible process/thread states running blocked ready Generally, if no IO, processes almost all sitting in running and ready Transitions between states shown some Transition Causing Events Running -&gt; Ready Voluntary Yield() End of timeslice Running -&gt; Blocked Waiting fot input File, network, Waiting for a timer(alarm signal) Waiting for a resource to become available Scheduler responsible for want to run next Sometimes alse called the dispatcher The literature is alse a little inconsistent on with terminology Has to choose a Ready process to run It is inefficent to search through all processes The Ready Queue What about blocked processes When an unblocking event occurs, we also wish to avoid scanning all processes to select one to make Ready using two queues(One option) another option(using a queue for every event) Implementation Minimally consist of three segments Text contains the code (instructions) Data Global variables Stack Activation records of procedure/function/method Local variables User-mode Processes (programs) scheduled by the kernel Isolated from each other No concurrency issues between each other System-calls transition into and return from the kernel Kernel-mode Nearly all activities still associated with a process Kernel memory shared between all processes Concurrency issues exist between processes concurrently executing in a system call ThreadThe Thread Model (a) Three processes each with one thread(b) One process with three thread Separating execution from the environment Per process items Per thread items Address space Program counter Global variables Registers Open files Stack Child processess State Pending alarms Signals and signal handlers Accounting information Items shared by all threads in a process Item private to each thread The Thread Model State implicitly stored on the stack Local variables are per thread Allocated on the stack Don’t have concurrency issues Global variables are shared between all threads Allocated in the data section Concurrency control is an issue Dynamically allocated memory(malloc) can be glocal or local Program defined(the pointer can be global or local) If the pointer is global variable(has concurrency issue) while locals don’t Thread UsageExample 1234567891011121314// dispatcher threadWhile(TRUE) { get_next_request(&amp;buf); handoff_work(&amp;buf);}// worker thread - can overlap disk I/O with the execution of other threadswhile (TRUE) { wait_for_work(&amp;buf); look_for_page_in_cache(&amp;buf, &amp;page); if (page_not_in_cache(&amp;page) read_page_from_disk(&amp;buf, &amp;page); return_page(&amp;page);} Model Characteristics Threads Parallelism, blocking system calls Sigle-threaded process No parallelism, blocking system calls Finite-state machine Parallelism, nonblocking system calls, interrupts Why Threads? Simpler to program than a state machine Less resources are associated with them a complete process Cheaper to create and destory Shares resources(especially memory) between them Performace: Threads waiting for I/O can be overlapped with computing threads Note if all threads are compute bound, then there is no performance improvement(on a uniprocessor) Threads can take advantage of the parallelism available on machines with more than one CPU(multiprocessor) User-level Threads Implementation at user-level User-level Thread Control Block (TCB), ready queue, blocked queue, and dispatcher Kernel has no knowledge of the threads (it only sees a single process) If a thread blocks waiting for a resource held by another thread inside the same process, its state is saved and the dispatcher switches to another ready thread Thread management (create, exit, yield, wait) are implemented in a runtime support library Pros Thread management and switching at user level is much faster than doing it in kernel level No need to trap (take syscall exception) into kernel and back to switch Dispatcher algorithm can be tuned to the application Can be implemented on any OS (thread or non-thread aware) Can easily support massive numbers of threads on a per-application basis Use normal application virtual memory Kernel memory more constrained. Difficult to efficiently support wildly differing numbers of threads for different applications. Cons Threads have to yield() manually (no timer interrupt delivery to userlevel) Co-operative multithreading A single poorly design/implemented thread can monopolise the available CPU time There are work-arounds (e.g. a timer signal per second to enable preemptive multithreading), they are course grain and a kludge. Does not take advantage of multiple CPUs (in reality, we still have a single threaded process as far as the kernel is concerned) If a thread makes a blocking system call (or takes a page fault), the process (and all the internal threads) blocks Can’t overlap I/O with computation Kernel-level threads Cons Thread creation and destruction, and blocking and unblocking threads requires kernel entry and exit. More expensive than user-level equivalent Pros Preemptive multithreading Parallelism Can overlap blocking I/O with computation Can take advantage of a multiprocessor Context Switch A context switch can refer to a switch between threads, involving saving and restoring of state associated with a thread; A switch between processes, involving the above, plus extra state associated with a process. Context Switch OccurrenceA switch between process/threads can happen any time the OS is invoked On a system call Mandatory if system call blocks or on exit() On an exception Mandatory if offender is killed On an interrupt Triggering a dispatch is the main purpose of the timer interrupt A thread switch can happen between any two instructions Note instructions do not equal program statements Context Switch Context switch must be transparent for processes/threads When dispatched again, process/thread should not notice that something else was running in the meantime (except for elapsed time) OS must save all state that affects the thread This state is called the process/thread context Switching between process/threads consequently results in a context switch. Concurrency and SynchronisationConcurrency Example 12345678910111213141516// count is a global variable shared between two threadsint count = 0;void increment () { int t; t = count; t = t + 1; count = t;{void decrement (){ int t; t = count; t = t - 1; count = t;} Have a race condition. Occurred when global variables shared by threads There is in-kernel concurrency even for single-threaded processThe OS has to deal with concurrency even if it is a single thread application. Critical Region We can control access to the shared resource by controlling access to the code that accesses the resource A critical region is a region of code where shared resources are accessed(e.g. Variables, memory, files, etc…) Uncoordinated entry to the critical region results in a race condition =&gt; Incorrect behavior, deadlock, lost work,… Indentifying critical regions Critical regions are regions of code that: Access a shared resource, and correctness relied on the shared resource not being concurrently modified by another thread/process/entity. 12345678910111213141516// count is a global variable shared between two threadsint count = 0;void increment (){ int t; t = count; // the start of critical region t = t + 1; count = t; // end{void decrement (){ int t; t = count; // start t = t - 1; count = t; // end} if we can make the critical regions never overlap, we won’t have concurrency problem. Example critical regions 1234567891011121314151617181920212223242526struct node { int data; struct node *next;}struct node *head;void init(void) { head = NULL;}void insert(struct *item){ item-&gt;next = head; head = item;}struct node *remove(void) { struct node *t; t = head; if (t != NULL) { head = head-&gt;next; } return t;} Race example 12345678910111213// thread 1void insert(struct *item){ item-&gt;next = head; // 1 head = item; // 2}// thread 2void insert(struct *item){ item-&gt;next = head; // 3 head = item; // 4} one possible sequence: 1 -&gt; 3 -&gt; 2 -&gt; 4, then the item in thread 1 is lost(seg fault)both1-2 and 3-4 are critical regions Critical regions solutions We seek a solution to coordinate access to critical regions Also called critical sections Conditions required of any solution to the critical region problem Mutual Exclusion: No two processes simultaneously in critical region No assumptions made about speeds or numbers of CPUs Progress No process running outside its critical region may block another process Bounded No process waits forever to enter its critical region A solution? A lock variable if lock == 1, somebody is in the critical section and we mush wait if lock == 0, nobody is in the critical section and we are free to enter 1234567891011121314151617181920// A problematic execution sequencewhile (TRUE) { while (lock == 1); // 1 lock = 1; // 2 critical(); // 3 lock = 0; // 4 non_critical(); // 5}while (TRUE) { while (lock == 1); // 6 lock = 1; // 7 critical(); // 8 lock = 0; // 9 non_critical(); // 10}/* does not work because if it execute like* 6 -&gt; 1 -&gt; 2 -&gt; 3* both of them see the lock is not one and may all work in their critical section*/ In the example above, there is race contition between the observation that the lock variable is not one and setting the lock to prevent somebody else from entering. Mutual Exclusion by Taking Turns 123456789101112while (TRUE) { while (turn != 0); critcal_rigion(); turn = 1; noncritical_region();}while (TRUE) { while (turn != 1); critcal_rigion(); turn = 0; noncritical_region();} This example works because the update is only been updated by the thread having its turn. Works due to strict alternation Each process takes turns Cons Busy waiting(loop) sol: not my turn, do sth else, come back to my turn Process must wait its turn even while the other process is doing something else. With many processes, must wait for everyone to have a turn Does not guarantee progress if a process no longer needs a turn Poor solution when processes require the critical section at differing rates Mutual Exclusion by Disabling Interrupts only used in very short critical sections, because it prespond other devices in the machine and make them less responsive Before entering a critical region, disable interrupts After leaving the critical region, enable interrupts Pros simple Cons Only available in the kernel Blocks everybody else, even with no contention slows interrupt response time Does not work on a multiprocessor Hardware Support for mutual exclusion Test and set instruction can be used to implement lock variables correctly It loads the value of the lock If lock == 0, set the lock to 1 return the result 0 – we acquire the lock If lock == 1 return 1 – another thread/process has the lock Hardware guarantees that the instruction executes atomically(atomically: as an indivisible unit) 123456789enter_region: TSL REFISTER,LOCK | copy lock to register and set lock to 1 CMP REGISTER, #0 | was lock one? JNE enter_region | if it was non zero, lock was set, so loop RET | return to caller; critical region enteredleave_region: MOVE LOCK, #0 | store a 0 in lock RET | return to caller Pros Simple(easy to show it’s correct) Available at user-level To any number of processors To implement any number of lock variables Cons Busy waits(also termed a spin lock) consumes CPU starvation is possible when a process leaves its critical section and more than one process is waiting Tackling the Busy-Wait Problem Sleep / Wake up The idea When process is waiting for an event, it calls sleep to block, instead of busy waiting The event happens, the event generator(another process) calls wakeup to unblock the sleeping process. Walking a ready/running process has no effect The producer-Consumer Problem Also called the bounded buffer problemA producer produces data items and stores the items in a bufferA consumer takes the items out of the buffer and consumes them. two problems: Concurrency(because shared resource) and the producer should be blocked when the buffer is full and the consumer should be blocked when the buffer is empty. We must keep accurate count of items in the buffer The consumer can call wakeup when it consumes the first entry of the full buffer The Producer can call wakeup when it adds the first item to the buffer Semaphores Dijkstra introduced two primitives that are more powerful than simple sleep and wakeup alone. P(): proberen, from Dutch to test V(): verhogen, from Dutch to increment Also called wait &amp; signal, down &amp; up How do they work If a resource is not available, the corresponding semaphore blocks any process waiting for the resource Blocked processes are put into a process queue maintained by the semaphore(avoids busy waiting) When a process releases a resource, it signals this by means of the semaphore Signalling resumes a blocked process if there is any Wait(P) and signal(V) operations cannot be interrupted Complex coordination can be implemented by multiple semaphores Semaphore implementation Define a semaphore as a record Each primitive is atomit(wait and signal), the OS would use the disabling of interrupts in the implementation of Semaphore inside the OS. 1234567891011121314151617typedef sturct { int count; struct process *L;} semaphore;// Semaphore operations now defined aswait(S): S.count --; if (S.count &lt; 0) { add this process to S.L; sleep; }signal(S): S.count++; if (S.count &lt;= 0) { remove a process P from S.L; wakeup(P); } Semaphore as a General Synchronization Tool Execute B in Pj only after A executed in Pi Use semaphore count initlialized to 0 Code: A first: signal(count is 1) -&gt; then B runs(count is 0) -&gt; A(this is what we want) B first: wait(count &lt; 0, sleep) -&gt; A runs, wake up B(still what we want) Semaphore implementation of a Mutex Mutex is short for Mutual Exclusion Can also be called a lock 12345semaphore mutex;mutex.count = 1; /* init the mutex */wait(mutex); /* enter the critical region */Blahblah();signal(mutex); /* exit the critical region */ Notice that the inital count determines how mant waits can progress before blocking and requiring a signal =&gt; mutex.count initialised as 1 Solve the producer-consumer problem 1234567891011121314151617181920212223242526272829#define N = 4;semaphore mutex = 1;/* count empty slots */semaphore empty = N/* count full slots */semaphore full = 0;prod() { while(TRUE) { item = produce() wait(empty); wait(mutex) insert_item(); signal(mutex); signal(full); }}con() { while(TRUE) { wait(full); wait(mutex); remove_item(); signal(mutex); signal(empty); }} Summary Semaphores can be used to solve a varity of concurrency problems However, programming with then can be error-prone E.g. must signal for every wait for mutexes Too many or few signals or waits, or signals and waits in the wrong order, can have catastrophic results Monitors To ease concurrent programming, Hoare(1974) proposed monitors A higher level synchronisation primitive Programming language construct Idea A set of procedures, variables, data types are grouped in a special kind of module, a monitor. Variables and data types only accessed from within the monitor Only one process/thread can be in the monitor at any one time Mutual exclusion is implemented by the compiler(which should be less error prone) When a thread calls a monitor procedure that has a thread already inside, it is queued and it sleeps until the current thread exits the monitorexample 12345678910111213141516monitor exmaple integer i; condition c; procedure producer(); . . . end; procedure consumer(); . . . end;end monitor; How do we block waiting for an event? We need a mechanism to block waiting for an event( in addition to ensuring mutual exclusion) for producer consumer problem, when buffer is empty or full Condition variables To allow a process to wait within the monitor, a condition variable must be declared, as condition x, y Condition variable can only be used with the operations wait and sigal The operation x.wait(); means that the process invoking this operation is suspended until another process invokes Another thread can enter the monitor while original is suspended x.signal(); The operation resumes exactly on suspended process. If no process is suspended, then the signal opeartion has no effect. The Readers and Writers Problem Models access to a data base E.g. airline reservation system Can have more thant one concurrent reader To check schedules and reservations Writers must have exclusive access To book a ticket or update a schedule A solution 12345678910111213141516171819202122232425262728typedef in semaphore; /* use your imagination */semaphore mutex = 1; /* controls access to 'rc'*/semaphore db = 1; /* controls access to the database */int rc = 0; /* # of processes reading ot wanting to */void reader(void) { while(TRUE) { /* repeat forever */ down(&amp;mutex); /* get exclusive access to 'rc' */ rc = rc + 1; /* one reader more now */ if(rc == 1) down(&amp;db); /* if this is the first reader */ up(&amp;mutex); /* release exclusive access to 'rc' */ read_data_base(); /* access the data */ down(&amp;mutex); /* get exclusive access to 'rc' */ rc = rc - 1; /* one reader fewer now */ if(rc == 0) up(&amp;db); /* if this is the last reader */ up(&amp;mutex); /* release exclusive access to 'rc */ use_data_read(); /* noncritical region */ } void writer(void) { while (TRUE) { /* repeat forever */ think_up_date(); /* noncritical regionn */ down(&amp;db); /* get exclusive access */ wirte_data_base(); /* update the data */ up(&amp;db); /* release exclusive access */ } }} DeadlockResources Examples of computer resources printers tape drives Tables in a database Processes need access to resources in reasonable order Preemptable resources e.g. virtual memory can be taken away from a process with no ill effects Nonpreemtable resources will cause the process to fail if take away Resources &amp; Deadlocks Suppose a process holds resource A and requests resource B at same time another process holds B and requests A both are blocked and remain so – Deadlock Deadlocks occur when … processes are granted exclusive access to devices, locks, tables, etc.. we refer to these entities generally as resources Resource Access Sequence of events required to use a resource request the resource use the resource release the resource Must wait if request is denied requesting process may be blocked may fail with error code Deadlock A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause. Usually the event is release of a currently held resource None of the processes can … run release resources be awakened Four Conditions for Deadlock Mutual exclusion condition each resource assigned to 1 process or is available Hold and wait condition process holding resources can request additional No preemption condition previously granted resources cannot be forcibly taken away Circular wait condition must be a circular chain of 2 or more processes each is waiting for resource held by next member of the chain Strategies for dealing with Deadlocks Just ignore the problem altogether prevention ​ Negating one of the four necessary conditions Detection and recovery Dynamic avoidance ​ Careful resource allocation Appoarch 1: The Ostrich Algorithm Pretend there is no problem Reasonable if deadlocks occur very rarely Cost of prevention is high Example of “cost”, only one process runs at a time UNIX and Windows takes this approach for some of the more complex resource relationships they manage It’s a trade off between Convenience (engineering approach) Correctness(mathematical approach) Approach 2: Deadlock Prevention The most common used strategy Resource allocation rules prevent deadlock by prevent one of the four condition required for deadlock from occuring Mutual exclusion Not feasible in general Some devices/resource are intrinsically not shareable Hold and wait Require processes to request resources before starting a process never has to wait for what it needs Issues may not know required resources at start of run must be determined at run time or by user input(e.g. using Word to open a file) $\\Rightarrow$ not always possible Also ties up resources other processes could be using Varitions: Process must give up all resources if it would block holding a resource then request all immediately needed prone to livelock Livelocked processes are not blocked, change state regularly, but never make progress. No preemption This is not a viable option Circular Wait aquire resource in the same order, order resources Approach 3: Detection and Recovery Need a method to determine if a system is deadlocked Assuming deadlocked is detected, we need a method of recorvery to restore progress to the system. Modeling resources and processes as a directed graph (a) represents everthing in the system (b) is a deadlock What about resources with multiple units? Some examples of multi-unit resources RAM Blocks on a hard disk drive Slots in a buffer We need an approach for dealing with resources that consists of more than a single unit Modeling using matrix Data structured needed by deadlock detection algorithm sum of current resource allocation + resources available = resources that exist$$\\sum_{i=1}^{n}{C_{ij}}+A_j = E_j$$Example Process 1: has 1 scanners Process 2: has 2 tape drivers and 1 CD roms Process 3: has 1 plotters and 2 scanners Detection Algoritm Look for an unmarked process $Pi$, for which the $i$-th row of R is less than equal to A If found, add the $i$-th row of C to A, and mark $Pi$. Got to step 1 If no such process exists, terminate Remaining processes are deadlocked. Recovery from Deadlock Recovery through preemption Take a resource from some other process Depends on nature of the resource Recovery through rollback Checkpoint a process periodically not come for free, not practical Use this saved state Restart the process if it is found deadlocked No guarantee is won’t deadlock again Recovery through killing processes Crudest but simplest way to break a deadlock Kill one of the processes in the deadlock cycle The other processes get its resources Choose process that can be rerun from the beginning Approach 4 Deadlock Avoidance Not practical for all systems, need to know enough information in advance Safe and Unsafe States A state is safe if The system is not deadlocked There exists a scheduling order that results in every process running to completion, even if they all request their maximum resources immediately Unsafe states are not necessarily deadlocked with a lucky sequence, all processes may complete However, we cannot guarantee that they will complete(not deadlock) Safe states guarantee we will eventually complete all processes Deadlock avoidance algorithm Only grant requests that result in safe states Bankers Algorithm Modelled on a Banker with Customers The banker has a limited amount of money to loan customers Limited number of resources Each customer can borrow money up to the customer’s credit limit Maximum number of resources required Basic idea keep the bank in a safe state So all customers are happy even if they all request to borrow up to their credit limit at the same time Customers wishing to borrow such that the back would enter an unsafe state must wait until somebody else repays their loan such that the transaction becomes safe. Bankers Algorithm is not commonly used in practice It is difficult (sometimes impossible ) to know in advance the resources a process will require the number of processes in a dynamic system Starvation A process never receives the resource it is waiting for, despite the resource(repeatedly) becoming free, the resource is always allocated to another waiting process. One solution: First-come, first-serve policy System CallSystem Calls Can be viewed as special function calls Provides for a controlled entry into the kernel While in kernel, they perform a privileged operation Returns to original caller with the result The system call interface represents the abstract machine provided by the operating system. A brief overviewFrom the user’s perspective Process Management File I/O Dirctories management Some other selected Calls There are many more On Linux, see man syscalls for a list Process Management Call Descritption pid=fork() Create a child process identical to the parent Pid = waitpid(pid, &amp;statloc, options) Wait for a child to terminate s = execve(name, argv, environp) Replace a process’ core image exit(status) Terminate process execution and return status File Management Call Descrition fd=open(file, how, …,) Open a file for reading, writing or both s = close(fd) Close an open file n = read(fd, buffer, bytes) Read data from a file into a buffer n = write(fd, buffer, bytes) Write data from a buffer into a file Position = lseek(fd, offset, whence) Move the file pointer s = stat(name, &amp;buf) Get a file’s status information A stripped down shell:1234567891011while(TRUE) { /* repeat forever */ type_prompt(); /* display prompt */ read_command(command, parameters); if (fork() != 0) { /* fork off child process*/ /* Parent code */ waitpid(-1, &amp;status, 0); /* wait for child to exit */ } else { /* Child code */ execve(command, parameters, 0); /* execute command */ }} System Call ImplementationA Simple Model of CPU ComputationThe fetch-execute cycle Load memory contents from address in program counter (PC) The instruction Execute the instruction increment PC Repeat A Simple Model of CPU Computation Stack Pointer(SP) Status Register Condition codes Positive result Zero result Negative result General Purpose Register Holds operands of most instructions Enables programmers (compiler) to minmise memory reference Privileged-mode OperationTo protect operating system execution, two or more CPU modes of operation exist Privileged mode(system, kernel-mode) All instructions and register are available User-mode Users ‘safe’ subset of instruction set Only affects the state of the application itself They cannot be used to uncontrollably interference with OS Only ‘safe’ registers are accessible System CallSystem Call Mechanism Overview System call transitions triggered by special processor instructions User to Kernel System call instruction Kernel to User Return from privileged mode instruction Processor mode Switched from user-mode to kernel-mode Switched back when returning to user mode Stack Pointer (SP) User-level SP is saved and a kernel SP is initialised User-level SP restored when returning to user-mode Program Counter (PC) User-level PC is saved and PC set to kernel entry point User-level PC restored when returning to user-level Kernel entry via the designated entry point must be strictly enforced Registers Set at user-level to indicate system call type and its arguments A convention between applications and the kernel Some registers are preserved at user-level or kernel-level in order to restart user-level execution Depends on language calling convention etc Result of system call placed in registers when returning to user-level Another convention Why do we need system calls? Why not simply jump into the kernel via a function call???? Function calls do not Change from user to kernel mode and eventually back again Restrict possible entry points to secure locations To prevent entering after any security checks Steps in Making a System Call File ManagementOverview of the FS abstraction User’s view Under the hood Uniform namespace Heterogenrous collection of storage devices Hierachical structure Flat address space(block numbers) Arbitrarily-sized files Fixed-size blocks Symbolic file names Numeric block addresses Access control No access control Tools for (formatting, defragmentation, Backup, Consistency checking) File NamesFile system must provide a convenient naming scheme Textual Names May have restrictions Only certain characters Limited length Only certain format Case (in)sensitive Names may obey conventions Interpreted by tools interpreted by operating system File StructureSequence of Bytes OS consider a file to be unstructured Applications can impose their own structure Used by UNIX, Window, most modern OSes File TypesRegular File Directories Device Files May be divided into Character Devices – stream of bytes Block Devices Some systems distinguish between regular file types ASCII text files, binary files File Access Types(Patterns)Sequential access read all bytes/records from the beginning cannot jump around, could rewind or back up convenient when medium was magnetic tape Random access bytes/records read in any order essential for data base systems read can be … move file pointer (seek), then read or lseek(location,…);read(…) each read specifies the file pointer read(location,…) 11 Typical File OperationsCreate Delete Open Close Read Write Append Rename Seek Get attributes Set Attributes File Organisation and Access(Programmer’s Perspective) Given an operating system supporting unstructured files that are a stream-of-bytes,how can one organise the contents of the files? E.g. Executable Linkable Format (ELF) Some possible access patterns: Read the whole file –Read individual records from a file record = sequence of bytes containing the record Read records preceding or following the current one Retrieve a set of records Write a whole file sequentially Insert/delete/update records in a file Programmers are free to structure the file to suit the application Criteria for File OrganizationThings to consider when designing file layout Rapid access Needed when accessing a single record Not needed for batch mode read from start to finish Ease of update File on CD-ROM will not be updated, so this is not a concern Economy of storage Should be minimum redundancy in the data Redundancy can be used to speed access such as an index File DirectoriesProvide mapping between file names and the files themselves Contain information about files Attributes Location Ownership Directory itself is a file owned by the operating system Hierarchical (Tree-Structured) DirectoryFiles can be located by following a path from the root, or master, directory down various branches This is the absolute pathname for the file Can have several files with the same file name as long as they have unique path names Current Working DirectoryAlways specifying the absolute pathname for a file is tedious! Introduce the idea of a working directory Files are referenced relative to the working directory Relative and Absolute PathnamesAbsolute pathname A path specified from the root of the file system to the file A Relative pathname A pathname specified from the cwd Note: ‘.’ (dot) and ‘..’ (dotdot) refer to current and parent directory Typical Directory OperationsCreate Delete Opendir Closedir Readdir Rename Link Unlink Nice properties of UNIX namingSimple, regular format Names referring to different servers, objects, etc., have the same syntax. Regular tools can be used where specialised tools would be otherwise be needed. Location independent Objects can be distributed or migrated, and continue with the same names. File SharingIn multiuser system, allow files to be shared among users Two issues Access rights None User may not know of the existence of the file User is not allowed to read the directory that includes the file Knowledge User can only determine that the file exists and who its owner is Execution The user can load and execute a program but cannot copy it Reading The user can read the file for any purpose, including copying and execution Appending The user can add data to the file but cannot modify or delete any of the file’s contents Updating The user can modify, delete, and add to the file’s data. This includes creating the file, rewriting it, and removing all or part of the data Changing protection User can change access rights granted to other users Deletion User can delete the file Owners Has all rights previously listed May grant rights to others using the following classes of users Specific user User groups All for public files Management of simultaneous access Most OSes provide mechanisms for users to manage concurrent access to files Example: flock(), lockf(), system calls Typically User may lock entire file when it is to be updated User may lock the individual records (i.e. ranges) during the update Mutual exclusion and deadlock are issues for shared access File System The FS must map symbolic file names into a collection of block addresses The FS must keep track of which blocks belong to which files in what order the blocks form the file which blocks are free for allocation Given a logical region of a file, the FS must track the corresponding block(s) on disk Stored in file system metadata File Allocation Methods A file is divided into “blocks” – the unit of transfer to storage External and internal fragmentationExternal fragmentation The space wasted external to the allocated memory regions Memory space exists to satisfy a request but it is unusable as it is not contiguous Internal fragmentation The space wasted internal to the allocated memory regions Allocated memory may be slightly larger than requested memory; this size difference is wasted memory internal to a partition Contiguous Allocation✅ Easy bookkeeping (need to keep track of the starting block and length of the file) ✅ Increases performance for sequential operations ❌ Need the maximum size for the file at the time of creation ❌ As files are deleted, free space becomes divided into many small chunks (external fragmentation) Dynamic Allocation Strategies Disk space allocated in portions as needed Allocation occurs in fixed-size blocks ✅ No external fragmentation ✅ Does not require pre-allocating disk space ❌ Partially filled blocks (internal fragmentation) ❌ File blocks are scattered across the disk ❌ Complex metadata management (maintain the collection of blocks for each file) Dynamic allocation: Linked list allocation Each block contains a pointer to the next block in the chain. Free blocks are also linked in a chain. ✅ Only single metadata entry per file ✅ Best for sequential files ❌ Poor for random access ❌ Blocks end up scattered across the disk due to free list eventually being randomised Dynamic Allocation: File Allocation Table (FAT)Keep a map of the entire FS in a separate table A table entry contains the number of the next block of the file The last block in a file and empty blocks are marked using reserved values The table is stored on the disk and is replicated in memory ✅ ​Random access is fast (following the in-memory list) ❌ Requires a lot of memory for large disks ❌ Free block lookup is slow Dynamical Allocation: inode-based FS structureIdea: separate table (index-node or i-node) for each file. Only keep table for open files in memory Fast random access The most popular FS structure today ❌ i-nodes occupy one or several disk areas ❌ i-nodes are allocated dynamically, hence free-space management is required for i-nodes Use fixed-size i-nodes to simplify dynamic allocation Reserve the last i-node entry for a pointer (a block number) to an extension i-node. i-node implementation issues Free-space management Approach 1: linked list of free blocks in free blocks on disk List of all unallocated blocks Background jobs can re-order list for better contiguity Store in free blocks themselves Does not reduce disk capacity Only one block of pointers need be kept in the main memory Approach 2: keep bitmaps of free blocks and free i-nodes on disk Individual bits in a bit vector flags used/free blocks May be too large to hold in main memory Expensive to search Concentrating (de)allocations in a portion of the bitmap has desirable effect of concentrating access Simple to find contiguous free space Implementing directories Directories are stored like normal files directory entries are contained inside data blocks The FS assigns special meaning to the content of these files a directory file is a list of directory entries a directory entry contains file name, attributes, and the file i-node number maps human-oriented file name to a system-oriented name Fixed-size vs variable-size directory entries Fixed-size directory entries Either too small Or waste too much space Variable-size directory entries Freeing variable length entries can create external fragmentation in directory blocks Can compact when block is in RAM Searching Directory Listings Locating a file in a directory Linear scan Implement a directory cache in software to speed-up search Hash lookup B-tree (100’s of thousands entries) Storing file attributes (a) disk addresses and attributes in directory entry –FAT (b) directory in which each entry just refers to an i-node –UNIX Inode Contents Size Offset of the highest byte written Block count Number of disk blocks used by the file. Note that number of blocks can be much less than expected given the file size Files can be sparsely populated E.g. write(f,“hello”); lseek(f, 1000000); write(f, “world”); Only needs to store the start and end of file, not all the empty blocks in between. Size = 1000005 Blocks = 2 + any indirect blocks Direct Blocks Block numbers of first 12 blocks in the file Most files are small – We can find blocks of file directly from the inode Single Indirect Block – Block number of a block containing block numbers Requires two disk access to read – One for the indirect block; one for the target block Max File Size – Assume 1Kbyte block size, 4 byte block numbers 12 * 1K + 1K/4 * 1K = 268 KiB For large majority of files (&lt; 268 KiB), given the inode, only one or two further accesses required to read any block in file Double Indirect Block – Block number of a block containing block numbers of blocks containing block numbers Triple Indirect Hard Links Note that inodes can have more than one name -– Called a Hard Link Symbolic links A symbolic link is a file that contains a reference to another file or directory Has its own inode and data block, which contains a path to the target file Marked by a special file attribute Transparent for some operations Can point across FS boundaries FS reliabilityDisk writes are buffered in RAM OS crash or power outage ==&gt; lost data Commit writes to disk periodically Use the sync command to force a FS flush FS operations are non-atomic Incomplete transaction can leave the FS in an inconsistent state Journaling file systems Keep a journal of FS updates Before performing an atomic update sequence write it to the journal Replay the last journal entries upon an unclean shutdown The ext3 journalOption1: Journal FS data structure updates ✅ Efficient use of journal space; hence faster journaling ❌ Individual updates are applied separately ❌ The journaling layer must understand FS semantics Option2: Journal disk block updates Ext3 implements Option 2 ❌ Even a small update adds a whole block to the journal ✅ Multiple updates to the same block can be aggregated into a single update ✅ The journaling layer is FSindependent (easier to implement) Journaling Block Device (JBD)JBD interface (continued) Commit: write transaction data to the journal (persistent storage) Multiple FS transactions are committed in one go Checkpoint: flush the journal to the disk Used when the journal is full or the FS is being unmounted Trade-off in FS block size File systems deal with 2 types of blocks Disk blocks or sectors (usually 512 bytes) File system blocks 512 * 2^N bytes Larger blocks require less FS metadata Smaller blocks waste less disk space (less internal fragmentation) Sequential Access The larger the block size, the fewer I/O operations required Random Access The larger the block size, the more unrelated data loaded Spatial locality of access improves the situation Choosing an appropriate block size is a compromise Virtual File System(VFS) Provides single system call interface for many file systems Transparent handling of network file systems File-based interface to arbitrary device drivers File-based interface to kernel data structures Provides an indirection layer for system calls File operation table set up at file open time Points to actual handling code for particular type Further file operations redirected to those functions VFS InterfaceTwo major data typesVFS Represents all file system types Contains pointers to functions to manipulate each file system as a whole (e.g. mount, unmount) Vnode Represents a file (inode) in the underlying filesystem Points to the real inode Contains pointers to functions to manipulate files/inodes (e.g. open, close, read, write,…) File Descriptors Each open file has a file descriptor Read/Write/lseek/…. use them to specify which file to operate on. State associated with a file descriptor File pointer Determines where in the file the next read or write is performed Mode Was the file opened read-only, etc…. Memory ManagementOS Memory Management Keeps track of what memory is in use and what memory is free Allocates free memory to process when needed And deallocates it when they don’t Manages the transfer of memory between RAM and disk. Two broad classes of memory management systems Those that transfer processes to and from external storage during execution. Called swapping or paging Those that don’t Simple Might find this scheme in an embedded device, dumb phone, or smartcard. Basic Memory ManagementMonoprogramming without Swapping or Paging Three simple ways of organizing memory MonoprogrammingOkay if Only have one thing to do Memory available approximately equates to memory required Otherwise, Poor CPU utilisation in the presence of I/O waiting Poor memory utilisation with a varied job mix Fixed partitioningSimple MM: Fixed, equal-sized partitions Any unused space in the partition is wasted Called internal fragmentation Processes smaller than main memory, but larger than a partition cannot run Simple MM: Fixed, variable-sized partitions Divide memory at boot time into a selection of different sized partitions Can base sizes on expected workload Each partition has queue: Place process in queue for smallest partition that it fits in. Processes wait for when assigned partition is empty to start Issue Some partitions may be idle Small jobs available, but only large partition free Workload could be unpredictable Alternative queue strategySingle queue, search for any jobs that fit Small jobs in large partition if necessary Increases internal memory fragmentation Fixed Partition Summary Simple Easy to implement Can result in poor memory utilisation Due to internal fragmentation Used on IBM System 360 operating system (OS/MFT) Announced 6 April, 1964 Still applicable for simple embedded systems Static workload known in advance Dynamic Partitioning Partitions are of variable length Allocated on-demand from ranges of free memory Process is allocated exactly what it needs Assumes a process knows what it needs We end up with unusable holes (external fragmentation) Classic ApproachRepresent available memory as a linked list of available “holes” (free memory ranges). Base, size Kept in order of increasing address Simplifies merging of adjacent holes into larger holes. List nodes be stored in the “holes” themselves Dynamic Partitioning Placement AlgorithmFirst-fit algorithm Scan the list for the first entry that fits If greater in size, break it into an allocated and free part Intent: Minimise amount of searching performed Aims to find a match quickly Biases allocation to one end of memory Tends to preserve larger blocks at the end of memory Next-fit Like first-fit, except it begins its search from the point in list where the last request succeeded instead of at the beginning. (Flawed) Intuition: spread allocation more uniformly over entire memory to avoid skipping over small holes at start of memory Performs worse than first-fit as it breaks up the large free space at end of memory. Best-fit algorithm Chooses the block that is closest in size to the request Performs worse than first-fit Has to search complete list does more work than first-fit Since smallest block is chosen for a process, the smallest amount of external fragmentation is left Create lots of unusable holes Worst-fit algorithm Chooses the block that is largest in size (worst-fit) (whimsical) idea is to leave a usable fragment left over Poor performer Has to do more work (like best fit) to search complete list Does not result in significantly less fragmentation Dynamic Partition Allocation Algorithm Summary First-fit generally better than the others and easiest to implement Compaction We can reduce external fragmentation by compaction Shuffle memory contents to place all free memory together in one large block. Only if we can relocate running programs? Pointers? Generally requires hardware support When are memory addresses bound? Compile/link time Compiler/Linker binds the addresses Must know “run” location at compile time Recompile if location changes Load time Compiler generates relocatable code Loader binds the addresses at load time Run time Logical compile-time addresses translated to physical addresses by special hardware. Hardware Support for Runtime Binding and Protection For process B to run using logical addresses Process B expects to access addresses from zero to some limit of memory size Need to add an appropriate offset to its logical addresses Achieve relocation Protect memory “lower” than B Must limit the maximum logical address B can generate Protect memory “higher” than B Base and Limit Register Also called Base and bound registers Relocation and limit registers Base and limit registers Restrict and relocate the currently active process Base and limit registers must be changed at Load time Relocation (compaction time) On a context switch Pro Supports protected multi-processing (-tasking) Cons Physical memory allocation must still be contiguous The entire process must be in memory Do not support partial sharing of address spaces No shared code, libraries, or data structures between processes Thus far, we have a system suitable for a batch system Limited number of dynamically allocated processes Enough to keep CPU utilised Relocated at runtime Protected from each other Swapping A process can be swapped temporarily out of memory to a backing store, and then brought back into memory for continued execution. Backing store – fast disk large enough to accommodate copies of all memory images for all users; must provide direct access to these memory images. Can prioritize – lower-priority process is swapped out so higher-priority process can be loaded and executed. Major part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped. slow we assume a process is smaller than memory Virtual Memory – Paging Overview Partition physical memory into small equal sized chunks Called frames Divide each process’s virtual (logical) address space into same size chunks Called pages Virtual memory addresses consist of a page number and offset within the page OS maintains a page table contains the frame location for each page Used by hardware to translate each virtual address to physical address The relation between virtual addresses and physical memory addresses is given by page table Process’s physical memory does not have to be contiguous No external fragmentation Small internal fragmentation (in last page) Allows sharing by mapping several pages to the same frame Abstracts physical organisation Programmer only deal with virtual addresses Minimal support for logical organisation Each unit is one or more pages Virtual MemoryMemory Management Unit(or TLB) Page-based VMVirtual Memory Divided into equalsized pages A mapping is a translation between A page and a frame A page and null Mappings defined at runtime – They can change Address space can have holes Process does not have to be contiguous in physical memory Physical Memory Divided into equal-sized frames Typical Address Space Layout Stack region is at top, and can grow down Heap has free space to grow up Text is typically read-only Kernel is in a reserved, protected, shared region 0-th page typically not used Page Faults Referencing an invalid page triggers a page fault – An exception handled by the OS Broadly, two standard page fault types Illegal Address (protection error) Signal or kill the process Page not resident Get an empty frame Load page from disk Update page (translation) table (enter frame #, set valid bit, etc.) Restart the faulting instruction Shared Pages Private code and data Each process has own copy of code and data Code and data can appear anywhere in the address space Shared code Single copy of code shared between all processes executing it Code must not be self modifying Code must appear at same address in all processes Page Table StructurePage table is (logically) an array of frame numbers Index by page number Each page-table entry (PTE) also has other bits Present/Absent bit Also called valid bit, it indicates a valid mapping for the page Modified bit Also called dirty bit, it indicates the page may have been modified in memory Reference bit Indicates the page has been accessed Protection bits Read permission, Write permission, Execute permission Or combinations of the above Caching bit Use to indicate processor should bypass the cache when accessing memory Example: to access device registers or memory Address TranslationEvery (virtual) memory address issued by the CPU must be translated to physical memory Every load and every store instruction Every instruction fetch Need Translation Hardware In paging system, translation involves replace page number with a frame number Page Tables Page tables are implemented as data structures in main memory Most processes do not use the full 4GB address space We need a compact representation that does not waste space Three basic schemes Use data structures that adapt to sparsity Use data structures which only represent resident pages Use VM techniques for page tables (details left to extended OS) Two-level Page Table2nd –level page tables representing unmapped pages are not allocated – Null in the top-level page table Alternative: Inverted Page Table “Inverted page table” is an array of page numbers sorted (indexed) by frame number (it’s a frame table). Algorithm Compute hash of page number Extract index from hash table Use this to index into inverted page table Match the PID and page number in the IPT entry If match, use the index value as frame # for translation If no match, get next candidate IPT entry from chain field If NULL chain entry $\\Rightarrow$ page fault Properties of IPTs IPT grows with size of RAM, NOT virtual address space Frame table is needed anyway (for page replacement, more later) Need a separate data structure for non-resident pages Saves a vast amount of space (especially on 64-bit systems) Used in some IBM and HP workstations Improving the IPT: Hashed Page Table Retain fast lookup of IPT – A single memory reference in best case Retain page table sized based on physical memory size (not virtual) Enable efficient frame sharing Support more than one mapping for same frame Sizing the Hashed Page Table HPT sized based on physical memory size With sharing Each frame can have more than one PTE More sharing increases number of slots used – Increases collision likelihood However, we can tune HPT size based on: Physical memory size Expected sharing Hash collision avoidance. HPT a power of 2 multiple of number of physical memory frame VM Implementation IssuePerformanceEach virtual memory reference can cause two physical memory accesses One to fetch the page table entry One to fetch/store the data $\\Rightarrow$ Intolerable performance impact!! Solution: High-speed cache for page table entries (PTEs) Called a translation look-aside buffer (TLB) Contains recently used page table entries Associative, high-speed memory, similar to cache memory May be under OS control (unlike memory cache) TLB Translation Lookaside Buffer Given a virtual address, processor examines the TLB If matching PTE found (TLB hit), the address is translated Otherwise (TLB miss), the page number is used to index the process’s page table If PT contains a valid entry, reload TLB and restart Otherwise, (page fault) check if page is on disk If on disk, swap it in Otherwise, allocate a new page or raise an exception TLB properties Page table is (logically) an array of frame numbers TLB holds a (recently used) subset of PT entries Each TLB entry must be identified (tagged) with the page # it translates Access is by associative lookup: All TLB entries’ tags are concurrently compared to the page # TLB is associative (or content-addressable) memory TLB may or may not be under direct OS control Hardware-loaded TLB On miss, hardware performs PT lookup and reloads TLB Example: x86, ARM Software-loaded TLB On miss, hardware generates a TLB miss exception, and exception handler reloads TLB Example: MIPS, Itanium (optionally) TLB size: typically 64-128 entries Can have separate TLBs for instruction fetch and data access TLBs can also be used with inverted page tables (and others) TLB and context switching TLB is a shared piece of hardware Normal page tables are per-process (address space) TLB entries are process-specific On context switch need to flush the TLB (invalidate all entries) high context-switching overhead (Intel x86) or tag entries with address-space ID (ASID) called a tagged TLB used (in some form) on all modern architectures TLB entry: ASID, page #, frame #, valid and write-protect bits TLB effect Without TLB Average number of physical memory references per virtual reference = 2 With TLB (assume 99% hit ratio) Average number of physical memory references per virtual reference = 0.99 * 1+ 0.01 * 2 = 1.01 Demand Paging/Segmentation With VM, only parts of the program image need to be resident in memory for execution Can transfer presently unused pages/segments to disk Reload non-resident pages/segment on demand. Reload is triggered by a page or segment fault Faulting process is blocked and another scheduled When page/segment is resident, faulting process is restarted May require freeing up memory first Replace current resident page/segment How determine replacement “victim”? If victim is unmodified (“clean”) can simply discard it This is reason for maintaining a “dirty” bit in the PT Why does demand paging/segmentation work? Program executes at full speed only when accessing the resident set. TLB misses introduce delays of several microseconds Page/segment faults introduce delays of several milliseconds Answer Less physical memory required per process Can fit more processes in memory Improved chance of finding a runnable one Principle of locality Principle of Locality An important observation comes from empirical studies of the properties of programs. Programs tend to reuse data and instructions they have used recently. 90/10 rule – “A program spends 90% of its time in 10% of its code” We can exploit this locality of references An implication of locality is that we can reasonably predict what instructions and data a program will use in the near future based on its accesses in the recent past Two different types of locality have been observed: Temporal locality: states that recently accessed items are likely to be accessed in the near future Spatial locality: says that items whose addresses are near one another tend to be referenced close together in time Working Set The pages/segments required by an application in a time window ($\\Delta$)is called its memory working set. Working set is an approximation of a programs’ locality if $\\Delta$ too small will not encompass entire locality if $\\Delta$ too large will encompass several localities if $\\Delta = \\infin\\Rightarrow$ will encompass entire program. $\\Delta$’s size is an application specific tradeoff System should keep resident at least a process’s working set – Process executes while it remains in its working set Working set tends to change gradually Get only a few page/segment faults during a time window Possible (but hard) to make intelligent guesses about which pieces will be needed in the future – May be able to pre-fetch page/segments ThrashingCPU utilisation tends to increase with the degree of multiprogramming – number of processes in system Higher degrees of multiprogramming – less memory available per process Some process’s working sets may no longer fit in RAM – Implies an increasing page fault rate Eventually many processes have insufficient memory Can’t always find a runnable process Decreasing CPU utilisation System become I/O limited This is called thrashing $ \\sum$working set sizes &gt; total physical memory size Recovery From ThrashingIn the presence of increasing page fault frequency and decreasing CPU utilisation Suspend a few processes to reduce degree of multiprogramming Resident pages of suspended processes will migrate to backing store More physical memory becomes available Resume suspended processes later when memory pressure eases VM Management PoliciesOperation and performance of VM system is dependent on a number of policies: Page table format (may be dictated by hardware) Multi-level Inverted/Hashed Page size (may be dictated by hardware) Fetch Policy Replacement policy Resident set size Minimum allocation Local versus global allocation Page cleaning policy Page SizeIncreasing page size ❌ Increases internal fragmentation –  reduces adaptability to working set size ✅ Decreases number of pages – Reduces size of page tables ✅ Increases TLB coverage – Reduces number of TLB misses ❌ Increases page fault latency – Need to read more from disk before restarting process ✅ Increases swapping I/O throughput – Small I/O are dominated by seek/rotation delays Optimal page size is a (work-load dependent) trade-off Multiple page sizes provide flexibility to optimise the use of the TLB Large page sizes can be use for code Small page size for thread stacks Most operating systems support only a single page size Dealing with multiple page sizes is hard! Fetch Policy Determines when a page should be brought into memory Demand paging only loads pages in response to page faults – Many page faults when a process first starts Pre-paging brings in more pages than needed at the moment ​ ✅ Improves I/O performance by reading in larger chunks ​ ❌ Wastes I/O bandwidth if pre-fetched pages aren’t used ​ ❌ Especially bad if we eject pages in working set in order to pre-fetch unused pages. ​ ❌ Hard to get right in practice Replacement Policy Page removed should be the page least likely to be references in the near future Most policies attempt to predict the future behaviour on the basis of past behaviour Constraint: locked frames Kernel code Main kernel data structure I/O buffers Performance-critical user-pages (e.g. for DBMS) Frame table has a lock (or pinned) bit Optimal Replacement policy Toss the page that won’t be used for the longest time ❌ Impossible to implement ❌ Only good as a theoretic reference point:The closer a practical algorithm gets to optimal, the better FIFO Replacement Policy First-in, first-out: Toss the oldest page ✅ Easy to implement ❌ Age of a page is isn’t necessarily related to usage Least Recently Used (LRU) Toss the least recently used page Assumes that page that has not been referenced for a long time is unlikely to be referenced in the near future ⚪ Will work if locality holds ⚪ Implementation requires a time stamp to be kept for each page, updated on every reference ❌ Impossible to implement efficiently ⚪ Most practical algorithms are approximations of LRU Clock Page Replacement Clock policy, also called second chance. Employs a usage or reference bit in the frame table. Set to one when page is used While scanning for a victim, reset all the reference bits Toss the first page with a zero reference bit Issue How do we know when a page is referenced? Use the valid bit in the PTE: When a page is mapped (valid bit set), set the reference bit When resetting the reference bit, invalidate the PTE entry On page fault Turn on valid bit in PTE Turn on reference bit We thus simulate a reference bit in software PerformanceIt terms of selecting the most appropriate replacement, they rank as follows: Optimal LRU Clock FIFO Resident Set Size How many frames should each process have? Fixed Allocation Gives a process a fixed number of pages within which to execute. Isolates process memory usage from each other When a page fault occurs, one of the pages of that process must be replaced. ❌ Achieving high utilisation is an issue – Some processes have high fault rate while others don’t use their allocation. Variable Allocation Number of pages allocated to a process varies over the lifetime of the process Global Scope Operating system keeps global list of free frames Free frame is added to resident set of process when a page fault occurs If no free frame, replaces one from any process ✅ Easiest to implement ✅ Adopted by many operating systems ✅ Automatic balancing across system ❌ Does not provide guarantees for important activities Local Scope Allocate number of page frames to a new process based on Application type Program request Other criteria (priority) When a page fault occurs, select a page from among the resident set of the process that suffers the page fault Re-evaluate allocation from time to time! Cleaning PolicyObservation – Clean pages are much cheaper to replace than dirty pages Demand cleaning A page is written out only when it has been selected for replacement High latency between the decision to replace and availability of free frame. Precleaning Pages are written out in batches (in the background, the pagedaemon) Increases likelihood of replacing clean frames Overlap I/O with current activity Multiprocessor SystemsAmdahl’s lawGiven a proportion P of a program that can be made parallel, and the remaining serial portion (1-P), speedup by using N processors$\\frac{1}{(1-P)+\\frac{P}{N}}$ Types of Multiprocessors (MPs) UMA MP (Uniform Memory Access) Access to all memory occurs at the same speed for all processors. NUMA MP (Non-uniform memory access) Access to some parts of memory is faster for some processors than other parts of memory Bus Based UMA Simplest MP is more than one processor on a single bus connect to memory Bus bandwidth becomes a bottleneck with more than just a few CPUs Each processor has a cache to reduce its need for access to memory Hope is most accesses are to the local cache Bus bandwidth still becomes a bottleneck with many CPUs With only a single shared bus, scalability can be limited by the bus bandwidth of the single bus - Caching only helps so much Alternative bus architectures do exist. They improve bandwidth available Don’t eliminate constraint that bandwidth is limited Multi-core Processor(Multi-core) share the same Bus interfaceMultiprocessors can Increase computation power beyond that available from a single CPU Share resources such as disk and memory However Assumes parallelizable workload to be effective Assumes not I/O bound Shared buses (bus bandwidth) limits scalability Can be reduced via hardware design Can be reduced by carefully crafted software behaviour Good cache locality together with limited data sharing where possible How do we construct an OS for a multiprocessor?Each CPU has its own OS? Statically allocate physical memory to each CPU Each CPU runs its own independent OS Share peripherals Each CPU (OS) handles its processes system calls Used in early multiprocessor systems to ‘get them going’ Simpler to implement Avoids CPU-based concurrency issues by not sharing Scales – no shared serial sections Modern analogy, virtualisation in the cloud. Issues Each processor has its own scheduling queue We can have one processor overloaded, and the rest idle Each processor has its own memory partition We can a one processor thrashing, and the others with free memory No way to move free memory from one OS to another Symmetric Multiprocessors (SMP) OS kernel run on all processors Load and resource are balance between all processors Including kernel execution Issue: Real concurrency in the kernel Need carefully applied synchronisation primitives to avoid disaster One alternative: A single mutex that make the entire kernel a large critical section Only one CPU can be in the kernel at a time The “big lock” becomes a bottleneck when in-kernel processing exceeds what can be done on a single CPU Better alternative: identify largely independent parts of the kernel and make each of them their own critical section Allows more parallelism in the kernel Issue: Difficult task Code is mostly similar to uniprocessor code Hard part is identifying independent parts that don’t interfere with each other Remember all the inter-dependencies between OS subsystems. Lock contention can limit overall system performance Test-and-Set Hardware guarantees that the instruction executes atomically on a CPU.Atomically: As an indivisible unit.The instruction can not stop half way through Test-and-Set on SMPIt does not work without some extra hardware support A solution: Hardware blocks all other CPUs from accessing the bus during the TSL instruction to prevent memory accesses by any other CPU. TSL has mutually exclusive access to memory for duration of instruction. Test-and Set is a busy-wait synchronisation primitive * Called a spinlock Issue: Lock contention leads to spinning on the lock Spinning on a lock requires blocking the bus which slows all other CPUs down Independent of whether other CPUs need a lock or no Caching does not help reduce bus contention Either TSL still blocks the bus Or TSL requires exclusive access to an entry in the local cache Reducing Bus Contention Read before TSL Spin reading the lock variable waiting for it to change When it does, use TSL to acquire the lock Allows lock to be shared read-only in all caches until its released no bus traffic until actual release No race conditions, as acquisition is still with TSL. Test and set performs poorly once there is enough CPUs to cause contention for lock Expected Read before Test and Set performs better Performance less than expected Still significant contention on lock when CPUs notice release and all attempt acquisition Critical section performance degenerates Critical section requires bus traffic to modify shared structure Lock holder competes with CPU that’s waiting as they test and set, so the lock holder is slower Slower lock holder results in more contention Cache ConsistencyCache consistency is usually handled by the hardware. Spinning versus Blocking and Switching Spinning (busy-waiting) on a lock makes no sense on a uniprocessor The was no other running process to release the lock Blocking and (eventually) switching to the lock holder is the only sensible option. On SMP systems, the decision to spin or block is not as clear. The lock is held by another running CPU and will be freed without necessarily switching away from the requestor Spinning versus SwitchingBlocking and switching to another process takes time Save context and restore another Cache contains current process not new process Adjusting the cache working set also takes time TLB is similar to cache Switching back when the lock is free encounters the same again Spinning wastes CPU time directly Trade offIf lock is held for less time than the overhead of switching to and back ​ $\\Rightarrow$ It’s more efficient to spin $\\Rightarrow$ Spinlocks expect critical sections to be short​ $\\Rightarrow$ No waiting for I/O within a spinlock​ $\\Rightarrow$ No nesting locks within a spinlock Preemption and Spinlocks Critical sections synchronised via spinlocks are expected to be short Avoid other CPUs wasting cycles spinning What happens if the spinlock holder is preempted at end of holder’s timeslice Mutual exclusion is still guaranteed Other CPUs will spin until the holder is scheduled again!!!!! $\\Rightarrow$ Spinlock implementations disable interrupts in addition to acquiring locks to avoid lock-holder preemption Scheduling The scheduler decides who to run next. This process is sheduling Application behaviourBursts of CPU usage alternate with periods of I/O waita) CPU-Bound process Spends most of its computing Time to completion largely determined by received CPU time b) I/O-Bound process Spend most of its time waiting for I/O to complete Small bursts of CPU to process I/O and request next I/O Time to completion largely determined by I/O request time We need a mix of CPU-bound and I/O-bound processes to keep both CPU and I/O systems busy Process can go from CPU- to I/O-bound (or vice versa) in different phases of execution Key InsightsChoosing to run an I/O-bound process delays a CPU-bound process by very littleChoosing to run a CPU-bound process prior to an I/O-bound process delays the next I/O request significantly No overlap of I/O waiting with computation Results in device (disk) not as busy as possible Generally, favour I/O-bound processes over CPU-bound processes Generally, a scheduling decision is required when a process (or thread) can no longer continue, or when an activity results in more than one ready process Preemptive versus Non-preemptive SchedulingNon-preemptive Once a thread is in the running state, it continues until it completes, blocks on I/O, or voluntarily yields the CPU A single process can monopolised the entire systemPreemptive Scheduling (responsive system) Current thread can be interrupted by OS and moved to ready state. Usually after a timer interrupt and process has exceeded its maximum run time Can also be as a result of higher priority process that has become ready (after I/O interrupt). Ensures fairer service as single thread can’t monopolise the system Requires a timer interrupt Categories of Scheduling AlgorithmsBatch Systems No users directly waiting, can optimise for overall machine performance Interactive Systems Users directly waiting for their results, can optimise for users perceived performance Realtime Systems Jobs have deadlines, must schedule such that all jobs (predictably) meet their deadlines Goals of Scheduling AlgorithmsAll Algorithms Fairness Give each process a fair share of the CPU Policy Enforcement What ever policy chosen, the scheduler should ensure it is carried out Balance/Efficiency Try to keep all parts of the system busy Interactive Algorithms Minimise response time Response time is the time difference between issuing a command and getting the result E.g selecting a menu, and getting the result of that selection Response time is important to the user’s perception of the performance of the system. Provide Proportionality Proportionality is the user expectation that short jobs will have a short response time, and long jobs can have a long response time. Generally, favour short jobs Real-time Algorithms Must meet deadlines Each job/task has a deadline. A missed deadline can result in data loss or catastrophic failure Aircraft control system missed deadline to apply brakes Provide Predictability For some apps, an occasional missed deadline is okay – E.g. DVD decoder Predictable behaviour allows smooth DVD decoding with only rare skips Interactive schedulingRound Robin Scheduling Each process is given a timeslice to run in When the timeslice expires, the next process preempts the current process, and runs for its timeslice, and so on The preempted process is placed at the end of the queue Implemented with A ready queue A regular timer interrupt Pros – Fair, easy to implementCon – Assumes everybody is equal What should the timeslice be? Too short Waste a lot of time switching between processes Example: timeslice of 4ms with 1 ms context switch = 20% round robin overhead Too long System is not responsive Example: timeslice of 100ms – If 10 people hit “enter” key simultaneously, the last guy to run will only see progress after 1 second. Degenerates into FCFS if timeslice longer than burst length Priorities Each Process (or thread) is associated with a priority Provides basic mechanism to influence a scheduler decision: Scheduler will always chooses a thread of higher priority over lower priority Priorities can be defined internally or externally Internal: e.g. I/O bound or CPU bound External: e.g. based on importance to the user Usually implemented by multiple priority queues, with round robin on each queue Con Low priorities can starve Need to adapt priorities periodically Based on ageing or execution history Traditional UNIX SchedulerTwo-level scheduler High-level scheduler schedules processes between memory and disk Low-level scheduler is CPU scheduler Based on a multilevel queue structure with round robin at each level The highest priority (lower number) is scheduled Priorities are re-calculated once per second, and re-inserted in appropriate queue Avoid starvation of low priority threads Penalise CPU-bound threads Priority = CPU_usage +nice +base CPU_usage = number of clock ticks Nice is a value given to the process by a user to permanently boost or reduce its priority ( Reduce priority of background jobs) Base is a set of hardwired, negative values used to boost priority of I/O bound system activities Single Shared Ready QueuePros Simple Automatic load balancing Cons Lock contention on the ready queue can be a major bottleneck Due to frequent scheduling or many CPUs or both Not all CPUs are equal The last CPU a process ran on is likely to have more related entries in the cache Affinity SchedulingBasic Idea – Try hard to run a process on the CPU it ran on last time One approach: Multiple Queue Multiprocessor Scheduling Multiple Queue SMP Scheduling Each CPU has its own ready queue Coarse-grained algorithm assigns processes to CPUs Defines their affinity, and roughly balances the load The bottom-level fine-grained scheduler: Is the frequently invoked scheduler (e.g. on blocking on I/O, a lock, or exhausting a timeslice) Runs on each CPU and selects from its own ready queue Ensures affinity If nothing is available from the local ready queue, it runs a process from another CPUs ready queue rather than go idle Termed “Work stealing” ✅ No lock contention on per-CPU ready queues in the (hopefully) common case✅ Load balancing to avoid idle queues✅ Automatic affinity to a single CPU for more cache friendly behaviour I/O ManagementI/O DevicesThere exists a large variety of I/O devices: Many of them with different properties They seem to require different interfaces to manipulate and manage them We don’t want a new interface for every device Diverse, but similar interfaces leads to code duplication Challenge: Uniform and efficient approach to I/O Device DriversDrivers classified into similar categories Block devices and character (stream of data) device OS defines a standard (internal) interface to the different classes of devices Example: USB Human Input Device (HID) class specifications human input devices follow a set of rules making it easier to design a standard interface. Device drivers job translate request through the device-independent standard interface (open, close, read, write) into appropriate sequence of commands (register manipulations) for the particular hardware Initialise the hardware at boot time, and shut it down cleanly at shutdown After issuing the command to the device, the device either Completes immediately and the driver simply returns to the caller Or, device must process the request and the driver usually blocks waiting for an I/O complete interrupt. Drivers are thread-safe as they can be called by another process while a process is already blocked in the driver Thead-safe: Synchronised… Device-Independent I/O SoftwareThere is commonality between drivers of similar classes Divide I/O software into device-dependent and device-independent I/O software Device independent software includes Buffer or Buffer-cache management TCP/IP stack Managing access to dedicated devices Error reporting I/O Device Handling Data rate May be differences of several orders of magnitude between the data transfer rates Driver $\\Leftrightarrow$ Kernel InterfaceMajor Issue is uniform interfaces to devices and kernel Uniform device interface for kernel code Allows different devices to be used the same way Allows internal changes to device driver with fear of breaking kernel code Uniform kernel interface for device code Drivers use a defined interface to kernel services (e.g. kmalloc, install IRQ handler, etc.) Allows kernel to evolve without breaking existing drivers Together both uniform interfaces avoid a lot of programming implementing new interfaces Retains compatibility as drivers and kernels change over time. Interrupts Devices connected to an Interrupt Controller via lines on an I/O bus (e.g. PCI) Interrupt Controller signals interrupt to CPU and is eventually acknowledged. Exact details are architecture specific. I/O InteractionProgrammed I/O Also called polling, or busy waiting I/O module (controller) performs the action, not the processor Sets appropriate bits in the I/O status register No interrupts occur Processor checks status until operation is complete Wastes CPU cycles Interrupt-Driven I/O Processor is interrupted when I/O module (controller) ready to exchange data Processor is free to do other work No needless waiting Consumes a lot of processor time because every word read or written passes through the processor Direct Memory Access (DMA) Transfers data directly between Memory and Device CPU not needed for copying Transfers a block of data directly to or from memory An interrupt is sent when the task is complete The processor is only involved at the beginning and end of the transfer DMA Considerations✅ Reduces number of interrupts Less (expensive) context switches or kernel entry-exits  ❌ Requires contiguous regions (buffers) Copying Some hardware supports “Scatter-gather” Synchronous/Asynchronous Shared bus must be arbitrated (hardware) – CPU cache reduces (but not eliminates) CPU need for bus I/O Management SoftwareI/O Software Layers Interrupt Handlers Interrupt handlers Can execute at (almost) any time Raise (complex) concurrency issues in the kernel Can propagate to userspace (signals, upcalls), causing similar issues Generally structured so I/O operations block until interrupts notify them of completion kern/dev/lamebus/lhd.c Interrupt Handler Steps Save Registers not already saved by hardware interrupt mechanism (Optionally) set up context for interrupt service procedure Typically, handler runs in the context of the currently running process No expensive context switch Set up stack for interrupt service procedure Handler usually runs on the kernel stack of current process Or “nests” if already in kernel mode running on kernel stack Ack/Mask interrupt controller, re-enable other interrupts Implies potential for interrupt nesting. Run interrupt service procedure Acknowledges interrupt at device level Figures out what caused the interrupt Received a network packet, disk read finished, UART transmit queue empty If needed, it signals blocked device driver In some cases, will have woken up a higher priority blocked thread Choose newly woken thread to schedule next. Set up MMU context for process to run next What if we are nested? Load new/original process’ registers Re-enable interrupt; Start running the new process Sleeping in Interrupts An interrupt generally has no context (runs on current kernel stack) Unfair to sleep on interrupted process (deadlock possible) Where to get context for long running operation? What goes into the ready queue? What to do? Top and Bottom Half Linux implements with tasklets and workqueues Generically, in-kernel thread(s) handle long running kernel operations. Top/Half Bottom Half Top Half Interrupt handler remains short Bottom half Is preemptable by top half (interrupts) performs deferred work (e.g. IP stack processing) Is checked prior to every kernel exit signals blocked processes/threads to continue Enables low interrupt latency Bottom half can’t block Deferring Work on In-kernel Threads Interrupt handler defers work onto in-kernel thread In-kernel thread handles deferred work (DW) Scheduled normally Can block Both low interrupt latency and blocking operations BufferingNo Buffering Process must read/write a device a byte/word at a time Each individual system call adds significant overhead Process must what until each I/O is complete Blocking/interrupt/waking adds to overhead. Many short runs of a process is inefficient (poor CPU cache temporal locality) User-level Buffering Process specifies a memory buffer that incoming data is placed in until it fills Filling can be done by interrupt service routine Only a single system call, and block/wakeup per data buffer Much more efficient Issues What happens if buffer is paged out to disk Could lose data while unavailable buffer is paged in Could lock buffer in memory (needed for DMA), however many processes doing I/O reduce RAM available for paging. Can cause deadlock as RAM is limited resource Consider write case When is buffer available for re-use? Either process must block until potential slow device drains buffer or deal with asynchronous signals indicating buffer drained Single Buffer Operating system assigns a buffer in kernel’s memory for an I/O request In a stream-oriented scenario Used a line at time User input from a terminal is one line at a time with carriage return signaling the end of the line Output to the terminal is one line at a time Block-oriented Input transfers made to buffer Block copied to user space when needed Another block is written into the buffer Read ahead User process can process one block of data while next block is read in Swapping can occur since input is taking place in system memory, not user memory Operating system keeps track of assignment of system buffers to user processes Assume T is transfer time for a block from device C is computation time to process incoming block M is time to copy kernel buffer to user buffer Computation and transfer can be done in parallel Speed up with buffering What happens if kernel buffer is full the user buffer is swapped out, or The application is slow to process previous buffer and more data is received??? =&gt; We start to lose characters or drop network packets Double Buffer Use two system buffers instead of one A process can transfer data to or from one buffer while the operating system empties or fills the other buffer Double Buffer Speed Up Computation and Memory copy can be done in parallel with transfer Speed up with double buffering Usually M is much less than T giving a favourable result May be insufficient for really bursty traffic Lots of application writes between long periods of computation Long periods of application computation while receiving data Might want to read-ahead more than a single block for disk Circular Buffer More than two buffers are used Each individual buffer is one unit in a circular buffer Used when I/O operation must keep up with process Notice that buffering, double buffering, and circular buffering are all Bounded-Buffer Producer-Consumer Problems Reference:UNSW COMP3231 2020 lecture slides","link":"/2020/08/11/OperatingSystems/"}],"tags":[{"name":"spin","slug":"spin","link":"/tags/spin/"},{"name":"concurrency","slug":"concurrency","link":"/tags/concurrency/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"Haskell","slug":"Haskell","link":"/tags/Haskell/"},{"name":"Functional Programming","slug":"Functional-Programming","link":"/tags/Functional-Programming/"},{"name":"OS","slug":"OS","link":"/tags/OS/"}],"categories":[{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"notes","slug":"notes","link":"/categories/notes/"},{"name":"Paper Review","slug":"Paper-Review","link":"/categories/Paper-Review/"}]}