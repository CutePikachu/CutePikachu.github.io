{"pages":[{"title":"Projects","text":"My journey to the Computer Science World starts in July 2018. The followings are the projects I have at least made a fair amount of contribution. Some of them might not be open sourced, but they will shine and work as expected in the future. 2020.7 - Now awesome-unswLink: https://github.com/UNSWEEB/awesome-unswJust to collect class notes of UNSW computer science courses for students. Looking for more notes. We are welcome and seeking for contribution. 2020.7 - Now XYZSASStudent Afair System using Aliyun table storage and function compute. 2020.7 - Now AauthWeb link: https://aauth.link/Repo link: https://github.com/yzITI/Aauthâ€œAuth with Anything: Fourth Party Login Service.â€ The goal is to make it is easier for other web developers to use third party login service. I have only contributed to front-end page yet. 2020.4 - Now AcewordsLink: https://acewords.topAn web app to help Chinese memorize foreign language words.This one is actually version2.0. I have made more than half of its backend(written in NodeJs) and about one third of its front-end(still VueJs).My journey to the backend world starts now. 2019.12 - 2020.2 YZSA-feStudent Affair system for Yangzhou Highschool of Jiangsu Province front-end.I have paticipated in this project which is currently using by Yangzhou Highschool of Jiangsu Province. This project is not open sourced and I wonâ€™t provide the link to the website because it requires students or staff account to login. 2019.9 - Now YZZX-techWebsite link: https://yzzx.techRepo link(deprecated): https://github.com/CutePikachu/yzzx-tech-deprecated-Repo link(in use): https://github.com/yzITI/yzzx-techThis is the web page for ITI(website language in Chinese).The deprecated version is not fully written by me, though I have made it suitable to view in phone. In the current version, I have written some pages(in about early 2020). 2019.7 Little littersLink: https://github.com/dsksi/cse-2019-hackathonThis was made during UNSW(university of New South Wales) cse hackathon, and our group got 3rd prize(3/40). The project itself was not complicated but it was impressive because we made it in 24 hours and only part of our team actually had some web programming experience. I wrote few components or functions in react and this was my first time coding using ReactJs. 2019.6 Building MazeLink: https://github.com/CutePikachu/BuildingMaze-FEIt is a maze game built using vue.This project was somewhat incomplete and can be inproved. However, I donâ€™t want to change it and have decided to leave it like that.This is the project opening the door of front-end development for me. Guided by Phantomlsh.","link":"/projects/index.html"}],"posts":[{"title":"Document analysis","text":"COMP4650 notes Information Retrieval â€œInformation Retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).â€ â€“ Manning et al. Introduction to Boolean RetrievalInformation RetrievalWhy Information RetrievalAn essential tool to deal with information overloadInformation overload â€œIt refers to the difficulty a person can have understanding an issue and making decisions that can be caused by the presence of too much information.â€ - wiki How to perform information retrievalCollection: A set of documentsGoal: Retrieve documents with information that is relevant to the userâ€™s information need and helps the user complete a task #mermaid-1633958236794 .label{font-family:'trebuchet ms', verdana, arial;color:#333}#mermaid-1633958236794 .node rect,#mermaid-1633958236794 .node circle,#mermaid-1633958236794 .node ellipse,#mermaid-1633958236794 .node polygon{fill:#ECECFF;stroke:#9370db;stroke-width:1px}#mermaid-1633958236794 .node.clickable{cursor:pointer}#mermaid-1633958236794 .arrowheadPath{fill:#333}#mermaid-1633958236794 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1633958236794 .edgeLabel{background-color:#e8e8e8}#mermaid-1633958236794 .cluster rect{fill:#ffffde !important;stroke:#aa3 !important;stroke-width:1px !important}#mermaid-1633958236794 .cluster text{fill:#333}#mermaid-1633958236794 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:'trebuchet ms', verdana, arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1633958236794 .actor{stroke:#ccf;fill:#ECECFF}#mermaid-1633958236794 text.actor{fill:#000;stroke:none}#mermaid-1633958236794 .actor-line{stroke:grey}#mermaid-1633958236794 .messageLine0{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1633958236794 .messageLine1{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1633958236794 #arrowhead{fill:#333}#mermaid-1633958236794 #crosshead path{fill:#333 !important;stroke:#333 !important}#mermaid-1633958236794 .messageText{fill:#333;stroke:none}#mermaid-1633958236794 .labelBox{stroke:#ccf;fill:#ECECFF}#mermaid-1633958236794 .labelText{fill:#000;stroke:none}#mermaid-1633958236794 .loopText{fill:#000;stroke:none}#mermaid-1633958236794 .loopLine{stroke-width:2;stroke-dasharray:'2 2';stroke:#ccf}#mermaid-1633958236794 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1633958236794 .noteText{fill:black;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:14px}#mermaid-1633958236794 .activation0{fill:#f4f4f4;stroke:#666}#mermaid-1633958236794 .activation1{fill:#f4f4f4;stroke:#666}#mermaid-1633958236794 .activation2{fill:#f4f4f4;stroke:#666}#mermaid-1633958236794 .section{stroke:none;opacity:0.2}#mermaid-1633958236794 .section0{fill:rgba(102,102,255,0.49)}#mermaid-1633958236794 .section2{fill:#fff400}#mermaid-1633958236794 .section1,#mermaid-1633958236794 .section3{fill:#fff;opacity:0.2}#mermaid-1633958236794 .sectionTitle0{fill:#333}#mermaid-1633958236794 .sectionTitle1{fill:#333}#mermaid-1633958236794 .sectionTitle2{fill:#333}#mermaid-1633958236794 .sectionTitle3{fill:#333}#mermaid-1633958236794 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1633958236794 .grid .tick{stroke:#d3d3d3;opacity:0.3;shape-rendering:crispEdges}#mermaid-1633958236794 .grid path{stroke-width:0}#mermaid-1633958236794 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1633958236794 .task{stroke-width:2}#mermaid-1633958236794 .taskText{text-anchor:middle;font-size:11px}#mermaid-1633958236794 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1633958236794 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1633958236794 .taskText0,#mermaid-1633958236794 .taskText1,#mermaid-1633958236794 .taskText2,#mermaid-1633958236794 .taskText3{fill:#fff}#mermaid-1633958236794 .task0,#mermaid-1633958236794 .task1,#mermaid-1633958236794 .task2,#mermaid-1633958236794 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1633958236794 .taskTextOutside0,#mermaid-1633958236794 .taskTextOutside2{fill:#000}#mermaid-1633958236794 .taskTextOutside1,#mermaid-1633958236794 .taskTextOutside3{fill:#000}#mermaid-1633958236794 .active0,#mermaid-1633958236794 .active1,#mermaid-1633958236794 .active2,#mermaid-1633958236794 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1633958236794 .activeText0,#mermaid-1633958236794 .activeText1,#mermaid-1633958236794 .activeText2,#mermaid-1633958236794 .activeText3{fill:#000 !important}#mermaid-1633958236794 .done0,#mermaid-1633958236794 .done1,#mermaid-1633958236794 .done2,#mermaid-1633958236794 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1633958236794 .doneText0,#mermaid-1633958236794 .doneText1,#mermaid-1633958236794 .doneText2,#mermaid-1633958236794 .doneText3{fill:#000 !important}#mermaid-1633958236794 .crit0,#mermaid-1633958236794 .crit1,#mermaid-1633958236794 .crit2,#mermaid-1633958236794 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1633958236794 .activeCrit0,#mermaid-1633958236794 .activeCrit1,#mermaid-1633958236794 .activeCrit2,#mermaid-1633958236794 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1633958236794 .doneCrit0,#mermaid-1633958236794 .doneCrit1,#mermaid-1633958236794 .doneCrit2,#mermaid-1633958236794 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1633958236794 .doneCritText0,#mermaid-1633958236794 .doneCritText1,#mermaid-1633958236794 .doneCritText2,#mermaid-1633958236794 .doneCritText3{fill:#000 !important}#mermaid-1633958236794 .activeCritText0,#mermaid-1633958236794 .activeCritText1,#mermaid-1633958236794 .activeCritText2,#mermaid-1633958236794 .activeCritText3{fill:#000 !important}#mermaid-1633958236794 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1633958236794 g.classGroup text{fill:#9370db;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:10px}#mermaid-1633958236794 g.classGroup rect{fill:#ECECFF;stroke:#9370db}#mermaid-1633958236794 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1633958236794 .classLabel .box{stroke:none;stroke-width:0;fill:#ECECFF;opacity:0.5}#mermaid-1633958236794 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1633958236794 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1633958236794 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #compositionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #aggregationStart{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #aggregationEnd{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #dependencyStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #dependencyEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 #extensionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236794 .commit-id,#mermaid-1633958236794 .commit-msg,#mermaid-1633958236794 .branch-label{fill:lightgrey;color:lightgrey} #mermaid-1633958236794 { color: rgb(0, 0, 0); font: normal normal 400 normal 16px / normal \"Times New Roman\"; }1. User task2. Info need3. query4. Search engine5. Result6. Query refinementCollection Key Objectives Good IR system need to achieve Scalability Accuracy Information Retrieval vs Natural Language Processing| IR | NLP||â€”-|â€”-||Computational approaches|Cognitive, symbolic and computational approaches||Statistical (shallow) understanding of language|Semantic (deep) understanding of language||Handle large scale problems|(often) smaller scale problems| IR and NLP are getting closerIR $\\Rightarrow$ NLPâ€“ Larger data collectionsâ€“ Scalable/robust NLP techniques, e.g., translation modelsNLP $\\Rightarrow$ IRâ€“ Deep analysis of text documents and queriesâ€“ Information extraction for structured IR tasks Queries and Indexing Indexing: Storing a mapping from terms to documentsQuerying: Looking up terms in the index and returning documents Boolean queryBoolean queryE.g., â€œcanberraâ€ AND â€œhealthcareâ€ NOT â€œcovidâ€Search ProceduresLookup query term in the dictionary Retrieve the posting lists Operation AND: intersect the posting lists OR: union the posting list NOT: diff the posting list Retrieval procedure in modern IR Boolean model provides all the ranking candidates Locate documents satisfying Boolean condition Rank candidates by relevance Efficiency consideration Top-k retrieval Term-document incidence matrix Incidence vectors We have a 0/1 vector for each term. To answer query: take the vectors for Brutus, Caesar and Calpurnia (complemented) -&gt; bitwise AND 110100 AND 110111 AND 101111 = 100100 Efficiency Bigger Collections 1 million documents Each 1,000 words long Avg 6 bytes/word including spaces/punctuation 6GB of data in the documents. Assume there are M = 500K distinct terms amongthese. Corresponds to a matrix with 500 billion entries But it has no more than one billion 1â€™s Extremely sparse matrix! Inverted IndexInverted index consists of a dictionary and postings Dictionary: a set of unique terms Posting: variable-size array that keeps the list of documents given term (sorted) INDEXER: Construct inverted index from raw text Document Tokenization Initial Stages of Text Processing Tokenization Task of chopping a document into tokens Chopping by whitespace and throwing punctuation are notenough. Regex tokenizer: simple and efficient always do the same tokenization of document and query text Normalization Keep equivalence class of terms Map text and query term to same form U.S.A = USA = united states Synonym list: car = automobile Capitalization: ferrari $\\rightarrow$ Ferrari Case-folding: Automobile $\\rightarrow$ automobile , CAT != cat Stemming Stemming turns tokens into stems, which are the same regardless of inflection. Stems need not be realwords E.g. authorize, authorization; run, running Lemmatization Lemmatization turns words into lemmas, which are dictionary entries Stop words removal Stop words usually refer to the most common words in a language. Reduce the number of postings that a system has to store These words are not very useful in keyword search. Indexer Step 1. Token sequence Scan documents for indexable terms Keep list of (token, docID) pairs. Indexer Step 2. Sort tuples by terms (and then docID)Indexer Step 3. Merge multiple term entries in a single document, Split into Dictionary and Postings, Add doc. frequency information Boolean Retrieval with Inverted IndexEasy to retrieve all documents containing term tLinear Intersection/union algorithm with sorted list Boolean Retrieval Can answer any query which is a Boolean expression: AND, OR, NOT Precise: each document matches, or not Extended Boolean allows more complex queries Primary commercial search for 30+ years, and is still used Bag-of-Words and Document Fields we did not care about the ordering of tokens in document â€“ Bag-of-Words (Bow) assumption A document is a collection of words Field (Zone) in Document A partial solution to the weakness of BoW Documents may be semi-structured: Title, author, published date, body Users may want to limit search scope to acertain field Basic Field IndexEquivalent to a separate index for each fieldField in PostingReduces the size of the dictionary. Enables efficient weighted field scoring. Limitations of Boolean Retrieval Documents either match or donâ€™t Good for expert users with precise understanding of their needs and the collection Not good for the majority of users Most users are incapable of writing Boolean queries Or they find it takes too much effort Boolean queries often result in either too few or too many resultsRanked Retrieval Given a query, rank documents so that â€œbestâ€ results are early in the list.When result are ranked a large result, we only show the top k(~10) results Need a scoring function for document $d$ and query $q$:$$Score(d, q)$$Documents with a larger score will be ranked before those with a smaller score. Weighted field scoring term importance depends on locationAssign different weights to terms based on their location (field)! Scoring with Weighted Fields $l$ fields, let $g_i$ be the weight of field $i$ and $\\sum^l_{i=1}{g_i=1}$ $t:$ a query term, $d:$ document$$Score(d,t)=\\sum^l_{i=1}g_i\\times s_i$$ ,where $s_i = 1$ if $t$ is in the field $i$ of $d$; 0 , otherwise A score of a query term ranges [0, 1] (because $\\sum^l_{i=1}{g_i=1}$)In short: If a query term occurs in field $i$ of a document then add $g_i$ to the score for that document and term Term frequency and inverse document frequencyTerm Frequency $tf_{t,d}$ is the number occurrences of term $t$ in document $d$ Rank by Term Frequency Rank based on the frequency of query terms in documents Let $q$ be a set of query terms ${w_1, w_2, â€¦, w_m}$, and $d$ be a document, a term frequency score is$$Score_{tf}(d,q)=\\sum^m_{i=1}{tf_{wi,d}}$$ Importance of Terms Every term could have a different weight How can we reduce the weight of terms that occur too often in the collection Document Frequency Document frequency $df_t$ is the number of documents in the collection that contains term $t$ df is a good way to measure an importance of a term High frequency $\\rightarrow$ not important (like stopwords) Low frequency $\\rightarrow$ importantWhy not collection frequency? cf is sensitive to documents that contain a word many times Inverse Document Frequency Let $df_t$ be the number of documents in the collection that contain a term $t$. The inverse document frequency (IDF) can be defined as follows:$$idf_t=log\\frac{N}{df_t}$$,where $N$ is the total number of documents The idf of a rare term is high, whereas the idf of a frequent term is likely to be low TF-IDF &amp; WF-IDF The tf-idf weight of term $t$ in document $d$ is as follows:$$tf-idf_{t,d}=tf_{t,d}\\times idf_t$$ Using tf-idf weights, the score of document $d$ given query $q = (t_1, t_2, â€¦, t_m)$ is:$$Score_{tf-idf}(d,q)=\\sum^m_{i=1}tf-idf_{ti,d}$$ Sublinear tf scalingUse logarithmically weighted term frequency (wf)$$wf_{t,d}= tf_{t,d} &gt; 0? 1+log(tf_{t,d}): 0$$Logarithmic term frequency version of tf-idf$$wf-idf_{t,d}=wf_{t,d}\\times idf_f$$ Limitation tf-idf relies heavily on term frequency tf-idf score increases linearly with term frequencyClaim: the marginal gain of term frequency should decrease as term frequency increases. The relationship should be non-linear Bothe tf-idf and wf-idf prefer longer documents Maximum tf normalizationLet $tf_{max}(d)$ be the maximum frequency of any term in document $d$Normalized term frequency is defined as$$ntf_{t,d}=\\alpha+(1-\\alpha)\\frac{tf_{t,d}}{tf_{max}(d)}$$The value of ntf is in [1, $\\alpha$]Note: it can be skewed by a single term that occurs frequently Vector space IR modelDocument as VectorsGiven a term-document matrix, a document can be represented as a vector of length $V$, where $V$ is the size of vocabularyDocument Similarity in Vector Space Distance from vector to vector Angle difference between vectors Cosine similarity:$$sim(\\overrightarrow d_1, \\overrightarrow d_2)=\\frac{\\overrightarrow d_1 \\bullet \\overrightarrow d_2}{|\\overrightarrow d_1|\\times|\\overrightarrow d_2|}$$Standard way of quantifying similarity between documents, 1 if directions of two vectors are the same, 0 if directions of two vectors are orthogonalCosine similarity is not sensitive to vector magnitude Why not euclidean distanceEuclidean distance is sensitive to the vector magnitudeThe Euclidean distance of normalized vectors is proportional to cosine similarity.Query as a vector Pretend it is a document When using tf-idf the document frequency for the query vector comes from the document collection We can compute the similarity between the query vector and document vector using cosine similarity Score Function of Vector Space Model$$Score_{vsm}(d,q)=sim(\\overrightarrow{d},\\overrightarrow{q})$$We use the cosine similarity with the query to rank documents. Evaluation of IR systemsPurpose of evaluationTo build IR systems that satisfy userâ€™s information needsGiven multiple candidate systems, decide the best oneWhat do we want to evaluate System Efficiency Speed Storage Memory Cost System Effectiveness Quality of search result relevance hit rates Test collection A test collection is a collection of relevance judgment on (query, document) pairs. This relevancy information is known as the ground truth. It is typically constructed by trained human annotators. Three Components of Test Collections A collection of documents A test suite of information needs, expressible as queries A set of relevance judgment; a binary assessment of either relevant or irrelevant for each query-document pair Relevance Judgment Relevance is assessed relative to an information need, not a query A document is relevant if it addresses theinformation need. The document does not need to containall/any of the query terms Two evaluation settings: Evaluation of unranked retrieval sets (Boolean retrieval) Ranks of retrived documents are not important Retrieved (returned) documents vs. Not retrieved documents Evaluation of ranked retrieval sets Rank of retrieved documents are important Relevant documents should be ranked above irrelevent documentsEvaluation of unranked retrieval sets Contingency Table a summary table of retrieval result Relevant Not relevant Retrieved true postive(tp) false postive(fp) Not retrieved false negative(fn) true nagative(tn) tp: Number of relevant documents returned by system fp: Number of irrelevant documents returned by system fn: Number of relevant documents not returned by system tn: Number of irrelevant documents not returned by system Precision, Recall, and Accuracy Precision: fraction of retrieved documents that are relevant $$Precision=\\frac{tp}{tp+fp}$$ Recall: fraction of relevant documents that are retrieved $$Precision=\\frac{tp}{tp+fn}$$ Accuracy: fraction of relevant documents that are correct $$Accuracy=\\frac{tp+tn}{tp+tn+fp+fn}$$Accuracy is not appropriate for IR because we donâ€™t care irrelevant resultsF-Measure a single measure that trades off precision and recall$$F=\\frac{1}{\\alpha\\frac{1}{p}+(1-\\alpha)\\frac{1}{R}}, \\alpha\\in[0,1]$$ This is the weighted harmonic mean of precision(P) and recall(R) $Î± &gt; 0.5$: emphasises precision, e.g., $(Î± = 1)$ Precision $Î± &lt; 0.5$: emphasises recall, e.g., $(Î± = 0)$ Recall F1-measure: the harmonic mean of precision and recall $(Î± = 0.5)$$$F=\\frac{2PR}{P+R}$$ Evaluation of ranked retrieval setsPrecision-Recall CurveCompute recall and precision at each rank k (i.e. using the top k docs)Plot (recall, precision) points until recall is 1Interpolated Precision-RecallAt a given recall level use the maximum precision at all higher recall levels.Makes it easier to interpretIntuition: There is no disadvantage to retrieving more documents if both precision and recall improve. For system evaluation we need to average across many queries. It is not easy to average a PR curve in its current form.Solution:11-point interpolated PR curve. Interpolated precision at 11 different recall points.For system evaluation: Each point in the 11-point interpolated precision is averaged across all queries in the test collection A perfect system will have a straight line from (0,1) to (1,1) Single Number MetricsAverage Precision Average Precision is the area under the uninterpolated PR curve for a single query. MAP Mean Average Precision (MAP) is the mean of the average precision for many queries. MAP is a single figure metric across all recall levels. Good if we care about all recall levels Mean Reciprocal Rank (MRR) Averaged inverse rank of the first relevent document.For if we only care about how high in the ranking the first relevant document is. Other ranking measures Precision at K Average precision at top k documents Recall at K Average recall at top k documents Receiver Operating Characteristics (ROC) curve Normalized Discounted Cumulative Gain (NDCG) Requires graded relevance judgement Web basicsThe Web and Other NetworksDocuments on the web are linked by hyperlinks.Academic papers are linked by citations and co-authorship relations.Legal documents are linked by citations.Online users are linked by interactions or formal social network ties. Nodes and EdgesNodes represent entities (e.g. documents, people)Edges are relationships between nodes. (e.g. hyperlinks, co-authorship relations)Edges can be directed (go in a specific direction) Degree of a Node Degree: the number of edges connected to a node In-degree: the number of edges going to a node Out-degree: the number of edges coming from a node. Link Analysis Link analysis uses information about the structure of the web graph to aid search It is one of the major innovations in web search. It was one of the primary reasons for Googleâ€™s initial success.Citation Analysis Many documents include bibliographies with citations to other previously published documents Using citations as edges, a collection of documents can be viewed as a graph. The structure of this graph can provide interesting information about the similarity of documents and the structure of information. Even when document content is ignored Impact Factor of a Scientific Journal measure the importance (quality, influence) of scientific journalsA measure of how often papers in a journal are citedComputed and published annually by the Institute for Scientific InformationThe impact factor of a journal $J$ in year $Y$ is the average number of citations (from indexed documents published in year $Y$) to a paper published in $J$ in year Yâˆ’1 or Yâˆ’2.Does not account for the quality of the citing article. Bibliographic Coupling Measure of similarity of documents introduced by Kessler in 1963.The bibliographic coupling of two documents A and B is the number of documents cited by both A and B.Size of the intersection of their bibliographies. Co-Citation An alternate citation-based measure of similarity introduced by Small in 1973Number of documents that cite both A and B. Citations vs. Link Many links are navigational Many pages with high in-degree are portals not content providers Not all links are endorsements Company websites donâ€™t point to their competitors Citations to relevant literature is enforced by peer-review.Authorities and Hubs: HITS algorithm Authorities Authorities are pages that are recognized as providing significant, trustworthy, and useful information on a topic. In-degree (number of pointers to a page) is one simple measure of authority However, in-degree treats all links as equal. Hubs Hubs are index pages that provide lots of useful links to relevant content pages (authorities). HITS Determines hubs and authorities for a particular topic through analysis of a relevant subgraph Based on mutually recursive, assumptions: Hubs point to lots of authorities. Authorities are pointed to by lots of hubs HITS Algorithm Computes hub and authority scores for documents on a particular topic The topic is specified by a query Relevant pages from the query are used to construct a base subgraph S. Analyze the link structure of pages in S to find authority and hub pages. Constructing a Base Subgraph S: For a specific query, let the set of documents returned by a standard search engine be called the root set R. Initialize S to R. Add to S all pages pointed to by any page in R Add to S all pages that point to any page in R. Base Limitations: To limit computational expense: Limit the number of root pages to the top 200 pages retrieved for the query â€“ Limit the number of â€œback-pointerâ€ pages to a random set of at most 50 pages returned by a â€œreverse linkâ€ To eliminate purely navigational links Eliminate links between two pages on the same host. To eliminate â€œnon-authority-conveyingâ€ links: Allow only m ($m \\in[4,8]$) pages from a given host as pointers to any individual page query Authorities and In-DegreeEven within the base set S for a given query, the nodes with highest in-degree are not necessarily authorities (may just be generally popular pages like Yahoo or Amazon)Authority pages are pointed to by several hubs (i.e. pages that point to lots of authorities). Iterative AlgorithmUse an iterative algorithm to slowly converge on a mutually reinforcing set of hubs and authorities: Maintain for each page $p\\in S$: Authority score: $a_p$ (vector a) Hub score: $h_p$ (vector h) Initialize all $a_p=h_p=1$ Maintain normalized scores:$\\sum_{p\\in S}(a_p)^2=1$$\\sum_{p\\in S}(h_p)^2=1$ HITS Update Rules Authorities are pointed to by lots of good hubs $a_p=\\sum_{q:q\\rightarrow p}h_q$ Hubs point to lots of good authorities: $h_p=\\sum_{q:p\\rightarrow q}a_q$ HITS Iterative AlgorithmInitialize for all $p \\in S: a_p = h_p = 1$For $i = 1$ to $k$: For all $p \\in S$: $a_p=\\sum_{q:q\\rightarrow p}h_q$ (update auth. scores) For all $p\\in S$: $h_p=\\sum_{q:p\\rightarrow q}a_q$(update hub scores) For all $p\\in S$: $a_p= a_p/c, c: \\sum_{p\\in S}(a_p/c)^2=1$ (normalize a)For all $p\\in S$: $h_p= h_p/c, c: \\sum_{p\\in S}(h_p/c)^2=1$ (normalize h) Convergence Algorithm converges to a fix-point if iterated indefinitely. Define A to be the adjacency matrix for the subgraph defined by S. $A_{ij} = 1$ for $i \\in S, j \\in S$ iff $i\\rightarrow j$ Authority vector, $a$, converges to the principal eigenvector of $A^TA$ Hub vector, $h$, converges to the principal eigenvector of $AA^T$ In practice, 20 iterations produces fairly stable results Finding Similar Pages Using Link Structure Given a page, P, let R (the root set) be t (e.g. 200) pages that point to P. Grow a base set S from R. Run HITS on S. Return the best authorities in S as the best similar-pages for P Finds authorities in the â€œlink neighborhoodâ€ of P PageRank Does not attempt to capture the distinction between hubs and authoritiesGives each page a score which measures authorityApplied to the entire document corpus rather than a local neighborhood of pages surrounding the results of a query PageRank AlgorithmLet $S$ be the total set of pages.Let $\\forall p\\in S:E(p)=\\alpha/|S|$ (for some $0&lt;\\alpha&lt;1$)Initialize $\\forall p\\in S: R(p) = 1/|S|$Until ranks do not change (much) (convergence) For each $p\\in S$: $Râ€™(P)=[(1-\\alpha)\\sum_{q:q\\rightarrow p}\\frac{R(q)}{N_q}+E(p)$ $c=1/\\sum_{p\\in S}Râ€™(p)$ for each $p\\in S: R(p) = cRÂ´(p)$ (normalize) $N_q$ is the total number of out-links from page q. A page, q, â€œgivesâ€ an equal fraction of its authority to all the pages it points to (e.g. p). c is a normalizing constant set so that the rank of all pages always sums to 1 a â€œrank sourceâ€ E that continually replenishes the rank of each page, p, by a fixed amount E(p) Speed of ConvergenceNumber of iterations required for convergence is empirically O(log n) (where n is the number of links). Therefore, calculation is efficient Machine LearningML Intro and notationSupervised Learning Goal: given both the inputs and the outputs for a training dataset, find a function h of the inputs that that returns the correct output (or as close as possible) Categorical output $\\Rightarrow$ classification problem Scalar output $\\Rightarrow$ regression problem Terminologydataset, corpus, training set, collection, set of examples Objects (samples, observations, individuals, examples, data points) Variables (attributes, features) = describes a objects Dimension = number of variables Size = number of objects Notation examples / data points / input $ğ’™^{(1)} ,â€¦, ğ’™^{(ğ‘›)}$ ~$X$ labels / annotations / target $y^{(1)} ,â€¦, y^{(ğ‘›)}$~$Y$ predicator / model ğ‘“$_w$ ğ’™ : ğ‘‹ $\\rightarrow$ Y features of datapoint k ğ’™$^{(ğ‘˜)}$ = (ğ‘¥$_1^{(ğ‘˜)}$ ,ğ‘¥$_2^{(ğ‘˜)}$ â€¦. ,ğ‘¥$_m^{(ğ‘˜)}$ ) Predictions from the model $\\hat y$ = ğ‘“$_w($ğ’™$)$ Bold denotes vectors Linear Regression We find the best linear function to model our data points To make a prediction for a new point we just evaluate our function at that point What do we need? A way of expressing linear functions A way to measure how well the line fits the data (called a loss function) A way to find the line with the smallest loss given the data Gradient descent Linear Functions:One Feature A straight line can be written $\\hat y=wx+b$ ğ‘¤ - is the slope of the line (parameter) ğ‘¥ - is the feature (input) ğ‘ - is the bias (parameter) $\\hat y$ - is the prediction of the target variable (output) Many Features We can have a linear function with many features: $\\hat y=ğ‘¤_1ğ‘¥_1 + ğ‘¤_2ğ‘¥_2 + ğ‘¤_3ğ‘¥_3+â€¦ +b$ Written more compactly for ğ‘š features: $\\hat y = b+\\sum_{i=0}^m w_ix_i$ Written in vector form with vectors $\\vec w$ and $\\vec x$: $\\hat y = b+\\vec w\\vec x$ Multiple Linear RegressionIf we have more than one target per example, then we can use a different set of parameters for each target: $$\\hat y_1=ğ‘¤_{1,1}ğ‘¥1 + ğ‘¤{1,2}ğ‘¥2 + ğ‘¤{1,3}ğ‘¥_3+â€¦ +b$$ $$\\hat y_2=ğ‘¤_{2,1}ğ‘¥1 + ğ‘¤{2,2}ğ‘¥2 + ğ‘¤{2,3}ğ‘¥_3+â€¦ +b$$ $$\\hat y_3 = w_{3,1}ğ‘¥1 + w{3,2}ğ‘¥2 + w{3,3}ğ‘¥_3+â€¦ +b$$ In matrix/vector form this is (for matrix $W$, and vectors: $\\vec {\\hat y}$ , $\\vec x$, $\\vec b$): $\\hat y = b+W\\vec x$ Loss Function Measures how well the line models the data: $ğ¿ = \\sum_{i=1}^ğ‘› (\\hat y^{(ğ‘–)} âˆ’ ğ‘¦^{(ğ‘–)})^2$ $ğ¿ = \\sum_{i=1}^ğ‘› (wx^{(ğ‘–)}+b âˆ’ ğ‘¦^{(ğ‘–)})^2$ $ğ’™^{(ğ‘–)}$ - the features of the iâ€™th datapoint $ğ‘¦^{(ğ‘–)}$ - the ground truth target of the iâ€™th datapoint $\\hat y^{(ğ‘–)}$ - the predicted target of the iâ€™th datapoint OptimizationWe have a loss function ğ¿ which measures how good a particular line (ğ‘¤, ğ‘) is for our training dataset D We want to solve: argmin$_{ğ‘¤,ğ‘}$ ğ¿(ğ‘¤, ğ‘,ğ·) There are many different algorithms for solving optimization problems, gradient descent is a very popular one when everything is differentiable Gradient Descent A greedy strategy for optimization Start by picking values for ğ‘¤ and ğ‘, e.g. by sampling fromğ‘(0,1). Then compute the gradients $\\frac{ğ‘‘ğ¿}{ğ‘‘ğ‘¤}$ and $\\frac{ğ‘‘ğ¿}{ğ‘‘ğ‘}$ Then update: $w:=w-\\alpha \\frac{ğ‘‘ğ¿}{ğ‘‘ğ‘¤}$ $b:=b-\\alpha \\frac{ğ‘‘ğ¿}{ğ‘‘b}$ ğ›¼ is the learning rate, it controls how much the values change each step. The role of learning rateThe learning rate ğ›¼ is a hyper-parameter that you set, it controls how much the weights are changed in each step. Too small ğ›¼ â‡’ need to take many steps. Too large ğ›¼ â‡’ may not converge. Multinomial Logistic RegressionClassificationTo use a linear model ğ‘Šğ’™ + ğ’ƒ (multiple linear regression) We can modify the task to predicting the probability that an example belongs to each class. Suppose that each point can be labelled with one of ğ‘‚ different classes. Then our model $\\bar P$(ğ’š|ğ’™) = ğ‘Šğ’™ + ğ’ƒ outputs a vector of size $O$. $\\bar P$ denotes unnormalized probabilities. (often called logits). These may be negative. From Logits to Probabilities The output values of the linear model are not probabilities: Need all the output values to be non-negative. Need output values to sum up to 1. $$softmax(v)i=\\frac{exp(v_i)}{\\sum^o{j=0}exp(v_j)}$$ Step 1: Apply exp to all values. This makes them positive Step 2: Divide each value by the sum of all values. This makes them sum to 1. The result is a categorical probability distribution Multinomial Logistic Regression $P(y \\mid x)$ = softmax (ğ‘Šğ’™ + ğ’ƒ) Classification: Loss Compute loss between our predicted probabilities and onehot encoding of class label $y$, i.e. the probability of the correct class should be 1, all others should be 0. $L$(softmax ğ‘Šğ’™ + ğ’ƒ , ğ’š) Classification: Cross-Entropy We could use the sum of squared differences as our loss function, just like for regression. In practice, using cross-entropy as the loss function for classification gives better results. Cross entropy for one datapoint: CrossEntropy(ğ’™, ğ’š)=$-\\sum_{i=1}^oy_ilogP(y_i\\mid ğ’™)$ Classification: Prediction To make a prediction for a new data point using the model compute probabilities for each class. predict the class with the largest probability. Representation in NLPRepresentation Text represented as a string of bits/characters/words is hard to work with Variable length High dimensional Similar representations may have very different meanings Meaning Meaning in language is: Relational (based on relationships) Compositional (built from smaller components) Distributional (related to usage context) Simple document representationDocument Representation We can represent documents as vectors of the words/terms they contain (BoW model) Vector may be Binary occurrence Word count TFIDF scores The vector representation may be the size of the vocabulary ~ 50000 words is common Very inefficient for many documents Fortunately, most documents do not contain most words so we can use a sparse representation with tuples of the form: (term_id, term_count) for tuples where term_count &gt; 0 Word representationWord Representation (One-hot) In traditional NLP, we regard words as discrete symbols Vector dimension = number of words in vocabulary hotel = 10 = [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] motel = 7 = [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0] issue: Similar words do not have similar vectors Context-based Word Representation Sentence Window Use k words on either side of the focal word as the context Word Co-occurrence Matrix A co-occurance matrix of words gives a vector representation for each word. This gives equal importance to all context words Weighting tf-idf PPMI: Positive Pointwise Mutual Information (more common) Pointwise Mutual Information The ratio of the probability that the words occur together compared to the probability that they would occur together by chance The amount of information the occurrence of a word $ğ‘¤_ğ‘–$ gives us about the occurrence of another word $ğ‘¤_ğ‘—$ . (and vise versa) PMI can be negative when words occur together less frequently than by chance. Typically, negative values are not reliable. We set them to 0: ğ‘ƒğ‘ƒğ‘€ğ¼ = max(0,ğ‘ƒğ‘€ğ¼) Similarity with Pointwise Mutual Information ğ‘ ğ‘–ğ‘š(x, y) = cos($ğ‘£_x,ğ‘£_y$) Drawbacks of a Sparse Representation The raw count / tf-idf / PPMI matrix is: High dimensional, thus more difficult to use in practice. Suffers from sparsity. Hard to find similarity between words Singular Value Decomposition reduce the dimensionality of a word-word co-occurrence matrix Word2VecWord Representation (Word Vector) dense word vectors are sometimes called word embeddings or word representations. They are distributed representations. Typical number of dimensions: 64, 128, 256, 300, 512, 1024 Several Word2Vec Algorithms: Most common is skip-gram with negative sampling Approach: A self-supervised classification problem We learn word embeddings to do classification In the end we care only about the embeddings Skip-gram Word2Vec as Logistic Regression Word2Vec uses a multinomial logistic regression classifier (without a bias term): P(y | x) = softmax($ğ‘Šx$) Here $y$ is the context word and $x$ is an embedding of the center word. In this model we can interpret the matrix $W$ as being composed of embedding of the context words Model Once trained with cross-entropy loss we use $ğ‘£_ğ‘$ as the word embedding and throw everything else away. Word2VecWhen we train word2vec we also train the center word embeddings How: Start with random embeddings Backpropagation to compute gradient (next lecture) Gradient descent Loss function (average cross entropy): Natural Language ProcessingLanguage Modelling Goal: assign a probability to a word sequence Speech recognition Spelling correction Collocation error correction Machine Translation Question-answering, summarisation, image capturing etc Probabilitistic Language Modelling A language model computes the probability of a sequence of words A vocabulary $V$ $p(x_1, x_2, \\dots, x_l)\\ge 0$ $\\sum p(x_1, x_2, \\dots, x_l)=1$ Related task: probability of an upcoming word ($p(x_1\\mid x_1, x_2, x_3)$) LM: Either $p(x_1\\mid x_1, x_2, x_3)$ or $p(x_1, x_2,\\dots, x_l)$ Probability CalculationChain rule To compute $p(x_1, x_2, â€¦, x_l)$ use Chain rule:$$p(x_1, x_2, â€¦, x_l)=p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\\dots(x_l|x_1,â€¦,x_{l-1})$$Drawbacks: it is likely that the whole context $(x_1, x_2, â€¦, x_l)$ doesnâ€™t appear in the data, resulting the probability of $0$. Markov Assumption simplifying assumption instead of using the whole context, use one or two most recent words Zero-order Markov assumption(Unigram Model)$$P(x_1, x_2, \\dots, x_l)=P(x_1)\\underset{i=1}{\\overset{l}\\Pi}P(x_i)$$ Not very good First-order Markov assumption (Bigram Model): Assumes only most recent words are relevant $$\\begin{align}P(x_1, x_2, \\dots, x_l)&amp;=P(x_1)\\underset{i=2}{\\overset{l}\\Pi}P(x_i\\mid x_1,â€¦,x_{x-1})\\&amp;=P(x_1)\\underset{i=2}{\\overset{l}\\Pi}P(x_i\\mid x_{x-1})\\end{align}$$ Maximum likelihood estimation:$$P(x_i\\mid x_{i-1})=\\frac{count(x_{i-1}, x_i)}{count(x_i)}$$ Log probabilities: logP(I want to eat) = logP(I)+logP(want|I)+logP(to|want)+logP(eat|to) using log can make probabilities in a certain range (it wonâ€™t be very small) Log helps with numerical stability (probabilities get small) Second-order Markov assumption (Trigram Model):$$P(x_1, x_2, \\dots, x_l)=P(x_1)\\underset{i=1}{\\overset{l}\\Pi}P(x_i|x_{i-1},x_{i-2})$$ can handle long-distance of language can extend to 4-gram, 5-gramâ€¦ Sequence GenerationCompute conditional probabilities: e.g. P(want|I) = 0.32 Sampling: Ensures you donâ€™t just get the same sentence all the time Generate a random number in [0,1] (based on a uniform distribution) Words are better fit are more likely to be selected Interpolation mix lower order n-gram probabilities $\\lambda$s are hyperparameters To handle the unseen words and the issue of $0$ probability (phrase with $0$ probability but single word may not) Bigram models:$$\\hat P(x_i\\mid x_{x-1})=\\lambda_1P(x_i\\mid x_{i-1})+\\lambda_2P(x_i\\mid x_{i})\\\\lambda_1\\ge 0, \\lambda_2\\ge 0\\\\lambda_1+\\lambda_2=1$$Trigram models:$$\\hat P(x_i\\mid x_{x-1}, x_{i-2})=\\lambda_1P(x_i\\mid x_{i-1}, x_{i-2})+\\lambda_2P(x_i\\mid x_{i-1})+\\lambda_3P(x_i\\mid x_{i})\\\\lambda_1\\ge 0, \\lambda_2\\ge 0, \\lambda_2\\ge 0\\\\lambda_1+\\lambda_2+ \\lambda_30=1$$How to fit $\\lambda$s Estimate $\\lambda_i$ on held-out data Training data, held-out data, test data Typically use Expectation Maximization (EM) One crude approach$$\\begin{align}\\lambda_1&amp;=\\frac{count(x_{i-1}, x_{i-2})}{count(x_{i-1}, x_{i-2})+\\gamma}\\\\lambda_2&amp;=(1-\\lambda_1)\\times\\frac{count(x_{i-1})}{count(x_{i-1})+\\gamma}\\\\lambda_3&amp;=1-\\lambda_1-\\lambda_2\\\\end{align}$$ Ensures $\\lambda_i$ is larger when count is larger Different lambda for each n-gram Only one parameter to estimate ($\\gamma$) Absolute-Discounting Interpolation We tend to systematically overestimate n-gram counts $\\longrightarrow$ we can use this to get better estimates and set lambdas. Using discounting Involves the interpolation of lower and higher-order models Aims to deal with sequences that occur infrequently Subtract $d$ (the discount, typically $0.75$) from each of the counts$$P_{AbsDiscount}(x_i\\mid x_{x-1})=\\frac{count(x_{i-1}, x_i)-d}{count(x_i)}+\\lambda(x_{i-1})P(x_{i-1})$$ The discount reserves some probabilities mass for the lower order n-grams Thus, the discount determines the $\\lambda$ Kneser-Ney Smoothing A techique built on top of Absolute-Discounting Interpolation. â€œSmoothingâ€ $\\rightarrow$ Adjust low probs upwards and high probs downwards If there are only a few words that come after a context, then a novel word in that context should be less likely But we also expect that if a word appears after a small number of contexts, then it should be less likely to appear in a novel context Works well and also used to improve NN approaches. Provides better estimates for probabilities of lower-order unigrams: $P_{continuation}(X)$ How likely is a word to continue a new context? For each word $x_i$, count the number of bigram types it completes$$P_{continuation}(x_i)\\propto\\mid{x_{i-1}\\mid count(x_{i-1}, x_i)}\\mid$$i.e. the unigram probability is proportional to the number of different words it follows Normalized by the total number of bigram types:$$\\mid{(x_{j-1}, x_j)\\mid count(x_{i-1}, x_i)&gt;0}\\mid\\P_{continuation}(x_i)=\\frac{\\mid{x_{i-1}\\mid count(x_{i-1}, x_i)}\\mid}{\\mid{(x_{j-1}, x_j)\\mid count(x_{i-1}, x_i)&gt;0}\\mid}$$Estimate of how likely a unigram is to continue a new context Kneser-Ney Smoothing definition for bigrams:$$P_{KN}(x_i\\mid x_{i-1})=\\frac{max(count(x_{i-i},x_i)-d, 0)}{count(x_{i-1})}+\\lambda(x_{i-1})P_{continuation}(x_i)$$where$$\\lambda(x_{i-1})=\\frac{d}{count(x_{i-1})}\\mid{x\\mid count(x_{i-1},x)&gt;0}\\mid$$Stupid Backoff Smoothing for web-scale N-grams No discounting, just use relative frequencies! Does not give a probability distribution (Gives a score)$$S(w_i\\mid w^i_{i-k+1})=\\begin{cases}\\frac{count(w^i_{i-k+1})}{count(w^{i-1}{i-k+1})} \\text{ if }count(w^i{i-k+1})&gt;0\\0.4S(w_i\\mid w^i_{i-k+2})\\text{ otherwise}\\end{cases}\\S(w_i)=\\frac{count(w^i)}{N}$$$N$ is the size of the training corpus in words","link":"/2021/09/23/Document%20Analysis/"},{"title":"Foundation of Concurrency","text":"Gain more insight about concurrency. Semantics#mermaid-1633958236764 .label{font-family:'trebuchet ms', verdana, arial;color:#333}#mermaid-1633958236764 .node rect,#mermaid-1633958236764 .node circle,#mermaid-1633958236764 .node ellipse,#mermaid-1633958236764 .node polygon{fill:#ECECFF;stroke:#9370db;stroke-width:1px}#mermaid-1633958236764 .node.clickable{cursor:pointer}#mermaid-1633958236764 .arrowheadPath{fill:#333}#mermaid-1633958236764 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1633958236764 .edgeLabel{background-color:#e8e8e8}#mermaid-1633958236764 .cluster rect{fill:#ffffde !important;stroke:#aa3 !important;stroke-width:1px !important}#mermaid-1633958236764 .cluster text{fill:#333}#mermaid-1633958236764 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:'trebuchet ms', verdana, arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1633958236764 .actor{stroke:#ccf;fill:#ECECFF}#mermaid-1633958236764 text.actor{fill:#000;stroke:none}#mermaid-1633958236764 .actor-line{stroke:grey}#mermaid-1633958236764 .messageLine0{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1633958236764 .messageLine1{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1633958236764 #arrowhead{fill:#333}#mermaid-1633958236764 #crosshead path{fill:#333 !important;stroke:#333 !important}#mermaid-1633958236764 .messageText{fill:#333;stroke:none}#mermaid-1633958236764 .labelBox{stroke:#ccf;fill:#ECECFF}#mermaid-1633958236764 .labelText{fill:#000;stroke:none}#mermaid-1633958236764 .loopText{fill:#000;stroke:none}#mermaid-1633958236764 .loopLine{stroke-width:2;stroke-dasharray:'2 2';stroke:#ccf}#mermaid-1633958236764 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1633958236764 .noteText{fill:black;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:14px}#mermaid-1633958236764 .activation0{fill:#f4f4f4;stroke:#666}#mermaid-1633958236764 .activation1{fill:#f4f4f4;stroke:#666}#mermaid-1633958236764 .activation2{fill:#f4f4f4;stroke:#666}#mermaid-1633958236764 .section{stroke:none;opacity:0.2}#mermaid-1633958236764 .section0{fill:rgba(102,102,255,0.49)}#mermaid-1633958236764 .section2{fill:#fff400}#mermaid-1633958236764 .section1,#mermaid-1633958236764 .section3{fill:#fff;opacity:0.2}#mermaid-1633958236764 .sectionTitle0{fill:#333}#mermaid-1633958236764 .sectionTitle1{fill:#333}#mermaid-1633958236764 .sectionTitle2{fill:#333}#mermaid-1633958236764 .sectionTitle3{fill:#333}#mermaid-1633958236764 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1633958236764 .grid .tick{stroke:#d3d3d3;opacity:0.3;shape-rendering:crispEdges}#mermaid-1633958236764 .grid path{stroke-width:0}#mermaid-1633958236764 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1633958236764 .task{stroke-width:2}#mermaid-1633958236764 .taskText{text-anchor:middle;font-size:11px}#mermaid-1633958236764 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1633958236764 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1633958236764 .taskText0,#mermaid-1633958236764 .taskText1,#mermaid-1633958236764 .taskText2,#mermaid-1633958236764 .taskText3{fill:#fff}#mermaid-1633958236764 .task0,#mermaid-1633958236764 .task1,#mermaid-1633958236764 .task2,#mermaid-1633958236764 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1633958236764 .taskTextOutside0,#mermaid-1633958236764 .taskTextOutside2{fill:#000}#mermaid-1633958236764 .taskTextOutside1,#mermaid-1633958236764 .taskTextOutside3{fill:#000}#mermaid-1633958236764 .active0,#mermaid-1633958236764 .active1,#mermaid-1633958236764 .active2,#mermaid-1633958236764 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1633958236764 .activeText0,#mermaid-1633958236764 .activeText1,#mermaid-1633958236764 .activeText2,#mermaid-1633958236764 .activeText3{fill:#000 !important}#mermaid-1633958236764 .done0,#mermaid-1633958236764 .done1,#mermaid-1633958236764 .done2,#mermaid-1633958236764 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1633958236764 .doneText0,#mermaid-1633958236764 .doneText1,#mermaid-1633958236764 .doneText2,#mermaid-1633958236764 .doneText3{fill:#000 !important}#mermaid-1633958236764 .crit0,#mermaid-1633958236764 .crit1,#mermaid-1633958236764 .crit2,#mermaid-1633958236764 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1633958236764 .activeCrit0,#mermaid-1633958236764 .activeCrit1,#mermaid-1633958236764 .activeCrit2,#mermaid-1633958236764 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1633958236764 .doneCrit0,#mermaid-1633958236764 .doneCrit1,#mermaid-1633958236764 .doneCrit2,#mermaid-1633958236764 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1633958236764 .doneCritText0,#mermaid-1633958236764 .doneCritText1,#mermaid-1633958236764 .doneCritText2,#mermaid-1633958236764 .doneCritText3{fill:#000 !important}#mermaid-1633958236764 .activeCritText0,#mermaid-1633958236764 .activeCritText1,#mermaid-1633958236764 .activeCritText2,#mermaid-1633958236764 .activeCritText3{fill:#000 !important}#mermaid-1633958236764 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1633958236764 g.classGroup text{fill:#9370db;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:10px}#mermaid-1633958236764 g.classGroup rect{fill:#ECECFF;stroke:#9370db}#mermaid-1633958236764 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1633958236764 .classLabel .box{stroke:none;stroke-width:0;fill:#ECECFF;opacity:0.5}#mermaid-1633958236764 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1633958236764 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1633958236764 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #compositionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #aggregationStart{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #aggregationEnd{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #dependencyStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #dependencyEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 #extensionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1633958236764 .commit-id,#mermaid-1633958236764 .commit-msg,#mermaid-1633958236764 .branch-label{fill:lightgrey;color:lightgrey} #mermaid-1633958236764 { color: rgb(0, 0, 0); font: normal normal 400 normal 16px / normal \"Times New Roman\"; }PseudocodeCCSLTSKSbehavioursLTL Notes Notes made by Liam. To use as a guide for revision. Bridging the GapWe have seen several semantic models for concurrent systems in this course. Initially, when we introduced Linear Temporal Logic, we viewed concurrent processes as sets of behaviours, where behaviours were infinite sequences of states. This model is very nice for specifying the semantics of LTL, which we did in Week 1. Later on, though, we used different semantic models, such as Transition Diagrams and Ben-Ariâ€™s pseudocode. We introduced the Calculus of Communicating Systems (CCS), a type of process algebra, to construct Labelled Transition Systems, analogous to our transition diagrams. This was intended to give us a formal language that is semantically connected to our diagrammatic notation. There is a missing link, however, between these two semantic notions: The world of CCS and labelled transition systems on one hand, and the world of behaviours and LTL properties on the other. We examined the two CCS processes $a â‹… (b + c)$ and $a â‹… b + a â‹… c$ and concluded that LTL would not be able to distinguish these two processes, even though they are not considered equal (bisimilar) in CCS. If we were to try to add the equation $aâ‹…(P + Q) = a â‹… P + a â‹… Q$ into CCS, then we would identify those two processes above, but we would also identify $a â‹… b + a$ with $a â‹… b$, and clearly one of these satisfies $â—Šb$ and the other doesnâ€™t! This is because the semantic equivalence we get by adding that equation is called partial trace equivalence. As mentioned in lectures, partial trace equivalence is like looking at all the possible sequences of actions (traces) in the process, including those sequences that do not take available transitions. So, the traces of $aâ‹…b+a$ are $Ïµ$ (the empty trace), $a$, and $ab$ â€“ exactly the same as the partial traces of $aâ‹…b$. Because partial trace equivalence includes these incomplete traces, using it is basically giving up on the vital progress assumption. Progress is the assumption that a process will take an action if one is available. To get the set of behaviours we expect for our LTL semantics, we need some way to choose which traces we want to keep and which ones we donâ€™t. This is the definition of what counts as a completed trace. There are many different completeness criteria. Progress is the weakest, and the most common. Other completeness criteria include weak and strong fairness, which we have seen already, and justness which I will describe below. ProgressA simple way to view progress as a completeness criteria is that it says a path is complete iff it is infinite or if it ends in a state with no outgoing transitions. This rules out all paths which end in states where they could take an outgoing transition but donâ€™t. Weak FairnessWeak and strong fairness are defined in terms of tasks. A task is a set of transitions. Weak fairness as a property of paths can be expressed by the LTL formulas$$â—»(â—»(enabled(t))â‡’â—Š(taken(t)))$$for each task tt. Here enabled(t)enabled(t) is an atomic proposition that is true if a transition in tt is enabled. Likewise taken(t)taken(t) holds if tt is taken in that state. Given a process $P=aâ‹…P+b$, weak fairness would state that eventually $b$ must occur, as we would not be able to loop infinitely around $a$, never taking the (forever enabled) $b$ transition. Viewed as a completeness criterion, weak fairness rules out all traces which do not obey the property above. Note that it implies progress, because any trace that will eventually take a forever enabled transition will surely not be able to sit in a state without taking an available transition. Strong FairnessStrong fairness is similar to weak fairness, except that the property has one extra operator:$$â—»(â—»â—Š(enabled(t))â‡’â—Š(taken(t))))$$This is saying that our task tt does not have to be forever enabled, just enabled infinitely often. This means a process can go away and come back. For example, the process $P=aâ‹…aâ‹…P+b$ would not eventually do $b$ under weak fairness, as after one $a$ the $b$ transition is no longer enabled. However because the infinite aa path has $b$ available infinitely often, strong fairness would require that $b$ is eventually taken. Viewed as a completeness criterion, this rules out all properties that donâ€™t obey the property above. It can be proven in LTL that strong fairness implies weak fairness. JustnessJustness is a slightly harder concept to formalise. It is weaker than both strong and weak fairness, but stronger than progress. Essentially, it is a criterion that says that individual components should be able to make progress on their own. For example, imagine a system that consists of three components, one which always does aa, another which always does bb, and another which synchronises with aa: $A=aâ‹…A$ $B=bâ‹…B$ $C=\\bar aâ‹…A$ System$=A|B|C$ Now, a valid trace of this system would be simply $bbbbbâ‹¯$ forever. This would satisfy progress, as a transition is always being taken. However, the actions $a$ and $\\bar a$ that could occur, and the communication $Ï„$ transition that could between $A$ and $C$ are never taken in this trace. Justness says that each component of the system will always make progress if their resources are available. In other words, $B$ doing unrelated $b$ moves should never prevent $A$ and $C$ from communicating. Viewed as a completeness criterion, this is stronger than progress, as it is essentially the same as progress except applied locally, rather than globally. It is weaker than weak fairness, as justness would not require that $A=aâ‹…A+b$ eventually does a $b$, as all transitions are from the same component ($A$). Kripke StructuresThese traces that we have examined are still sequences of actions, not states. Behaviours, however, are sequences of states. Normally, to convert our labelled transition systems into something we can reason about in LTL, we first translate them into an automata called a Kripke Structure. A Kripke structure is a 4-tuple $(S,I,â†¦,L)$ which contains a set of states $S$, an initial state $I$, a transition relation $â†¦$ which, unlike a labelled transition system, does not have any action labels on the transitions, and a labelling function $L$ which associates to every state $S$ a (set of) atomic propositions â€“ these are the atomic propositions we use in our LTL formulae. A Kripke structure for an OS process behaviour was actually shown in the lecture on temporal logic in Week 1, I just never told you it was a Kripke structure. Kripke Structures deal with states, not transition actions. This means, to translate from a labelled transition system to a Kripke structure, we need a way to move labels from transitions to states. The simplest translation is due to de Nicola and Vaandrager, where each transition $s_i\\xrightarrow a s_j$ in the LTS is split into two in the Kripke Structure: $s_iâ†’X$ and $Xâ†’s_j$, where $X$ is a new state that is labelled with $a$. Because this converts existing LTS locations into blank, unlabelled states in the Kripke structure, this introduces problems with the next state operator in LTL. For this reason (and others) we usually consider LTL without the next state operator in this field. Normally with LTL, we require that Kripke structures have no deadlock states, that is, states with no outgoing transitions. The usual solution here is to add a self loop to all terminal states. We can extract our normal notion of a behaviour by using the progress completeness criterion. Because of the restriction above, the progress criterion is equivalent to examining only the infinite runs of the automata. Putting it all togetherNow we can: Translate Pseudocode to CCS or transition diagrams Translate transition systems to Kripke structures Extract behaviours from Kripke structures using various completeness criteria Specify LTL properties about those behaviours Now we have connected all the semantic models used in the course. PseudocodeCalculus of Communicating Systems(CCS)The Calculus of Communicating Systems: Is a process algebra, a simple formal language to describe concurrent systems. Is given semantics in terms of labelled transition systems. Was developed by Turing-award winner Robin Milner in the 1980s Has an abstract view of synchronization that applies well to message passing. Processes Processes in CCS are defined by equations: The equation:$$\\textbf{CLOCK} = \\text{tick}$$defines a process CLOCK that simply executes the action_ â€œtickâ€ and then terminates. This process corresponds to the first location in this **_labelled transition system** (LTS):$$\\bullet\\xrightarrow{\\text{tick}}\\bullet$$An LTS is like a transition diagram, save that our transitions are just abstract actions and we have no initial or final location. Action PrefixingIf a is an action and $P$ is a process, then $x.P$ is a process that executes $x$ before $P$. This brackets to the right, so:$$x.y.z.P = x.(y.(z.P))$$Example$$\\textbf{CLOCK}_2 = \\text{tick.tock}$$defines a process called CLOCK$_2$ that executes the action â€œtickâ€ then the action â€œtockâ€ and then terminates$$\\bullet\\xrightarrow{\\text{tick}}\\bullet\\xrightarrow{\\text{tock}}\\bullet$$The process:$$\\textbf{CLOCK}_3 = \\text{tock.tick}$$has the same actions as CLOCK$_2$ but arranges them in another order. StoppingMore precisely, we should write:$$\\textbf{CLOCK}_2 = \\text{tick.tock.}\\textbf{STOP}$$where STOP is the trivial process with no transitions. LoopsUp to now, all processes make a finite number of transitions and then terminate. Processes that can make a infinite number of transitions can be pictured by allowing loops: CLOCK$_4 =$ tick.CLOCK$_4$ CLOCK$5_ =$ tick.tick,CLOCK$_5$ We accomplish loops in CCS using recursion. Equality of ProcessesThese two processes(CLOCK$_4$ and CLOCK$_5$) are physically different: But they both have the same behaviour â€” an infinite sequence of â€œtickâ€ transitions. (Informal definition) We consider two process to be equal if an external observer cannot distinguish them by their actions. We will refine this definition later. Choice If $P$ and $Q$ are processes then $P + Q$ is a process which can either behave as the process $P$ or the process $Q$. Choice Equalities$$\\begin{align}&amp;P + (Q + R) &amp;= \\space&amp;(P + Q) + R &amp;(\\text{associativity})\\\\&amp;P + Q &amp;= \\space&amp;Q + P &amp;(\\text{commutativity})\\\\&amp;P + STOP &amp;= \\space&amp;P &amp;(\\text{neutral element})\\\\&amp;P + P &amp;= \\space&amp;P &amp;(\\text{idempotence})\\end{align}$$ What about the equation:$$a.(P + Q)=^?(a.P) + (a.Q)$$ Branching TimeExmaple: $\\textbf{VM}_1 = \\text{in}50Â¢.(\\text{outCoke + outPepsi})$ $ \\textbf{VM}_2 = (\\text{in}50Â¢.\\text{outCoke}) + (\\text{in}50Â¢.\\text{outPepsi})$ Reactive Systems VM$_1$ allows the customer to choose which drink to vend after inserting 50Â¢. In VM$_2$ however, the machine makes the choice when the customer inserts a coin. They different in this reactive view, but they have the same behaviours! EquivalencesThe equation$$a.(P + Q) = (a.P) + (a.Q)$$is usually not admitted for this reason. If we do admit it, then our notion of equality is very coarse (it is called partial trace equivalence). This is enough if we want to prove safety properties, but progress is not guaranteed. Our notion of equality without this equation is called (strong) bisimulation equivalence or (strong) bisimilarity. Parallel Composition If $P$ and $Q$ are processes then $P | Q$ is the parallel composition of their processes â€” i.e. the non-deterministic interleaving of their actions $$\\textbf{ACLOCK}=\\text{tick.beep | tock}$$ SynchronizationIn CCS, every action a has an opposing coaction $\\bar a$ (and $\\bar{\\bar a}$ = a): It is a convention to think of an action as an output event and a coaction as an input event. If a system can execute both an action and its coaction, it may execute them both simultaneously by taking an internal transition marked by the special action $Ï„$ . Expansion TheoremLet $P$ and $Q$ be processes. By expanding recursive definitions and using our existing equations for choice we can express $P$ and $Q$ as n-ary choices of action prefixes:$$P =\\sum_{iâˆˆI}\\alpha_i. P_i \\text{ and } Q =\\sum_{jâˆˆJ}Î²_j. Q_j$$Then, the parallel composition can be expressed as follows:$$P | Q =\\sum_{iâˆˆI}Î±i.(P_i| Q) +\\sum_{jâˆˆJ}Î²_j.(P | Q_j) + \\sum_{iâˆˆI, jâˆˆJ, Î±_i=\\barÎ²_j}Ï„.(P_i| Q_j)$$From this, many useful equations are derivable:$$\\begin{align}&amp;P|Q &amp;= \\space&amp;Q|P\\\\&amp;P|(Q|R) &amp;=\\space &amp;(P|Q)|R\\\\&amp;P|\\text{STOP} &amp;=\\space &amp;P\\\\\\end{align}$$ Restriction If $P$ is a process and a is an action (not $Ï„$ ), then $P \\ a$ is the same as the process $P$ except that the actions a and a may not be executed. We have $(a.P)$ \\ $ b = a.(P $\\ $ b) \\text{ if } a \\not\\in {b, \\bar b}$ Example:$$\\begin{align}&amp;\\textbf{CLOCK}_4 &amp;= \\space&amp;\\text{tick}\\textbf{.CLOCK}_4\\\\&amp;\\textbf{MAN} &amp;=\\space &amp;\\bar {\\text{tick}}\\text{.eat.}\\textbf{MAN} \\\\&amp;\\textbf{EXAMPLE} &amp;=\\space &amp;(\\textbf{MAN|CLOCK}_4)\\text{\\ tick}\\\\\\end{align}$$ SemanticsUp until now, our semantics were given informally in terms of pictures. Now we will formalise our semantic intuitions. Our set of locations in our labelled transition system will be the set of all CCS processes. Locations can now be labelled with what process they are: We will now define what transitions exist in our LTS by means of a set of inference rules. This technique is called operational semantics. Inference RulesIn logic we often write:$$\\frac{A_1, A_2, â€¦ A_n}{C}$$To indicate that C can be proved by proving all assumptions A1 through An. For example, the classical logical rule of modus ponens is written as follows:$$\\frac{A\\Rightarrow B\\qquad A}{B}\\text{Modus Ponens}$$ Operational Semantics$$\\frac{}{a.P\\xrightarrow{a} P}\\text{ACT}\\qquad\\frac{P\\xrightarrow{a} Pâ€™}{P+Q\\xrightarrow{a} Pâ€™}\\text{CHOICE}_1\\qquad\\frac{Q\\xrightarrow{a} Qâ€™}{P+Q\\xrightarrow{a} Qâ€™}\\text{CHOICE}_2$$ $$\\frac{P\\xrightarrow{a} Pâ€™}{P|Q\\xrightarrow{a} Pâ€™|Q}\\text{PAR}_1\\qquad\\frac{Q\\xrightarrow{a} Qâ€™}{P|Q\\xrightarrow{a} P|Qâ€™}\\text{PAR}_2\\qquad\\frac{P\\xrightarrow{a} Pâ€™\\quad Q\\xrightarrow{a} Qâ€™}{P|Q\\xrightarrow{Ï„} Pâ€™|Qâ€™}\\text{SYNC}$$ $$\\frac{P\\xrightarrow{a} Pâ€™\\quad a\\not\\in{b,\\bar b}}{P\\text{ \\ }b\\xrightarrow{a} Pâ€™\\text{ \\ }b}\\text{RESTRICT}$$ Bisimulation Equivalence Two processes (or locations) P and Q are bisimilar iff they can do the same actions and those actions themselves lead to bisimilar processes. All of our previous equalities can be proven by induction on the semantics here. Proof TreesThe advantages of this rule presentation is that they can be â€œstackedâ€ to give a neat tree like derivation of proofs. Value PassingWe introduce synchronous channels into CCS by allowing actions and coactions to take parameters.$$\\begin{align}&amp;\\text{Actions:}&amp;a(3)\\qquad &amp;c(15) &amp;x(True)â€¦\\&amp;\\text{Coactions:}&amp;\\bar a(x)\\qquad &amp;\\bar c(y) &amp;\\bar c(z)â€¦\\end{align}$$The parameter of an action is the value to be sent, and the parameter of a coaction is the variable in which the received value is stored. Example A one-cell sized buffer is implemented as:$$\\textbf{BUFF}=\\bar{\\text{in}}(x).\\text{out}(x).\\textbf{BUFF}$$Larger buffers can be made by stitching multiple BUFF processes together! This is how we model asynchronous communication in CCS. Merge and GuardsGuard: If $P$ is a value-passing CCS process and $Ï•$ is a formula about the variables in scope, then $[Ï•]P$ is a process that executes just like $P$ if $Ï•$ is holds for the current state and like STOP otherwise We can define an if statement like so:$$\\textbf{if } Ï• \\textbf{ then } P \\textbf{ else } Qâ‰¡ ([Ï•].P) + ([Â¬Ï•].Q)$$ Assignment If $P$ is a process and $x$ is a variable in the state, and $e$ is an expression, then $[\\![x := e]\\!]$ $P$ is is the same as $P$ except that it first updates the variable $x$ to have the value $e$ before making a transition. Some presentations of value passing CCS also include assignment to update variables in the state. With this, our value-passing CCS is now just as expressive as Ben-Ariâ€™s pseudocode. Moreover, the connection between CCS and transition diagrams is formalised, enabling us to reason symbolically about processes rather than semantically. Process AlgebraThis is an example of a process algebra. There are many such algebras and they have been very influential on the design of concurrent programming languages. Kripke Structures(KS)Linear Temporal Logic(LTL)Logic A logic is a formal language designed to express logical reasoning. Like any formal languages, logics have a syntax and semantics(meaning of the value). Example: Proposition Logic Syntax A set of atomic propositions $P = {a, b, c, â€¦}$ An inductively defined set of formulae: Each $p \\in P$ is a formula If $P$ and $Q$ are formulae, then $P \\land Q$ is a formula If $P$ is a formula, then $\\neg P$ is a formula (Other connectives(like or) are just sugar for these, so we omit them) Sematics Semantics are a mathematical representation of the meaning of a piece of syntax. There are many ways of giving a logic semantics, but we will use models. Example (Propositional Logic Semantics) A model for propositional logic is a valuation $V âŠ† P$, a set of â€œtrueâ€ atomic propositions. We can extend a valuation over an entire formula, giving us a satisfaction relation:$$\\begin{align} V âŠ¨ p &amp;â‡” p \\in V\\\\ V âŠ¨ \\varphi \\land \\psi &amp;â‡” V âŠ¨ \\varphi\\text{ and }V âŠ¨ \\psi\\\\ V âŠ¨ \\neg \\varphi &amp;â‡” V |\\ne \\varphi\\end{align}$$We read $V âŠ¨ Ï†$ as $V$ â€œsatisfiesâ€ $Ï†$. LTL Linear temporal logic (LTL) is a logic designed to describe linear time properties. Linear temporal logic syntax We have normal propositional operators: $p âˆˆ P$ is an LTL formula. If $Ï•$, $Ïˆ$ are LTL formulae, then $Ï• âˆ§ Ïˆ$ is an LTL formula. If $Ï•$ is an LTL formula, $Â¬Ï•$ is an LTL formula. We also have modal or temporal operators: If $Ï•$ is an LTL formula, then $\\circ Ï•$ is an LTL formula. The circle is read as â€˜nextâ€™ If $Ï•$, $Ïˆ$ are LTL formulae, then $Ï• U Ïˆ$ is an LTL formula. The U is read as until. LTL Semanticslet $\\sigma = \\sigma_0\\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5â€¦$ be a behavior. Then define the notation: $\\sigma|_0 = \\sigma$ $\\sigma|_1 = \\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5$ $\\sigma|_{n+1} = (\\sigma|_1)|_n$ Semantics The models of LTL are behaviours. For atomic propositions, we just look at the first state. We often identify states with the set of atomic propositions they satisfy.$$\\begin{align}&amp;\\sigma \\vDash p &amp;\\Leftrightarrow\\space &amp;p\\in \\sigma_0\\\\&amp;\\sigma \\vDash \\varphi\\land\\psi &amp;\\Leftrightarrow \\space&amp;\\sigma \\vDash \\varphi\\space \\text{and} \\space \\sigma\\vDash\\psi\\\\&amp;\\sigma \\vDash \\neg\\varphi &amp;\\Leftrightarrow \\space&amp;\\sigma \\not\\vDash \\varphi\\\\&amp;\\sigma \\vDash \\bigcirc\\varphi &amp;\\Leftrightarrow\\space &amp;\\sigma_1 \\vDash \\varphi\\\\&amp;\\sigma \\vDash \\varphi U\\psi &amp;\\Leftrightarrow\\space &amp;\\text{There exists an $i$ such that} \\text{$\\sigma_i\\vDash\\varphi$ and for all $j&lt; i$, $\\sigma|_j\\vDash\\varphi$}\\end{align}$$We say $P\\vDash\\varphi$ iff $\\forall\\sigma\\in[\\![P]\\!]$. $\\sigma\\vDash\\varphi$. Derived Operators The operator $\\Diamond\\varphi$(â€œfinallyâ€ or â€œeventuallyâ€) says that $\\varphi$ will be true at some point. The operator$\\square\\varphi$(â€œglobalâ€ or â€œalwaysâ€) says that $\\varphi$ is always true from now on. Fairness The fairness assumption means that if a process can always make a move, it will eventually be scheduled to make that move. Expressing Fairness in LTLWeak fairness for action $Ï€$ is then expressible as:$$\\square(\\square \\text{enabled}(Ï€) â‡’ \\Diamond\\text{taken}(Ï€))$$Strong fairness for action Ï€ is then expressible as:$$\\square(\\square\\Diamond\\text{enabled}(Ï€) â‡’ \\Diamond\\text{taken}(Ï€))$$ Critical SectionsDesiderata We want to ensure two main properties and two secondary ones: Mutual Exclusion No two processes are in their critical section at the same time. Eventual Entry (or starvation-freedom) Once it enters its pre-protocol, a process will eventually be able to execute its critical section. Absence of Deadlock The system will never reach a state where no actions can be taken from any process. Absence of Unneccessary Delay If only one process is attempting to enter its critical section, it is not prevented from doing so. Eventual Entry is liveness, the rest are safety. Dekkerâ€™s algorithm Dekkerâ€™s algorithm works well except if the scheduler pathologically tries to run the loop at q3 Â· Â· Â· q7 when turn = 2 over and over rather than run the process p (or vice versa). With fairness assumption, Dekkerâ€™s algorithm is correct. Tie-Breaker (Petersonâ€™s) Algorithmfor 2 Processes for n Processes Properties of the Tie-Breaker AlgorithmDo we satisfy: Eventual entry? Yes Linear waiting? No Linear Waiting Linear waiting is the property that says the number of times a process is â€œovertakenâ€ (bypassed) in the preprotocol is bounded by $n$ (the number of processes). Bakery Algorithm Mutual ExclusionThe following are invariants$$\\begin{align}np = 0&amp;\\equiv p1..2\\\\nq = 0&amp;\\equiv q1..2\\\\p4 &amp;\\Rightarrow nq= 0\\lor np\\le nq\\\\q4 &amp;\\Rightarrow np= 0\\lor nq\\lt nq\\end{align}$$and hence also $Â¬(p4 âˆ§ q4)$. Other Safety PropertiesDeadlock freedom: the disjunction $nq = 0 \\lor np\\le nq \\lor np=0\\lor np\\lt np$ of the conditions on the await statements at $p3/q3$ is equivalent to $T$. Hence it is not possible for both processes to be blocked there. Absence of unnecessary delay: Even if one process prefers to stay in its non-critical section, no deadlock will occur by the first two invariants(1) and (2) Eventual EntryFor $p$ to fail to reach its CS despite wanting to, it needs to be stuck at p3, where it will evaluate the condition infinitely often by weak fairness. To remain stuck, each of these evaluations must yield false. in LTL:$$\\square\\Diamond\\neg(nq=0\\lor np\\le nq)$$Which implies$$(5)\\space \\square\\Diamond nq\\ne0, \\text{and}\\(6)\\qquad \\square \\Diamond nq\\lt np$$Because there is no deadlock, (5) implies that process $q$ goes through infinitely many iterations of the main loop without getting lost in the non-critical section. But thrn it must set $nq$ to the constant $np+1$. From then onwards it is no longer possible to fail the test ($nq=0\\lor np\\le nq$), contradiction. N processes once again relying on atomicity of non-LCR lines of Ben-Ari pseudo-code; \u001c $&lt;\\!&lt;$breaks ties using PIDs. An Implementable Algorithm Properties of Lamportâ€™s bakery algorithmCons: $O(n)$ pre-protocol; unbounded ticket numbers Assertion 1: If $p_k 1..2 âˆ§ p_i5..9$ and $k$ then reaches $p_5..9$ while $i$ is still there, then number[i] &lt; number[k]. Assertion 2: p_i8..9 âˆ§ p_k 5..9 âˆ§ i $\\ne$ k â‡’ (number[i], i) \u001c $&lt;\\!&lt;$ (number[k], k) Fast Algorithm sacrifice eventual entry Szymanskiâ€™s Algorithm enforces linear wait requires at most $4p âˆ’ \\lceil{\\frac{p}{n}}\\rceil$ writes for p CS entries by n competing processes can be made immune to process failures and restarts as well as read errors occurring during writes Phases of the pre-protocol announce intention to enter CS enter waiting room through door 1; wait there for other processes last to enter the waiting room closes door 1 n the order of PIDs, leave waiting room through door 2 to enter CS Shared variablesEach process $i$ exclusively writes a variable called flag, which is read by all the other processes. It assumes one of five values: $0$ denoting that i is in its non-CS, $1$ declares iâ€™s intention to enter the CS $2$ shows that i waits for other processes to enter the waiting room $3$ denotes that i has just entered the waiting room $4$ indicates that i left the waiting room Hardware-Assisted Critical SectionMachine InstructionsExchange test and set LocksThe variable common is called a lock_ (or **_mutex**). A lock is the most common means of concurrency control in a programming language implementation. Typically it is abstracted into an abstract data type, with two operations: Taking the lock â€“ the first exchange (step $p_2$/ $q_2$) Releasing the lock â€“ the second exchange (step $p_5$ / $q_5$) Architectural ProblemsIn a mulitprocessor execution environment, reads and writes to variables initially only read from/write to cache. Writes to shared variables must eventually trigger a write-back to main memory over the bus. These writes cause the shared variable to be cache invalidated. Each processor must now consult main memory when reading in order to get an up-to-date value. The problem: Bus traffic is limited by hardware. With these instructionsâ€¦ The processes spin while waiting, writing to shared variables on each spin. This quickly causes the bus to become jammed, and can delay processes from releasing the lock and violating eventual entry.","link":"/2020/08/09/FoundationofConcurreny/"},{"title":"Macè£…spinå’Œispin","text":"ç¿»äº†å¾ˆä¹…ä¹Ÿæ²¡æ‰¾åˆ°Macçš„å®‰è£…æ•™ç¨‹(å¯èƒ½æ˜¯å¤ªç®€å•äº†)ï¼Œ å®‰è£…spinè¿™ä¸ªå°±å¾ˆç®€å•äº†ï¼Œæ„Ÿè°¢brew, 1brew install spin å®‰è£…ispinå»https://github.com/nimble-code/Spinä¸‹è½½ï¼Œå¯ä»¥ç›´æ¥æ‰“åŒ…æˆ–è€…git cloneã€‚å…¶å®ispinåªæ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼Œåœ¨ â€˜optional_guiâ€™è¿™ä¸ªfolderé‡Œï¼ˆä¸ç¡®å®šåªæœ‰è¿™ä¸ªæ–‡ä»¶æ˜¯ä¸æ˜¯å°±å¯ä»¥ç›´æ¥è¿è¡Œï¼‰ã€‚ç„¶åæˆ‘ä»¬è¿›å…¥ispinæ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ã€‚ 1cd Spin/optional_gui ç„¶ååœ¨å‘½ä»¤è¡Œè¾“å…¥ 1wish -f ispin.tcl å°±å¯ä»¥è¿è¡Œäº†ã€‚","link":"/2020/06/01/Mac%E8%A3%85spin%E5%92%8Cispin/"},{"title":"ã€åŒå‰‘åˆç’§ã€‘Gitå’ŒGithubä½¿ç”¨æ•™ç¨‹","text":"æ˜¯å¦æœ‰é‡åˆ°è¿‡å†™ç€å†™ç€æƒ³å›åˆ°ä¹‹å‰ç‰ˆæœ¬ï¼Œå´åˆä¸è®°å¾—å…·ä½“å®ç°ï¼›åˆæˆ–æ˜¯æƒ³å’Œé˜Ÿå‹å…±äº«ä»£ç ï¼Œæ¯æ¬¡ä¿®æ”¹å‘é€æ–‡ä»¶ï¼›æŠ‘æˆ–æ˜¯æƒ³æ¢ä¸ªç”µè„‘å†™å´ä¸æƒ³ç”¨emailç­‰æ¬è¿å…¨éƒ¨æ–‡ä»¶è¿™ç±»çƒ¦ä¸èƒœçƒ¦ç±»ä¼¼çš„é—®é¢˜ï¼Œä¼Ÿå¤§çš„ç¨‹åºå‘˜ä»¬è‡ªç„¶æ—©å°±ä¸ºæˆ‘ä»¬é€ å¥½äº†è½®å­ï¼Œé‚£å°±æ˜¯ç‰ˆæœ¬æ§åˆ¶çš„åˆ©å™¨â€”â€”Gitä»¥åŠcloud based Githubã€‚æœ¬æ•™ç¨‹ä¹Ÿä¸»è¦è®²Gitä¸Githubçš„ä½¿ç”¨ã€‚ 1. èƒŒæ™¯ä»‹ç»æœ¬æ¥æƒ³ç²—æš´å†™ä¸€ä¸‹å®‰è£…ä½¿ç”¨æ•™ç¨‹ï¼Œæƒ³äº†æƒ³è¿˜æ˜¯å…ˆå†™ä¸€ç‚¹èƒŒæ™¯ä»‹ç»ï¼Œä¸æ„Ÿå…´è¶£å¯ä»¥ç›´æ¥è·³è¿‡ã€‚ ç‰ˆæœ¬æ§åˆ¶é¦–å…ˆæˆ‘ä»¬éœ€è¦äº†è§£ä¸€ä¸ªæ¦‚å¿µâ€”â€”Version Controlï¼Œä¹Ÿå°±æ˜¯ç‰ˆæœ¬æ§åˆ¶ã€‚å½“æˆ‘ä»¬å†™ä»£ç çš„æ—¶å€™æ€»ä¼šæœ‰æ„æ— æ„åˆ¶é€ å‡ºä¸€äº›bugï¼Œæœ‰æ—¶å€™æˆ‘ä»¬ä¼šæƒ³è¿”å›å‰ä¸€æ¬¡æ²¡æœ‰é—®é¢˜çš„æ—¶å€™ï¼Œå› æ­¤è¿™å°±æ˜¯æˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦ç‰ˆæœ¬æ§åˆ¶ã€‚ ç®€å•æ¥è¯´ï¼Œç‰ˆæœ¬æ§åˆ¶å°±æ˜¯åœ¨ä¸åŒæ—¶é—´èŠ‚ç‚¹ä¿å­˜ä½ çš„ç¨‹åºï¼Œç„¶åä½ å¯ä»¥é€šè¿‡å®ƒå›çœ‹ç”šè‡³å›åˆ°ä¹‹å‰ä¿å­˜çš„ç‰ˆæœ¬ã€‚ ä»€ä¹ˆæ˜¯GitGitæ˜¯åœ¨2005å¹´çš„æ—¶å€™åˆæ¬¡å¼€å‘å‡ºæ¥çš„ç‰ˆæœ¬æ§åˆ¶åˆ©å™¨ï¼Œå¹¶é£é¡å…¨çƒã€‚Gitæ˜¯ä¸€ä¸ªå®‰è£…å¹¶ç®¡ç†æœ¬åœ°ç³»ç»Ÿçš„å·¥å…·è€Œä¸”å¯ä»¥ç»™ä½ æä¾›ç°æœ‰æ–‡ä»¶çš„ä½ ä¿å­˜çš„ä¸åŒç‰ˆæœ¬ã€‚å› ä¸ºæ˜¯æœ¬åœ°çš„ï¼Œä¸‹è½½ä¹‹åå°±ä¸å†éœ€è¦ç½‘ç»œä¹Ÿå¯ä»¥ä½¿ç”¨ã€‚ ä»€ä¹ˆæ˜¯GithubGithubæœ‰ä¸€ç‚¹åƒå¯è§†åŒ–çš„Gitï¼Œå¹¶ä¸”æ˜¯åœ¨çº¿çš„æœåŠ¡ã€‚è®©ä½ å¯ä»¥åœ¨çº¿ç®¡ç†ä½ çš„Gitä»“åº“ï¼ˆè¿™ä¸ªå…·ä½“ä¼šåœ¨åé¢è®²ï¼‰ã€‚é€šè¿‡Githubï¼Œä½ å¯ä»¥åˆ†äº«ä½ çš„ç¨‹åºï¼Œè®©å…¶ä»–åˆä½œè€…ä¸€èµ·è¿›è¡Œç¼–è¾‘ã€‚å®ƒä¸ä»…ä¿å­˜äº†Gitçš„å…¨éƒ¨åŠŸèƒ½ï¼Œè¿˜è¿›è¡Œäº†æ‰©å……ï¼Œå®ƒå¯ä»¥è®©ä½ åœ¨ä»»æ„ç”µè„‘ä»»æ„åœ°åŒºè®¿é—®ï¼Œåªè¦ä½ æœ‰æƒé™ã€‚å®ƒæœ€å¤§çš„ä¼˜ç‚¹å°±æ˜¯å®ƒæ˜¯ä¸€ä¸ªå¾ˆåºå¤§çš„æ•°æ®åº“ï¼Œä½ å¯ä»¥æœç´¢ï¼Œé˜…è¯»ç”šè‡³ä½¿ç”¨åˆ«äººå†™å¥½çš„ç¨‹åºã€‚å½“ç„¶Githubè¿˜æœ‰å¾ˆå¤šæ›¿ä»£ç‰©å¦‚Gitlabä¹‹ç±»ï¼Œæœ¬æ–‡å°±ä¸èµ˜è¿°äº†ã€‚ Git vs. Githubç®€å•çš„è¯´ï¼ŒGitæ˜¯ä¸€ä¸ªå¸®åŠ©ä½ ç®¡ç†å’Œè¿½è¸ªæœ¬åœ°æºç å†å²çš„ç‰ˆæœ¬æ§åˆ¶å·¥å…·ï¼Œè€ŒGithubåˆ™æ˜¯ä¸€ä¸ªåŸºäºäº‘ç«¯è¿ç”¨GitæŠ€æœ¯çš„è®©ä½ ç®¡ç†Gitä»“åº“çš„æœåŠ¡ã€‚ 2. Gité¦–å…ˆï¼Œæˆ‘ä»¬æ¥åº·åº·Gitçš„ä½¿ç”¨ã€‚ æ³¨æ„ï¼šæ‰€æœ‰å’ŒGitç›¸å…³çš„å‘½ä»¤éƒ½æ˜¯gitå¼€å¤´å™¢ã€‚ å®‰è£…å¦‚æœå·²ç»å®‰è£…è¿‡ï¼Œå¯ä»¥è·³è¿‡ã€‚ Linux å¦‚æœæ˜¯Fedoraæˆ–å…¶ä»–ç±»ä¼¼çš„ RPM-based distribution, è­¬å¦‚RHEL å’Œ CentOSï¼š 1$ sudo dnf install git-all æˆ–Debian-based distributionåƒUbuntu 1$ sudo apt install git-all macOS å¯ä»¥å…ˆè¯• 1$ git --version å¦‚æœæ²¡æœ‰å®‰è£…ï¼Œåº”è¯¥ä¼šå¼¹å‡ºå®‰è£…è¯·æ±‚ï¼Œä¹Ÿå¯ä»¥å»https://git-scm.com/download/mac ä¸‹è½½ã€‚ Windows è¿™ä¸ªç¨å¾®æœ‰ç‚¹çƒ¦ï¼Œè§https://git-scm.com/book/en/v2/Getting-Started-Installing-Git ã€‚ æ–°å»ºä¸€ä¸ªGitä»“åº“å®‰è£…å¥½ä¹‹åæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹ç”¨äº†ï¼ŒGitçš„ä¸€ä¸ªä»“åº“ï¼ˆRepoï¼‰å°±æ˜¯ä½ çš„ä¸€ä¸ªé¡¹ç›®ã€‚æˆ‘ä»¬é¦–å…ˆæ–°å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ 12$ mkdir firstRepo$ cd firstRepo åˆå§‹åŒ–ä¸€ä¸ªä»“åº“ 1$ git init è¿™æ—¶å€™ä¼šæ˜¾ç¤ºè¿™ä¹ˆä¸€è¡Œ Initialized empty Git repository in /Users/tina/Desktop/firstRepo/.git/, åé¢ä¸€éƒ¨åˆ†æ˜¯ä½ å½“å‰ä»“åº“çš„è·¯å¾„ï¼Œä¼šæ ¹æ®è·¯å¾„ä¸åŒè¿›è¡Œå˜åŒ–ã€‚é€šè¿‡.gitæˆ‘ä»¬å¯ä»¥çŸ¥é“Gitå…¶å®æ˜¯åˆ›å»ºäº†ä¸€ä¸ªéšè—æ–‡ä»¶å¤¹ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿è¡Œlsè¿™ä¸ªå‘½ä»¤ï¼Œå¹¶ä¸ä¼šçœ‹åˆ°æœ‰å…³Gitçš„ä¿¡æ¯ã€‚ æ·»åŠ æ–‡ä»¶æˆ‘ä»¬ä¾ç„¶å…ˆåœ¨å½“å‰ç›®å½•å»ºæ–‡ä»¶ï¼Œæ–‡ä»¶ç±»å‹æ— æ‰€è°“ã€‚è¿™é‡Œå°±ç”¨txtæ–‡ä»¶äº†ã€‚ 123$ touch first.txt$ lsfirst.txt æ­¤æ—¶æˆ‘ä»¬å¯ä»¥çœ‹è§æ–‡ä»¶é‡Œæœ‰ä¸€ä¸ªæ–‡ä»¶å«first.txtï¼Œä½†æ˜¯è¿™ä¸ªæ–‡ä»¶å¹¶ä¸åœ¨æˆ‘ä»¬Gitä»“åº“é‡Œï¼ˆåˆ’é‡ç‚¹ï¼‰ï¼Œä¸ºäº†çœ‹ä»“åº“é‡Œæœ‰ä»€ä¹ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ 1234567891011$ git statusOn branch masterInitial commitUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) first.txtnothing added to commit but untracked files present (use \"git add\" to track) è¿™ä¸ªå‘½ä»¤æˆ‘ä»¬ä¹‹åå…·ä½“è®²ï¼Œç°åœ¨æˆ‘ä»¬çœ‹åˆ°untracked filesè¿™é‡Œï¼Œè¿™ä¸ªçš„æ„æ€å°±æ˜¯è¯´è¿™äº›ä¸ªæ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹ä½†æ˜¯æ²¡æœ‰ä¿å­˜åˆ°æˆ‘ä»¬ä»“åº“é‡Œï¼Œæ‰€ä»¥Gitä¸ä¼šå¯¹å®ƒçš„æ”¹å˜è¿›è¡Œè¿½è¸ªã€‚æŠŠæ–‡ä»¶æ·»åŠ åˆ°ä»“åº“é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ 1$ git add first.txt æ­¤æ—¶first.txtå·²ç»è¢«åŠ è¿›å»äº†ã€‚å¦‚æœæˆ‘ä»¬æƒ³æ·»åŠ å¤šä¸ªæ–‡ä»¶å‘¢ï¼Œå¯ä»¥æŠŠæƒ³åŠ å…¥çš„æ–‡ä»¶ç”¨ç©ºæ ¼éš”å¼€ï¼Œå†™åœ¨åé¢ï¼Œåƒè¿™æ · 1$ git add file1 file2 ä¸è¿‡æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªæ›´ç®€å•çš„æ–¹æ³•ï¼Œé‚£å°±æ˜¯. 1$ git add . è¿™æ ·å¯ä»¥æŠŠå½“å‰ç›®å½•ä¸‹å…¨éƒ¨çš„æ²¡æœ‰trackçš„æ–‡ä»¶éƒ½åŠ è¿›æ¥ï¼ˆæ˜¯ä¸æ˜¯å¾ˆæ–¹ä¾¿ï¼ï¼‰ åˆ é™¤æ–‡ä»¶æåˆ°æ·»åŠ å°±ä¸å¾—ä¸è¯´åˆ é™¤ï¼Œå¦‚æœä¸€ä¸ªæ–‡ä»¶ä¸æƒ³è¢«è·Ÿè¸ªäº†æ€ä¹ˆåŠå‘¢ï¼Œé‚£å°±åˆ æ‰å•¦ã€‚ 1$ git rm file æ³¨æ„ï¼šè¿™ä¸ªæ“ä½œä¸ä¼šåœ¨å½“å‰ç›®å½•åˆ é™¤æ–‡ä»¶ ä¸€ä¸ªæ¯”è¾ƒé‡è¦çš„flag --force é¡¾åæ€ä¹‰, è¿™ä¸ªflagæ˜¯ç”¨æ¥å¼ºåˆ¶åˆ é™¤æ–‡ä»¶çš„ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœä½ çš„æ–‡ä»¶æ²¡æœ‰è¢«commitï¼ˆä¸‹é¢å°±è®²è¿™ä¸ªï¼‰ï¼Œæ˜¯ä¸å¯ä»¥è¢«åˆ é™¤çš„ï¼Œä½†æ˜¯åŠ ä¸Šè¿™ä¸ªflagå°±å¯ä»¥åˆ é™¤äº†ã€‚ â€œæˆªå›¾â€å’³ï¼Œå®è¯å®è¯ï¼Œæˆ‘ä¹Ÿä¸çŸ¥é“commitæ€ä¹ˆå‡†ç¡®ç¿»è¯‘æˆä¸­æ–‡ã€‚ç°åœ¨æˆ‘ä»¬æ¥çœ‹Gitéå¸¸éå¸¸é‡è¦çš„ä¸€ä¸ªå‘½ä»¤ï¼Œcommitã€‚å®ƒå…¶å®å°±æ˜¯ç±»ä¼¼ä¸€ä¸ªæˆªå›¾ï¼Œå­˜ä¸‹æ¥ä½ å½“å‰çš„é¡¹ç›®å®Œæˆæƒ…å†µå¹¶ä¿å­˜ä¸‹æ¥ï¼Œä»¥åå¯ä»¥å›é¡¾ç”šè‡³å›åˆ°å½“å‰èŠ‚ç‚¹ã€‚è‡³äºæ€ä¹ˆå›åˆ°æˆ‘ä»¬åé¢å†è®²ï¼Œç°åœ¨å°±æ¥è¿›è¡Œâ€œæˆªå›¾â€ã€‚è¿™ä¸ªæˆªå›¾åªæ˜¯å¯¹äºGitè¿½è¸ªçš„æ–‡ä»¶ï¼Œæ‰€ä»¥å®ƒä¸addæ˜¯ä¸å¯åˆ†å‰²çš„ã€‚ 1234$ git commit -m \"Your message about the commit\"[master (root-commit) b345d9a] This is my first commit! 1 file changed, 1 insertion(+) create mode 100644 first.txt è¿™æ ·å°±å»ºäº†ä¸€ä¸ªæ–°çš„commitå•¦ï¼Œçœ‹åˆ°è¿™ä¸ªb345d9aä¸œè¥¿äº†ä¹ˆï¼Œè¿™ä¸ªæ˜¯ä¸€ä¸ªcommit idã€‚ä½†æ˜¯å¦‚æœä½ å¯¹æ–‡ä»¶è¿›è¡Œäº†ä¿®æ”¹ï¼Œåˆè¿è¡Œäº†è¿™ä¸ªè¯­å¥ï¼Œä½ ä¼šçœ‹åˆ°è¿™ä¹ˆä¸€è¡ŒAlready Up to date. Bugï¼ŸNononoï¼Œ è¿™å°±æ˜¯éœ€è¦æˆ‘ä»¬addæ¥å‚ä¸äº†ã€‚å› ä¸ºåœ¨Gitå¿ƒé‡Œï¼Œä½ çš„æ–‡ä»¶è¿˜æ˜¯ä¸Šä¸€æ¬¡addæ¥çš„ï¼Œä»–ä¸€çœ‹ï¼Œæ–‡ä»¶å’Œä¸Šä¸€æ¬¡commitçš„æ²¡æœ‰å˜åŒ–å‘€ï¼Œä¸æˆªå›¾ä¸æˆªå›¾ã€‚æ‰€ä»¥æˆ‘ä»¬è¦é‡æ–°addä¸€éï¼Œè¿™ä¸ªæ—¶å€™çš„addå°±æ˜¯ä¸€ä¸ªæ›´æ–°çš„æ“ä½œäº†ï¼Œæˆ‘ä»¬æ¯æ¬¡commitä¹‹å‰éƒ½è¦å…ˆaddå†commitã€‚ ä¸è¿‡ï¼Œè¿™ä¸¤æ­¥æŸäº›æƒ…å†µä¸‹æ˜¯å¯ä»¥åˆå¹¶çš„ï¼Œå˜æˆ 1$ git commit -am \"Your message about the commit\" è¿™æ ·åšå°±æ˜¯æ›´æ–°æˆ‘è·Ÿè¸ªçš„æ‰€æœ‰æ–‡ä»¶å¹¶è¿›è¡Œç°æœ‰æˆæœæˆªå›¾ï¼Œä½†æ˜¯å¦‚æœä½ æ–°å»ºçš„æ–‡ä»¶è¿˜æ˜¯è¦ç”¨å•ç‹¬çš„addæ¥è¿›è¡Œæ·»åŠ å“¦ã€‚ æ³¨æ„ï¼šè¿™ä¸ªè¯­å¥åƒä¸‡ä¸è¦çå†™å“¦ï¼Œä»¥åä½ å¯èƒ½ä¼šç”¨åˆ° åˆ†æ”¯è™½ç„¶æˆ‘ä¸€ç›´åˆ»æ„æ²¡æœ‰æåˆ°ä¸Šé¢å‡ºç°è¿‡å‡ æ¬¡çš„ä¸€ä¸ªè¯masterï¼Œå®ƒæ˜¯ä»€ä¹ˆå‘¢ï¼Œå®ƒæ˜¯æˆ‘ä»¬çš„ä¸»åˆ†æ”¯ã€‚æƒ³è±¡ä¸€æ£µæ ‘ï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬çš„æ ‘å¹²ã€‚åˆ†æ”¯çš„ä½œç”¨æ˜¯å½“ä½ æƒ³ä¿®æ”¹æŸä¸ªéƒ¨åˆ†çš„ä»£ç ï¼Œæ·»åŠ æ–°åŠŸèƒ½ï¼Œä½†å´ä¸æƒ³å½±å“ä¹‹å‰å†™å¥½çš„ä»£ç ï¼Œå°±å¯ä»¥åˆ†å‡ºä¸€ä¸ªbranchï¼Œåœ¨ä¸Šé¢è¿›è¡Œã€‚ä¸åŒåˆ†æ”¯ä¹‹é—´ä¸ä¼šç›¸äº’å¹²æ‰°ï¼Œé™¤éä½ è¿›è¡Œåˆå¹¶ç­‰æ“ä½œã€‚ æˆ‘ä»¬åˆå§‹åŒ–ä¸€ä¸ªGitä»“åº“æ—¶ï¼Œä¸»åˆ†æ”¯å°±å·²ç»å­˜åœ¨ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥æ–°å»ºè‡ªå·±çš„branchã€‚ä½¿ç”¨ 1234$ git branch branchName$ git branch* master branchName ç¬¬ä¸€ä¸ªå‘½ä»¤æ˜¯æ–°å»ºï¼Œç¬¬äºŒä¸ªæ˜¯åˆ—å‡ºå…¨éƒ¨åˆ†æ”¯ã€‚*è¡¨ç¤ºå½“å‰æ‰€åœ¨åˆ†æ”¯ã€‚åˆ‡æ¢åˆ†æ”¯æˆ‘ä»¬ä½¿ç”¨ 1234$ git checkout branchName$ git branch master* branchName é€šå¸¸æ¥è¯´ï¼Œæˆ‘ä»¬æ–°å»ºä¸€ä¸ªåˆ†æ”¯å°±æ˜¯ä¸ºäº†åˆ‡æ¢åˆ°æ–°åˆ†æ”¯ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æŠŠä¸Šä¸¤æ­¥åˆäºŒä¸ºä¸€ 1$ git checkout -b &lt;my branch name&gt; æ³¨æ„ï¼šæˆ‘ä»¬æ–°å»ºåˆ†æ”¯çš„å†…å®¹æ˜¯å’Œä½ å»ºåˆ†æ”¯æ—¶æ‰€åœ¨çš„ä¸€è‡´ï¼Œè¦æ³¨æ„æ–°å»ºåˆ†æ”¯æ—¶æ‰€åœ¨çš„branchå“¦ï¼Œé€šå¸¸æƒ…å†µæˆ‘ä»¬éƒ½æ˜¯åœ¨masterä¸ŠåŠ åˆ†æ”¯ã€‚ åœ¨è¿›è¡Œå†™ä»£ç ï¼Œä¿®æ”¹ä¹‹åï¼Œæˆ‘ä»¬æƒ³æŠŠåˆ†æ”¯ä¸Šå†…å®¹åˆå¹¶è‡³masterï¼Œä½¿ç”¨ 12$ git checkout master$ git merge branchName è¿™é‡Œç¨å¾®æœ‰ä¸€äº›é¥¶äººï¼Œæˆ‘ä»¬éœ€è¦åœ¨masterä¸Šè¿›è¡Œmergeæ“ä½œæ¥ä½¿å¾—masterä¸åˆ†æ”¯ä¸€è‡´ã€‚ç°åœ¨æˆ‘ä»¬æ¥ç®€å•è®²è§£ä¸‹mergeã€‚ å‡è®¾åœ¨masterä¸Šæˆ‘ä»¬æœ‰å‡ æ¬¡commitåï¼Œæ–°å»ºä¸€ä¸ªåˆ†æ”¯Aï¼Œå‡ æ¬¡commitä¹‹åï¼Œæˆ‘ä»¬æƒ³æŠŠAåˆå¹¶åˆ°masterä¸Šã€‚åˆå¹¶ä¹‹å‰ï¼Œ common baseâ€”â€” â€” commit â€”â€” commit (master) â€‹ ï½œâ€” commit â€”â€” commit (A) åˆå¹¶å common baseâ€”â€” â€” commit â€”â€” commit (master) â€”â€” new merge commit â€‹ ï½œâ€” commit â€”â€” commit (A) â€”â€”ï½œ è¿™ä¸ªæ—¶å€™masterä¸Šå°±æœ‰äº†Aä¸Šé¢çš„å†…å®¹å•¦ã€‚ mergeåˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯fast forward, å¦å¤–ä¸€ç§æ˜¯3-way mergeã€‚ ç¬¬ä¸€ç§å¾ˆç›´æ¥ï¼Œåˆå¹¶å‰ common baseâ€”â€” (master) â€‹ ï½œâ€” commit â€”â€” commit (A) åˆå¹¶å common baseâ€”â€” â€”â€” new merge commit â€‹ ï½œâ€” commit â€”â€” commit (A) â€”â€”ï½œ å°±ä¸ç»†è®²GitåŸç†ï¼Œåº·å›¾ã€‚ ç¬¬äºŒç§å…¶å®å°±æ˜¯ç¬¬ä¸€ä¸ªé‚£ä¸ªå›¾ã€‚å°æœ‹å‹ï¼Œä½ æ˜¯å¦æœ‰å¾ˆå¤šé—®å·ï¼Œä¸ºä»€ä¹ˆå°±è¿™æ ·åˆå¹¶èµ·æ¥ï¼Œè€Œæ²¡æœ‰å†²çªã€‚mergeçš„è¿‡ç¨‹å¾ˆå®¹æ˜“äº§ç”Ÿçš„é—®é¢˜å°±æ˜¯å†²çªï¼å°½ç®¡Gitçš„mergeå·²ç»å¾ˆnbäº†ï¼Œä½†ä¾ç„¶ä¸å¯é¿å…äº§ç”Ÿå†²çªï¼Œä½†è¿™äº›æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°è§£å†³ã€‚ å†²çªè¿™æ˜¯Gité‡Œç»å¸¸é‡åˆ°å¹¶ä¸”è¦è§£å†³çš„é—®é¢˜ï¼å½“ä½ mergeä¸åŒåˆ†æ”¯æ—¶ï¼Œå¾ˆå®¹æ˜“é‡åˆ°ã€‚ä¸ºäº†è®©å¤§å®¶ç›´è§‚æ„Ÿå—ï¼Œæˆ‘ä»¬åˆ›é€ ä¸€ä¸ªå†²çªã€‚ 1234567891011121314151617$ git checkout master$ echo 'this is conflicted text from master' &gt; first.txt$ git commit -am 'added one line'[master 8cc7111] added one line 1 file changed, 1 insertion(+)$ git checkout branchNameSwitched to branch 'branchName'$ echo 'this is conflicted text from feature branch' &gt; first.txt$ git commit -am 'added one line'[a 23f5790] added one line 1 file changed, 1 insertion(+)$ git checkout masterSwitched to branch 'master'$ git merge branchNameAuto-merging first.txtCONFLICT (content): Merge conflict in first.txtAutomatic merge failed; fix conflicts and then commit the result. YES!!å†²çªå‡ºç°äº†ï¼Œç°åœ¨æˆ‘ä»¬è¦è§£å†³å®ƒã€‚è¿™ä¸ªæ—¶å€™æ‰“å¼€æ­¤æ–‡ä»¶ 1$ vim first.txt vimæ˜¯æœ€å¥½çš„text editorï¼ æˆ‘ä»¬ä¼šçœ‹åˆ°è¿™ä¸ª 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADthis is conflicted text from master=======this is conflicted text from feature branch&gt;&gt;&gt;&gt;&gt;&gt;&gt; branchName &lt;&lt;&lt;&lt;&lt;&lt;&lt; å’Œ=======æ˜¯å‘Šè¯‰æˆ‘ä»¬å“ªé‡Œæœ‰å†²çªï¼Œå¹¶ä¸”æ˜¯å“ªä¸ªbranchã€‚å°±è¿™ä¸ªè€Œè¨€å‰ç¬¬äºŒè¡Œæ˜¯masterä¸Šå†…å®¹è€Œç¬¬å››è¡Œåˆ™æ˜¯branchNameä¸Šçš„å†…å®¹ï¼Œæˆ‘ä»¬é€‰å–éœ€è¦å†…å®¹åè®°å¾—åˆ é™¤135è¡Œå“¦ï¼ Vimä½¿ç”¨iè¿›è¡Œç¼–è¾‘ï¼Œå®Œæˆåescç„¶åæ‰“:qæ¨å‡ºã€‚è¿™ä¸ªæ—¶å€™å†²çªå·²ç»è§£å†³å®Œå•¦ï¼Œæˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€æ¬¡commitæ¥ç»“æŸè¿™æ¬¡çš„åˆå¹¶æ“ä½œã€‚ 1$ git commit -am 'solve conflict' å…¶ä»–git statuså…¶å®ç®—æ˜¯çœ‹å½“å‰åˆ†æ”¯çš„ä¸€ä¸ªçŠ¶æ€å§ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯å¦æœ‰æœªè¢«è¿½è¸ªçš„æ–‡ä»¶ï¼Œæœ‰æ²¡æœ‰å˜åŒ–æ²¡æœ‰è¢«commitã€‚ä½¿ç”¨ 1$ git status git logç®€å•ç²—æš´ï¼Œæ˜¾ç¤ºcommitè®°å½•ï¼Œä½¿ç”¨ 1$ git log ä¼šæ˜¾ç¤ºå‡ºä¸€ä¸²commitè®°å½•ï¼Œæ¯ä¸ªåŒ…æ‹¬idï¼Œä½œè€…ï¼Œæ—¶é—´ï¼Œåˆ†æ”¯ï¼Œä»¥åŠcommitçš„æ—¶å€™çš„é‚£ä¸ªmessageã€‚ åˆ‡æ¢åˆ°æŸæ¬¡commit1$ git checkout specific-commit-id è¿™ä¸ªcommit idå°±éœ€è¦æˆ‘ä»¬å»git logé‡Œé¢æ‰¾æ¥ï¼Œå—¯å°±æ˜¯é‚£ä¸€é•¿ä¸²çœ‹ä¸æ‡‚çš„hashã€‚è‡³äºä½ å…·ä½“æƒ³åˆ‡æ¢åˆ°å“ªä¸ªï¼Œå°±çœ‹ä½ commitçš„æ—¶é—´ä»¥åŠä½ è‡ªå·±å†™çš„commitä¿¡æ¯å•¦ï¼ˆæ»‘ç¨½ï¼‰çå†™çš„è¯ç°åœ¨å°±ä½œèŒ§è‡ªç¼šäº†å˜»å˜»ã€‚ ç»“è¯­Gitå¤§è‡´å°±è®²è¿™ä¹ˆå¤šå•¦ï¼Œæƒ³æ›´äº†è§£å…·ä½“çš„Gitï¼Œé˜”ä»¥å‚è€ƒdocumentã€‚https://git-scm.com/doc 3. Githubä¸å¾—ä¸è¯´Githubæ˜¯çœŸçš„å¥½ç”¨ï¼Œé€ å¥½çš„è½®å­éšä¾¿ç”¨ï¼Œç”šè‡³è¿˜å¯ä»¥æ‰¾åˆ°ä½œä¸šç­”æ¡ˆï¼Œå½“ç„¶è¦è‡ªå·±å†™ä½œä¸šäº†ï¼ï¼Œè€Œä¸”Githubæ¨å‡ºçš„Github desktopæ˜¯çœŸé¦™ã€‚ä½¿ç”¨å¾ˆç®€å•ï¼Œæ— å¸ˆè‡ªé€šã€‚è¨€å½’æ­£ä¼ ï¼Œæˆ‘ä»¬æ¥åº·åº·Githubæ€ä¹ˆç”¨ã€‚ æ³¨å†ŒåŠå®‰è£…æƒ³æ³¨å†Œçš„è¯ä½ éœ€è¦å‡†å¤‡çš„ä¸œè¥¿æœ‰ï¼šä¸€ä¸ªæˆ–å¤šä¸ªé‚®ç®±ã€‚ä¸€ä¸ªè´¦å·å¯ä»¥ç»‘å®šNä¸ªé‚®ç®±ï¼Œç„¶åå»https://github.com/ è¿›è¡Œæ³¨å†Œã€‚å¦‚æœæ˜¯å­¦ç”Ÿä¸”æƒ³å…è´¹ç”³è¯·Github proå¯ä»¥è®¿é—®https://education.github.com/pack ã€‚ ç”³è¯·å®Œä¹‹åæˆ‘ä»¬æ‰“å¼€å‘½ä»¤è¡Œ 12$ git config --global user.name \"&lt;your_name_here&gt;\"$ git config --global user.email \"&lt;your_email@email.com&gt;\" æ³¨æ„ï¼šå†™åå­—æ—¶å»é™¤æ‹¬å·ä½†ä¿ç•™å¼•å·ï¼Œé‚®ç®±ä½¿ç”¨ç”³è¯·æ—¶çš„é‚®ç®±ã€‚ä»¥åŠï¼Œå¦‚æœåªæ˜¯æƒ³åœ¨å½“å‰ä»“åº“ç”¨çš„è¯ï¼Œå»æ‰globalå°±okäº† ç°åœ¨æˆ‘ä»¬è¿æ¥sshåˆ°Githubï¼Œè¿™æ ·å°±å¯ä»¥é€šè¿‡sshè¿›è¡Œcloneæ“ä½œè€Œä¸æ˜¯httpã€‚å…ˆå¤åˆ¶ssh key 1$ pbcopy &lt; ~/.ssh/id_rsa.pub ç„¶åå»Githubç½‘é¡µï¼Œç‚¹å‡»å¤´åƒï¼Œå‡ºç°ä¸‹æ‹‰èœå•ï¼Œç‚¹å‡»Settingsï¼Œåœ¨ç”¨æˆ·è®¾ç½®çš„èœå•æ é€‰æ‹©SSH and GPG keysï¼Œç‚¹å‡»New SSH keyï¼Œåœ¨titleæ å†™ä¸Šä½ è®¾å¤‡çš„åå­—ï¼Œæ–¹ä¾¿ä½ è¾¨è®¤ï¼ŒæŠŠåˆšåˆšå¤åˆ¶å¥½çš„keyå¤åˆ¶åˆ°keyé‚£é‡Œã€‚ç„¶åç‚¹å‡»æ·»åŠ ï¼Œå¦‚æœå‡ºç°è¾“å¯†ç æ¡†å°±è¾“å…¥å¯†ç ã€‚ æ–°å»ºä»“åº“å…ˆå»Githubä¸Šå»ºç«‹ä¸€ä¸ªæ–°ä»“åº“ï¼Œç‚¹å‡»Repositoriesæ—NewæŒ‰é’®ï¼Œå¡«å†™Repoçš„åå­—åå°±å¯ä»¥ç¡®è®¤å»ºç«‹ã€‚æ–°å»ºå®Œåè‡ªåŠ¨è·³è½¬åˆ°è¿™ä¸ªåˆå§‹é¡µé¢ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™ä¸ªä»“åº“çš„httpsåœ°å€ï¼Œç‚¹å‡»æ—è¾¹çš„å¤åˆ¶æŒ‰é’®å¤‡ç”¨ã€‚ è¿æ¥ä»“åº“ç°åœ¨ï¼Œä¹‹å‰Gité‡Œçš„æ“ä½œéƒ½å¯ä»¥æ­£å¸¸ç”¨äº†ï¼Œä¸è¿‡æ•ˆæœä¾ç„¶å­˜åœ¨æœ¬åœ°ï¼Œæƒ³è¦å’Œäº‘ç«¯è¿æ¥ï¼Œæˆ‘ä»¬éœ€è¦å­¦ä¸€äº›æ–°æ“ä½œã€‚é¦–å…ˆå’Œremoteè¿æ¥ 1$ git remote add origin remote repository URL è¿™é‡Œçš„URLå°±æ˜¯ä¸Šä¸€æ­¥æ“ä½œå¤åˆ¶å¾—åˆ°çš„ã€‚ç„¶å 1$ git push -u origin master è¿è¡Œå®Œä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥åœ¨Githubä¸Šçœ‹åˆ°å•¦ã€‚æ¯å½“æˆ‘ä»¬æƒ³è¦å‘å¸ƒä¸€ä¸ªæ–°çš„åˆ†æ”¯çš„æ—¶å€™ï¼Œéƒ½éœ€è¦è¿è¡Œè¿™ä¸ªå‘½ä»¤ï¼Œè¦å°†masteræ”¹æˆåˆ†æ”¯çš„åå­—å³å¯ã€‚ æ›´æ–°å½“remoteå’Œæœ¬åœ°çš„ç‰ˆæœ¬ä¸ä¸€è‡´æ—¶ï¼Œæˆ‘ä»¬ä¼šéœ€è¦è¿›è¡Œæ›´æ–°ï¼Œå¯èƒ½æ˜¯æœ¬åœ°å¿«äºremoteä¹Ÿå¯èƒ½æ˜¯remoteå¿«äºæœ¬åœ°ã€‚æˆ‘ä»¬ä½¿ç”¨git pullæ¥æ‹‰å–remoteçš„æ›´æ–°ï¼Œä½¿ç”¨å‰è¦è®°å¾—å…ˆcommitæœ¬åœ°çš„ç‰ˆæœ¬å“¦ã€‚å¦‚æœè¦æ›´æ–°remoteç‰ˆæœ¬ï¼Œæˆ‘ä»¬ç›´æ¥ç”¨git pushå°±å¯ä»¥åšåˆ°äº†ã€‚ å…‹éš†åœ¨Githubä¸Šæ‰“å¼€ä½ è¦å…‹éš†çš„ä»“åº“ä¸»é¡µï¼Œç‚¹å‡»Clone or Downloadï¼Œç„¶åå¯ä»¥é€‰æ‹©ç”¨httpsè¿˜æ˜¯sshè¿›è¡Œå…‹éš†ï¼Œå¤åˆ¶é“¾æ¥ï¼Œæ‰“å¼€å‘½ä»¤è¡Œ 1$ git clone url å¦‚æœä¸æ˜¯ä½ çš„ä»“åº“ï¼Œåªèƒ½ä½¿ç”¨httpså“¦ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥é€‰æ‹©ä¸‹è½½ã€‚ Githubç½‘é¡µæ—¢ç„¶ä»–æ˜¯æœ‰ç½‘é¡µç‰ˆçš„ï¼Œé‚£è‡ªç„¶ä¸èƒ½å¿½ç•¥ç½‘é¡µç‰ˆæä¾›çš„æœåŠ¡ã€‚ æœç´¢æˆ‘è¦è¯´çš„æœç´¢æ˜¯åœ¨é¡µé¢é¡¶éƒ¨Navigation baré‡Œçš„é‚£ä¸ªæœç´¢æ¡†ï¼Œåœ¨å“ªé‡Œå¯ä»¥è¿›è¡Œå…³é”®è¯æœç´¢ï¼Œèƒ½æ£€ç´¢åˆ°æœ‰ç›¸å…³ä¿¡æ¯çš„å…¬å¼€ä»“åº“ã€‚åœ¨æœç´¢ç»“æœçš„é¡µé¢ï¼Œæˆ‘ä»¬å¯ä»¥é€‰å–ç‰¹å®šè¯­è¨€ï¼ˆè¿™ä¸ªé¡¹ç›®ä½¿ç”¨çš„è¯­è¨€ï¼‰ï¼Œç»“æœçš„æ’åºæ–¹æ³•ç­‰ç­‰ã€‚è¿˜æœ‰å…¶ä»–ä¸€äº›ä¿¡æ¯ï¼Œå°±è‡ªå·±å»çœ‹å•¦ã€‚ æ¯ä¸ªä»“åº“éƒ½ä¼šæ˜¾ç¤ºstarsï¼Œä¸€èˆ¬æ¥è¯´starsè¶Šå¤šï¼Œè®¤å¯åº¦è¶Šé«˜ï¼Œä¹Ÿå°±è¶Šå¥½ã€‚ å…³äºRepoå› ä¸ºè¦ç´ è¿‡å¤šæœ‰å¾ˆå¤šå†…å®¹ï¼Œæˆ‘å°±é€‰å–éƒ¨åˆ†è¯´æ˜ï¼Œå…¶ä»–çš„åŠŸèƒ½è‡ªå·±åº·åº·å°±å¥½å•¦ã€‚ æ–‡ä»¶ ç‚¹åˆ°è‡ªå·±çš„ä»»æ„ä¸€ä¸ªä»“åº“ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨ç½‘ä¸Šç¼–è¾‘æ–‡ä»¶ã€‚ç‚¹å¼€æ–‡ä»¶Aï¼Œç„¶åç‚¹å‡»Editå°±å¯ä»¥è¿›è¡Œä¿®æ”¹ï¼Œä¿®æ”¹å®Œæˆåä½¿ç”¨ä¸‹æ–¹çš„commitè¿›è¡Œä¿å­˜ã€‚ä¹Ÿå¯ä»¥ç›´æ¥æ–°å»ºæ–‡ä»¶ï¼Œä¸Šä¼ æ–‡ä»¶ï¼Œåˆ é™¤æ–‡ä»¶ç­‰ã€‚ä¸è¿‡æ¯”è¾ƒç¥å¥‡çš„æ“ä½œæ˜¯æ–°å»ºæ–‡ä»¶å¤¹ã€‚å…·ä½“å¦‚ä¸‹ï¼š ç‚¹å‡»Create New Fileï¼Œåœ¨æ–‡ä»¶åçš„åœ°æ–¹è¾“å…¥æ–‡ä»¶å¤¹åç§°å¹¶åŠ ä¸Š/, ç¥å¥‡çš„äº‹æƒ…å‘ç”Ÿäº†ï¼Œæ–‡ä»¶å¤¹å°±å»ºå¥½äº†ã€‚ä¸è¿‡ä¸å¯ä»¥å»ºç©ºæ–‡ä»¶å¤¹å“¦ï¼Œæ‰€ä»¥å®ƒä¼šå¼ºè¡Œè®©ä½ å†™ä¸€ä¸ªæ–‡ä»¶åã€‚ åˆä½œ å¦‚æœæƒ³å’Œç»„å‘˜å…±åŒç¼–è¾‘ä¸€ä¸ªä»“åº“ï¼Œæˆ‘ä»¬ç‚¹å‡»settingsï¼Œç„¶åManage Accessï¼Œè¿™ä¸ªæ—¶å€™ä¼šè®©ä½ è¾“å…¥ä¸€ä¸‹å¯†ç ã€‚è¿›å»åå¯ä»¥ä¿®æ”¹å½“å‰ä»“åº“æƒé™ä»¥åŠé‚€è¯·åˆä½œè€…ï¼Œæä¾›è¾“å…¥åˆä½œäººçš„Githubåå­—æ¥è¿›è¡ŒæŸ¥æ‰¾å’Œæ·»åŠ ã€‚ å…¶ä»–æ€ä¹ˆfetchä¸€ä¸ªäº‘ç«¯çš„åˆ†æ”¯ï¼ˆä¸åœ¨æœ¬åœ°çš„ï¼‰ï¼Ÿ 1$ git checkout --track origin/daves_branch å¦‚æœä¸æƒ³ç”¨å‘½ä»¤è¡Œæ€ä¹ˆåŠï¼Ÿ Github Desktopä½ å€¼å¾—æ‹¥æœ‰ï¼šhttps://desktop.github.com/ 4. å†™åœ¨æœ€åæ„Ÿè°¢å¤§å®¶çœ‹å®Œæˆ‘çš„åºŸè¯æ•™ç¨‹ï¼Œè¿™å¤§æ¦‚æ˜¯æœ¬äººå¤šå¹´ä¸€å¹´çš„ä½¿ç”¨ç»å¸¸ç”¨åˆ°çš„éƒ¨åˆ†ï¼Œå¸Œæœ›å¯ä»¥ç»™ä½ ä»¬å¸¦æ¥å¸®åŠ©ã€‚","link":"/2020/03/21/%E3%80%90%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7%E3%80%91Git%E5%92%8CGithub%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"},{"title":"Paper Revirew","text":"DEFINING LIVENESS Bowen ALPERN and Fred B. SCHNEIDERThis paper provides a formal definition of liveness property and has shown that no less restrictive definition would be correct. It also proves some theorems based on the definition. After the formalization of safety property, this paper provides some insight on liveness properties and shows that all properties are the intersection of safety and liveness properties. This research is important because it converts the definition of liveness property from â€œsometing good will happenâ€ to an expression which helps understand the meaning and characteristics of liveness properties. As mentioned in the conclusion, â€˜good thingsâ€™ and â€˜bad thingsâ€™ are not well-defined concepts, the topological definition given in the paper indeed gives correct intuition about liveness properties. Though it might be unnecessary, when temporal logic been mentioned the first time, it should be given some explanations. In general, this paper clearly states the definition the liveness properties and explains in an understandble way.","link":"/2020/08/11/paperReview/"},{"title":"å­¦ç”Ÿäº‹åŠ¡ç³»ç»Ÿ(xyzsas)","text":"It is my first time to participate in a real world project from the design phase to product phase. SummaryI have indeed learnt a lot from this project, thanks to the team leader and code reviewer Phantomlsh. The aim of this project is to provide services to high schools or even secondary schools and primary schools, including course enrollments, surveys, notices. Currently yzzx is using this system. Design phaseIt was started in July, 2020 and had lasted for about a month. The main problem we had to handle was concurrency because of the requirement of enrollment. We had discussed a lot of the backend model, message formatsâ€¦ One day, Phantomlsh suggested that we could use table store and function compute provided by Alibaba Cloud. This is a huge process so that we moved into development phase. Development phaseWe have four main developers: Phantomlsh(leader, main code reviewer, full stack developer), Cmdblockzqg(full stack developer, code reviewer), Tina(front-end developer, code reviewer), Queenie(Backend developer, code reviewer). Actually all our team members are full stack developers, but we are asigned to different roles to accomplish this task together. Both phantomlsh and Cmdblockzqg have made the previous version of the student affair system and they knew what the challenges are and what should we achieve in miminal.During the development, our client is using the system and giving us feedbacks as well as some new feature required.I will talk more about myself and my experience. I didnâ€™t think stuff like user stories were important until this time. When designing complex pages with mutiple components, thinking as a user is extremely crutial. Because this not only help improve user experience but can improve the logic of the program. Decoupling is also necessary(though it couldnâ€™t be done in front-end, but MVVM can help to create components). It can help reduce the size of a single file. Moreover, split the view into a cominbation of different components improves the code reuseability and reduce the code smell. Specific issues faced during the developmentThis time, I have written most of the front-end code and met lots of small issues.","link":"/2020/09/16/xyzsas/"},{"title":"Operating Systems","text":"The more you know about operating system, the better you can cooperate with it. Operating System OverviewRoleRole 1 The Operating System is an Abstract Machine Extends the basic hardware with added functionality Provides high-level abstractions More programmer friendly Common core for all applications It hides the details of the hardware Makes application code portable Role 2 The Operating System is a Resource Manager Responsible for allocating resources to users and processes Must ensure No Starvation Progress Allocation is according to some desired policy First-come, first-served; Fair share; Weighted fair share; limits (quotas), etcâ€¦ Overall, that the system is efficiently used Structural (Implementation) View the Operating System is the software Privileged mode. Operating System Kernel Portion of the operating system that is running in privileged mode Usually resident (stays) in main memory Contains fundamental functionality Whatever is required to implement other services Whatever is required to provide security Contains most-frequently used functions Also called the nucleus or supervisor The Operating System is Privileged Applications should not be able to interfere or bypass the operating system OS can enforce the â€œextended machineâ€ OS can enforce its resource allocation policies Prevent applications from interfering with each other The Structure of a Computer System Applications interact with themselves and via function calls to library procedures OS and application interacts via System calls A Note on System Libraries System libraries are just that, libraries of support functions (procedures, subroutines) Only a subset of library functions are actually system calls, and system call functions are in the library for convenience Privilege-less OS Can implement OS functionality, but cannot enforce it. Some Embedded OSs have no privileged component All software runs together No isolation One fault potentially brings down entire system Operating System Software Fundamentally, OS functions the same way as ordinary computer software. It is machine code that is executed (same machine instructions as application). It has more privileges (extra instructions and access). Operating system relinquishes control of the processor to execute other programs, it reestablishes control after System calls and Interrupts (especially timer interrupts). Operating System Internal Structure The Monolithic Operating System Structure, Also called the â€œspaghetti nestâ€ approach(Everything is tangled up with everything else. ) Processes and ThreadsMajor Requirements of an Operating System Interleave the execution of several processes to maximize processor utilization while providing reasonable response time Allocate resources to processess Support interprocess communication and user creation of processes Processes and ThreadsProcesses Also called a task or job Execution of an individual program â€œOwnerâ€ of resources allocated for program execution trace the usage of processes, clean up memory after finishing execution Encompasses one or more threads Threads The sequence of execution through an application Unit of execution Can be traced list the sequence of instructions that execute Belongs to a process Process provide environment, and all thread running in the process share the environment Executes within it ProcessThe Process ModelSingle process machine Multiprogramming of four programs Conceptual model of 4 independent, sequential processes(with a single thread each) Only one program active at any instant the box represents a process, and the line represents the thread and the dotted lines represent what the OS can do Process and thread models of selected OSes Single process, single thread MSDOS Signle process, multiple threads OS/161 as distributed Multiple processes, single thread Traditional UNIX Multiple processes, multiple threads Modern Unix(Linux, Solaris), Windows Process CreationPrincipal events that cause process creation System initialization Foreground processes(interactive programs) Background processes Email server, web server, print server, etc. Called a daemon(unix) or service(Windows) Execution of a process creation system call by running a process New login shell for an incoming ssh connection User request to create a new process Initiation of a bach job Note: Technically, all these cases use the same system machanism to create new processes Process TerminationConditions which terminate processes Normal exit(voluntary) Error exit(voluntary) Fatal error(involuntary) e.g. segmentation fault Killed by another process(involuntary) Implementation of Processes A processesâ€™ information is stored in a process control block(PCB) The PCBs form a process table Reality can be complex(hashing, chaining, allocation bitmaps,â€¦) Size can be dynamic Example fields of a process table entry Process Management Memory management File management Registers Pointer to text segment Root directory Program counter Pointer to data segment working directory Program status word Pointer to stack segement File descriptors Stack pointer User ID Process state Group ID Priority Scheduling parameters Process ID Parent process process group Signals time when process started CPU time used Childrenâ€™s CPU time Time of next alarm Process/Thread StatesThree states process model(the minial model the OSes implement) In the image the numbers prepresent: Process blocks for input Scheduler picks another process Scheculer picks this process Input becomes available Possible process/thread states running blocked ready Generally, if no IO, processes almost all sitting in running and ready Transitions between states shown some Transition Causing Events Running -&gt; Ready Voluntary Yield() End of timeslice Running -&gt; Blocked Waiting fot input File, network, Waiting for a timer(alarm signal) Waiting for a resource to become available Scheduler responsible for want to run next Sometimes alse called the dispatcher The literature is alse a little inconsistent on with terminology Has to choose a Ready process to run It is inefficent to search through all processes The Ready Queue What about blocked processes When an unblocking event occurs, we also wish to avoid scanning all processes to select one to make Ready using two queues(One option) another option(using a queue for every event) Implementation Minimally consist of three segments Text contains the code (instructions) Data Global variables Stack Activation records of procedure/function/method Local variables User-mode Processes (programs) scheduled by the kernel Isolated from each other No concurrency issues between each other System-calls transition into and return from the kernel Kernel-mode Nearly all activities still associated with a process Kernel memory shared between all processes Concurrency issues exist between processes concurrently executing in a system call ThreadThe Thread Model (a) Three processes each with one thread(b) One process with three thread Separating execution from the environment Per process items Per thread items Address space Program counter Global variables Registers Open files Stack Child processess State Pending alarms Signals and signal handlers Accounting information Items shared by all threads in a process Item private to each thread The Thread Model State implicitly stored on the stack Local variables are per thread Allocated on the stack Donâ€™t have concurrency issues Global variables are shared between all threads Allocated in the data section Concurrency control is an issue Dynamically allocated memory(malloc) can be glocal or local Program defined(the pointer can be global or local) If the pointer is global variable(has concurrency issue) while locals donâ€™t Thread UsageExample 1234567891011121314// dispatcher threadWhile(TRUE) { get_next_request(&amp;buf); handoff_work(&amp;buf);}// worker thread - can overlap disk I/O with the execution of other threadswhile (TRUE) { wait_for_work(&amp;buf); look_for_page_in_cache(&amp;buf, &amp;page); if (page_not_in_cache(&amp;page) read_page_from_disk(&amp;buf, &amp;page); return_page(&amp;page);} Model Characteristics Threads Parallelism, blocking system calls Sigle-threaded process No parallelism, blocking system calls Finite-state machine Parallelism, nonblocking system calls, interrupts Why Threads? Simpler to program than a state machine Less resources are associated with them a complete process Cheaper to create and destory Shares resources(especially memory) between them Performace: Threads waiting for I/O can be overlapped with computing threads Note if all threads are compute bound, then there is no performance improvement(on a uniprocessor) Threads can take advantage of the parallelism available on machines with more than one CPU(multiprocessor) User-level Threads Implementation at user-level User-level Thread Control Block (TCB), ready queue, blocked queue, and dispatcher Kernel has no knowledge of the threads (it only sees a single process) If a thread blocks waiting for a resource held by another thread inside the same process, its state is saved and the dispatcher switches to another ready thread Thread management (create, exit, yield, wait) are implemented in a runtime support library Pros Thread management and switching at user level is much faster than doing it in kernel level No need to trap (take syscall exception) into kernel and back to switch Dispatcher algorithm can be tuned to the application Can be implemented on any OS (thread or non-thread aware) Can easily support massive numbers of threads on a per-application basis Use normal application virtual memory Kernel memory more constrained. Difficult to efficiently support wildly differing numbers of threads for different applications. Cons Threads have to yield() manually (no timer interrupt delivery to userlevel) Co-operative multithreading A single poorly design/implemented thread can monopolise the available CPU time There are work-arounds (e.g. a timer signal per second to enable preemptive multithreading), they are course grain and a kludge. Does not take advantage of multiple CPUs (in reality, we still have a single threaded process as far as the kernel is concerned) If a thread makes a blocking system call (or takes a page fault), the process (and all the internal threads) blocks Canâ€™t overlap I/O with computation Kernel-level threads Cons Thread creation and destruction, and blocking and unblocking threads requires kernel entry and exit. More expensive than user-level equivalent Pros Preemptive multithreading Parallelism Can overlap blocking I/O with computation Can take advantage of a multiprocessor Context Switch A context switch can refer to a switch between threads, involving saving and restoring of state associated with a thread; A switch between processes, involving the above, plus extra state associated with a process. Context Switch OccurrenceA switch between process/threads can happen any time the OS is invoked On a system call Mandatory if system call blocks or on exit() On an exception Mandatory if offender is killed On an interrupt Triggering a dispatch is the main purpose of the timer interrupt A thread switch can happen between any two instructions Note instructions do not equal program statements Context Switch Context switch must be transparent for processes/threads When dispatched again, process/thread should not notice that something else was running in the meantime (except for elapsed time) OS must save all state that affects the thread This state is called the process/thread context Switching between process/threads consequently results in a context switch. Concurrency and SynchronisationConcurrency Example 12345678910111213141516// count is a global variable shared between two threadsint count = 0;void increment () { int t; t = count; t = t + 1; count = t;{void decrement (){ int t; t = count; t = t - 1; count = t;} Have a race condition. Occurred when global variables shared by threads There is in-kernel concurrency even for single-threaded processThe OS has to deal with concurrency even if it is a single thread application. Critical Region We can control access to the shared resource by controlling access to the code that accesses the resource A critical region is a region of code where shared resources are accessed(e.g. Variables, memory, files, etcâ€¦) Uncoordinated entry to the critical region results in a race condition =&gt; Incorrect behavior, deadlock, lost work,â€¦ Indentifying critical regions Critical regions are regions of code that: Access a shared resource, and correctness relied on the shared resource not being concurrently modified by another thread/process/entity. 12345678910111213141516// count is a global variable shared between two threadsint count = 0;void increment (){ int t; t = count; // the start of critical region t = t + 1; count = t; // end{void decrement (){ int t; t = count; // start t = t - 1; count = t; // end} if we can make the critical regions never overlap, we wonâ€™t have concurrency problem. Example critical regions 1234567891011121314151617181920212223242526struct node { int data; struct node *next;}struct node *head;void init(void) { head = NULL;}void insert(struct *item){ item-&gt;next = head; head = item;}struct node *remove(void) { struct node *t; t = head; if (t != NULL) { head = head-&gt;next; } return t;} Race example 12345678910111213// thread 1void insert(struct *item){ item-&gt;next = head; // 1 head = item; // 2}// thread 2void insert(struct *item){ item-&gt;next = head; // 3 head = item; // 4} one possible sequence: 1 -&gt; 3 -&gt; 2 -&gt; 4, then the item in thread 1 is lost(seg fault)both1-2 and 3-4 are critical regions Critical regions solutions We seek a solution to coordinate access to critical regions Also called critical sections Conditions required of any solution to the critical region problem Mutual Exclusion: No two processes simultaneously in critical region No assumptions made about speeds or numbers of CPUs Progress No process running outside its critical region may block another process Bounded No process waits forever to enter its critical region A solution? A lock variable if lock == 1, somebody is in the critical section and we mush wait if lock == 0, nobody is in the critical section and we are free to enter 1234567891011121314151617181920// A problematic execution sequencewhile (TRUE) { while (lock == 1); // 1 lock = 1; // 2 critical(); // 3 lock = 0; // 4 non_critical(); // 5}while (TRUE) { while (lock == 1); // 6 lock = 1; // 7 critical(); // 8 lock = 0; // 9 non_critical(); // 10}/* does not work because if it execute like* 6 -&gt; 1 -&gt; 2 -&gt; 3* both of them see the lock is not one and may all work in their critical section*/ In the example above, there is race contition between the observation that the lock variable is not one and setting the lock to prevent somebody else from entering. Mutual Exclusion by Taking Turns 123456789101112while (TRUE) { while (turn != 0); critcal_rigion(); turn = 1; noncritical_region();}while (TRUE) { while (turn != 1); critcal_rigion(); turn = 0; noncritical_region();} This example works because the update is only been updated by the thread having its turn. Works due to strict alternation Each process takes turns Cons Busy waiting(loop) sol: not my turn, do sth else, come back to my turn Process must wait its turn even while the other process is doing something else. With many processes, must wait for everyone to have a turn Does not guarantee progress if a process no longer needs a turn Poor solution when processes require the critical section at differing rates Mutual Exclusion by Disabling Interrupts only used in very short critical sections, because it prespond other devices in the machine and make them less responsive Before entering a critical region, disable interrupts After leaving the critical region, enable interrupts Pros simple Cons Only available in the kernel Blocks everybody else, even with no contention slows interrupt response time Does not work on a multiprocessor Hardware Support for mutual exclusion Test and set instruction can be used to implement lock variables correctly It loads the value of the lock If lock == 0, set the lock to 1 return the result 0 â€“ we acquire the lock If lock == 1 return 1 â€“ another thread/process has the lock Hardware guarantees that the instruction executes atomically(atomically: as an indivisible unit) 123456789enter_region: TSL REFISTER,LOCK | copy lock to register and set lock to 1 CMP REGISTER, #0 | was lock one? JNE enter_region | if it was non zero, lock was set, so loop RET | return to caller; critical region enteredleave_region: MOVE LOCK, #0 | store a 0 in lock RET | return to caller Pros Simple(easy to show itâ€™s correct) Available at user-level To any number of processors To implement any number of lock variables Cons Busy waits(also termed a spin lock) consumes CPU starvation is possible when a process leaves its critical section and more than one process is waiting Tackling the Busy-Wait Problem Sleep / Wake up The idea When process is waiting for an event, it calls sleep to block, instead of busy waiting The event happens, the event generator(another process) calls wakeup to unblock the sleeping process. Walking a ready/running process has no effect The producer-Consumer Problem Also called the bounded buffer problemA producer produces data items and stores the items in a bufferA consumer takes the items out of the buffer and consumes them. two problems: Concurrency(because shared resource) and the producer should be blocked when the buffer is full and the consumer should be blocked when the buffer is empty. We must keep accurate count of items in the buffer The consumer can call wakeup when it consumes the first entry of the full buffer The Producer can call wakeup when it adds the first item to the buffer Semaphores Dijkstra introduced two primitives that are more powerful than simple sleep and wakeup alone. P(): proberen, from Dutch to test V(): verhogen, from Dutch to increment Also called wait &amp; signal, down &amp; up How do they work If a resource is not available, the corresponding semaphore blocks any process waiting for the resource Blocked processes are put into a process queue maintained by the semaphore(avoids busy waiting) When a process releases a resource, it signals this by means of the semaphore Signalling resumes a blocked process if there is any Wait(P) and signal(V) operations cannot be interrupted Complex coordination can be implemented by multiple semaphores Semaphore implementation Define a semaphore as a record Each primitive is atomit(wait and signal), the OS would use the disabling of interrupts in the implementation of Semaphore inside the OS. 1234567891011121314151617typedef sturct { int count; struct process *L;} semaphore;// Semaphore operations now defined aswait(S): S.count --; if (S.count &lt; 0) { add this process to S.L; sleep; }signal(S): S.count++; if (S.count &lt;= 0) { remove a process P from S.L; wakeup(P); } Semaphore as a General Synchronization Tool Execute B in Pj only after A executed in Pi Use semaphore count initlialized to 0 Code: A first: signal(count is 1) -&gt; then B runs(count is 0) -&gt; A(this is what we want) B first: wait(count &lt; 0, sleep) -&gt; A runs, wake up B(still what we want) Semaphore implementation of a Mutex Mutex is short for Mutual Exclusion Can also be called a lock 12345semaphore mutex;mutex.count = 1; /* init the mutex */wait(mutex); /* enter the critical region */Blahblah();signal(mutex); /* exit the critical region */ Notice that the inital count determines how mant waits can progress before blocking and requiring a signal =&gt; mutex.count initialised as 1 Solve the producer-consumer problem 1234567891011121314151617181920212223242526272829#define N = 4;semaphore mutex = 1;/* count empty slots */semaphore empty = N/* count full slots */semaphore full = 0;prod() { while(TRUE) { item = produce() wait(empty); wait(mutex) insert_item(); signal(mutex); signal(full); }}con() { while(TRUE) { wait(full); wait(mutex); remove_item(); signal(mutex); signal(empty); }} Summary Semaphores can be used to solve a varity of concurrency problems However, programming with then can be error-prone E.g. must signal for every wait for mutexes Too many or few signals or waits, or signals and waits in the wrong order, can have catastrophic results Monitors To ease concurrent programming, Hoare(1974) proposed monitors A higher level synchronisation primitive Programming language construct Idea A set of procedures, variables, data types are grouped in a special kind of module, a monitor. Variables and data types only accessed from within the monitor Only one process/thread can be in the monitor at any one time Mutual exclusion is implemented by the compiler(which should be less error prone) When a thread calls a monitor procedure that has a thread already inside, it is queued and it sleeps until the current thread exits the monitorexample 12345678910111213141516monitor exmaple integer i; condition c; procedure producer(); . . . end; procedure consumer(); . . . end;end monitor; How do we block waiting for an event? We need a mechanism to block waiting for an event( in addition to ensuring mutual exclusion) for producer consumer problem, when buffer is empty or full Condition variables To allow a process to wait within the monitor, a condition variable must be declared, as condition x, y Condition variable can only be used with the operations wait and sigal The operation x.wait(); means that the process invoking this operation is suspended until another process invokes Another thread can enter the monitor while original is suspended x.signal(); The operation resumes exactly on suspended process. If no process is suspended, then the signal opeartion has no effect. The Readers and Writers Problem Models access to a data base E.g. airline reservation system Can have more thant one concurrent reader To check schedules and reservations Writers must have exclusive access To book a ticket or update a schedule A solution 12345678910111213141516171819202122232425262728typedef in semaphore; /* use your imagination */semaphore mutex = 1; /* controls access to 'rc'*/semaphore db = 1; /* controls access to the database */int rc = 0; /* # of processes reading ot wanting to */void reader(void) { while(TRUE) { /* repeat forever */ down(&amp;mutex); /* get exclusive access to 'rc' */ rc = rc + 1; /* one reader more now */ if(rc == 1) down(&amp;db); /* if this is the first reader */ up(&amp;mutex); /* release exclusive access to 'rc' */ read_data_base(); /* access the data */ down(&amp;mutex); /* get exclusive access to 'rc' */ rc = rc - 1; /* one reader fewer now */ if(rc == 0) up(&amp;db); /* if this is the last reader */ up(&amp;mutex); /* release exclusive access to 'rc */ use_data_read(); /* noncritical region */ } void writer(void) { while (TRUE) { /* repeat forever */ think_up_date(); /* noncritical regionn */ down(&amp;db); /* get exclusive access */ wirte_data_base(); /* update the data */ up(&amp;db); /* release exclusive access */ } }} DeadlockResources Examples of computer resources printers tape drives Tables in a database Processes need access to resources in reasonable order Preemptable resources e.g. virtual memory can be taken away from a process with no ill effects Nonpreemtable resources will cause the process to fail if take away Resources &amp; Deadlocks Suppose a process holds resource A and requests resource B at same time another process holds B and requests A both are blocked and remain so â€“ Deadlock Deadlocks occur when â€¦ processes are granted exclusive access to devices, locks, tables, etc.. we refer to these entities generally as resources Resource Access Sequence of events required to use a resource request the resource use the resource release the resource Must wait if request is denied requesting process may be blocked may fail with error code Deadlock A set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause. Usually the event is release of a currently held resource None of the processes can â€¦ run release resources be awakened Four Conditions for Deadlock Mutual exclusion condition each resource assigned to 1 process or is available Hold and wait condition process holding resources can request additional No preemption condition previously granted resources cannot be forcibly taken away Circular wait condition must be a circular chain of 2 or more processes each is waiting for resource held by next member of the chain Strategies for dealing with Deadlocks Just ignore the problem altogether prevention â€‹ Negating one of the four necessary conditions Detection and recovery Dynamic avoidance â€‹ Careful resource allocation Appoarch 1: The Ostrich Algorithm Pretend there is no problem Reasonable if deadlocks occur very rarely Cost of prevention is high Example of â€œcostâ€, only one process runs at a time UNIX and Windows takes this approach for some of the more complex resource relationships they manage Itâ€™s a trade off between Convenience (engineering approach) Correctness(mathematical approach) Approach 2: Deadlock Prevention The most common used strategy Resource allocation rules prevent deadlock by prevent one of the four condition required for deadlock from occuring Mutual exclusion Not feasible in general Some devices/resource are intrinsically not shareable Hold and wait Require processes to request resources before starting a process never has to wait for what it needs Issues may not know required resources at start of run must be determined at run time or by user input(e.g. using Word to open a file) $\\Rightarrow$ not always possible Also ties up resources other processes could be using Varitions: Process must give up all resources if it would block holding a resource then request all immediately needed prone to livelock Livelocked processes are not blocked, change state regularly, but never make progress. No preemption This is not a viable option Circular Wait aquire resource in the same order, order resources Approach 3: Detection and Recovery Need a method to determine if a system is deadlocked Assuming deadlocked is detected, we need a method of recorvery to restore progress to the system. Modeling resources and processes as a directed graph (a) represents everthing in the system (b) is a deadlock What about resources with multiple units? Some examples of multi-unit resources RAM Blocks on a hard disk drive Slots in a buffer We need an approach for dealing with resources that consists of more than a single unit Modeling using matrix Data structured needed by deadlock detection algorithm sum of current resource allocation + resources available = resources that exist$$\\sum_{i=1}^{n}{C_{ij}}+A_j = E_j$$Example Process 1: has 1 scanners Process 2: has 2 tape drivers and 1 CD roms Process 3: has 1 plotters and 2 scanners Detection Algoritm Look for an unmarked process $Pi$, for which the $i$-th row of R is less than equal to A If found, add the $i$-th row of C to A, and mark $Pi$. Got to step 1 If no such process exists, terminate Remaining processes are deadlocked. Recovery from Deadlock Recovery through preemption Take a resource from some other process Depends on nature of the resource Recovery through rollback Checkpoint a process periodically not come for free, not practical Use this saved state Restart the process if it is found deadlocked No guarantee is wonâ€™t deadlock again Recovery through killing processes Crudest but simplest way to break a deadlock Kill one of the processes in the deadlock cycle The other processes get its resources Choose process that can be rerun from the beginning Approach 4 Deadlock Avoidance Not practical for all systems, need to know enough information in advance Safe and Unsafe States A state is safe if The system is not deadlocked There exists a scheduling order that results in every process running to completion, even if they all request their maximum resources immediately Unsafe states are not necessarily deadlocked with a lucky sequence, all processes may complete However, we cannot guarantee that they will complete(not deadlock) Safe states guarantee we will eventually complete all processes Deadlock avoidance algorithm Only grant requests that result in safe states Bankers Algorithm Modelled on a Banker with Customers The banker has a limited amount of money to loan customers Limited number of resources Each customer can borrow money up to the customerâ€™s credit limit Maximum number of resources required Basic idea keep the bank in a safe state So all customers are happy even if they all request to borrow up to their credit limit at the same time Customers wishing to borrow such that the back would enter an unsafe state must wait until somebody else repays their loan such that the transaction becomes safe. Bankers Algorithm is not commonly used in practice It is difficult (sometimes impossible ) to know in advance the resources a process will require the number of processes in a dynamic system Starvation A process never receives the resource it is waiting for, despite the resource(repeatedly) becoming free, the resource is always allocated to another waiting process. One solution: First-come, first-serve policy System CallSystem Calls Can be viewed as special function calls Provides for a controlled entry into the kernel While in kernel, they perform a privileged operation Returns to original caller with the result The system call interface represents the abstract machine provided by the operating system. A brief overviewFrom the userâ€™s perspective Process Management File I/O Dirctories management Some other selected Calls There are many more On Linux, see man syscalls for a list Process Management Call Descritption pid=fork() Create a child process identical to the parent Pid = waitpid(pid, &amp;statloc, options) Wait for a child to terminate s = execve(name, argv, environp) Replace a processâ€™ core image exit(status) Terminate process execution and return status File Management Call Descrition fd=open(file, how, â€¦,) Open a file for reading, writing or both s = close(fd) Close an open file n = read(fd, buffer, bytes) Read data from a file into a buffer n = write(fd, buffer, bytes) Write data from a buffer into a file Position = lseek(fd, offset, whence) Move the file pointer s = stat(name, &amp;buf) Get a fileâ€™s status information A stripped down shell:1234567891011while(TRUE) { /* repeat forever */ type_prompt(); /* display prompt */ read_command(command, parameters); if (fork() != 0) { /* fork off child process*/ /* Parent code */ waitpid(-1, &amp;status, 0); /* wait for child to exit */ } else { /* Child code */ execve(command, parameters, 0); /* execute command */ }} System Call ImplementationA Simple Model of CPU ComputationThe fetch-execute cycle Load memory contents from address in program counter (PC) The instruction Execute the instruction increment PC Repeat A Simple Model of CPU Computation Stack Pointer(SP) Status Register Condition codes Positive result Zero result Negative result General Purpose Register Holds operands of most instructions Enables programmers (compiler) to minmise memory reference Privileged-mode OperationTo protect operating system execution, two or more CPU modes of operation exist Privileged mode(system, kernel-mode) All instructions and register are available User-mode Users â€˜safeâ€™ subset of instruction set Only affects the state of the application itself They cannot be used to uncontrollably interference with OS Only â€˜safeâ€™ registers are accessible System CallSystem Call Mechanism Overview System call transitions triggered by special processor instructions User to Kernel System call instruction Kernel to User Return from privileged mode instruction Processor mode Switched from user-mode to kernel-mode Switched back when returning to user mode Stack Pointer (SP) User-level SP is saved and a kernel SP is initialised User-level SP restored when returning to user-mode Program Counter (PC) User-level PC is saved and PC set to kernel entry point User-level PC restored when returning to user-level Kernel entry via the designated entry point must be strictly enforced Registers Set at user-level to indicate system call type and its arguments A convention between applications and the kernel Some registers are preserved at user-level or kernel-level in order to restart user-level execution Depends on language calling convention etc Result of system call placed in registers when returning to user-level Another convention Why do we need system calls? Why not simply jump into the kernel via a function call???? Function calls do not Change from user to kernel mode and eventually back again Restrict possible entry points to secure locations To prevent entering after any security checks Steps in Making a System Call File ManagementOverview of the FS abstraction Userâ€™s view Under the hood Uniform namespace Heterogenrous collection of storage devices Hierachical structure Flat address space(block numbers) Arbitrarily-sized files Fixed-size blocks Symbolic file names Numeric block addresses Access control No access control Tools for (formatting, defragmentation, Backup, Consistency checking) File NamesFile system must provide a convenient naming scheme Textual Names May have restrictions Only certain characters Limited length Only certain format Case (in)sensitive Names may obey conventions Interpreted by tools interpreted by operating system File StructureSequence of Bytes OS consider a file to be unstructured Applications can impose their own structure Used by UNIX, Window, most modern OSes File TypesRegular File Directories Device Files May be divided into Character Devices â€“ stream of bytes Block Devices Some systems distinguish between regular file types ASCII text files, binary files File Access Types(Patterns)Sequential access read all bytes/records from the beginning cannot jump around, could rewind or back up convenient when medium was magnetic tape Random access bytes/records read in any order essential for data base systems read can be â€¦ move file pointer (seek), then read or lseek(location,â€¦);read(â€¦) each read specifies the file pointer read(location,â€¦) 11 Typical File OperationsCreate Delete Open Close Read Write Append Rename Seek Get attributes Set Attributes File Organisation and Access(Programmerâ€™s Perspective) Given an operating system supporting unstructured files that are a stream-of-bytes,how can one organise the contents of the files? E.g. Executable Linkable Format (ELF) Some possible access patterns: Read the whole file â€“Read individual records from a file record = sequence of bytes containing the record Read records preceding or following the current one Retrieve a set of records Write a whole file sequentially Insert/delete/update records in a file Programmers are free to structure the file to suit the application Criteria for File OrganizationThings to consider when designing file layout Rapid access Needed when accessing a single record Not needed for batch mode read from start to finish Ease of update File on CD-ROM will not be updated, so this is not a concern Economy of storage Should be minimum redundancy in the data Redundancy can be used to speed access such as an index File DirectoriesProvide mapping between file names and the files themselves Contain information about files Attributes Location Ownership Directory itself is a file owned by the operating system Hierarchical (Tree-Structured) DirectoryFiles can be located by following a path from the root, or master, directory down various branches This is the absolute pathname for the file Can have several files with the same file name as long as they have unique path names Current Working DirectoryAlways specifying the absolute pathname for a file is tedious! Introduce the idea of a working directory Files are referenced relative to the working directory Relative and Absolute PathnamesAbsolute pathname A path specified from the root of the file system to the file A Relative pathname A pathname specified from the cwd Note: â€˜.â€™ (dot) and â€˜..â€™ (dotdot) refer to current and parent directory Typical Directory OperationsCreate Delete Opendir Closedir Readdir Rename Link Unlink Nice properties of UNIX namingSimple, regular format Names referring to different servers, objects, etc., have the same syntax. Regular tools can be used where specialised tools would be otherwise be needed. Location independent Objects can be distributed or migrated, and continue with the same names. File SharingIn multiuser system, allow files to be shared among users Two issues Access rights None User may not know of the existence of the file User is not allowed to read the directory that includes the file Knowledge User can only determine that the file exists and who its owner is Execution The user can load and execute a program but cannot copy it Reading The user can read the file for any purpose, including copying and execution Appending The user can add data to the file but cannot modify or delete any of the fileâ€™s contents Updating The user can modify, delete, and add to the fileâ€™s data. This includes creating the file, rewriting it, and removing all or part of the data Changing protection User can change access rights granted to other users Deletion User can delete the file Owners Has all rights previously listed May grant rights to others using the following classes of users Specific user User groups All for public files Management of simultaneous access Most OSes provide mechanisms for users to manage concurrent access to files Example: flock(), lockf(), system calls Typically User may lock entire file when it is to be updated User may lock the individual records (i.e. ranges) during the update Mutual exclusion and deadlock are issues for shared access File System The FS must map symbolic file names into a collection of block addresses The FS must keep track of which blocks belong to which files in what order the blocks form the file which blocks are free for allocation Given a logical region of a file, the FS must track the corresponding block(s) on disk Stored in file system metadata File Allocation Methods A file is divided into â€œblocksâ€ â€“ the unit of transfer to storage External and internal fragmentationExternal fragmentation The space wasted external to the allocated memory regions Memory space exists to satisfy a request but it is unusable as it is not contiguous Internal fragmentation The space wasted internal to the allocated memory regions Allocated memory may be slightly larger than requested memory; this size difference is wasted memory internal to a partition Contiguous Allocationâœ… Easy bookkeeping (need to keep track of the starting block and length of the file) âœ… Increases performance for sequential operations âŒ Need the maximum size for the file at the time of creation âŒ As files are deleted, free space becomes divided into many small chunks (external fragmentation) Dynamic Allocation Strategies Disk space allocated in portions as needed Allocation occurs in fixed-size blocks âœ… No external fragmentation âœ… Does not require pre-allocating disk space âŒ Partially filled blocks (internal fragmentation) âŒ File blocks are scattered across the disk âŒ Complex metadata management (maintain the collection of blocks for each file) Dynamic allocation: Linked list allocation Each block contains a pointer to the next block in the chain. Free blocks are also linked in a chain. âœ… Only single metadata entry per file âœ… Best for sequential files âŒ Poor for random access âŒ Blocks end up scattered across the disk due to free list eventually being randomised Dynamic Allocation: File Allocation Table (FAT)Keep a map of the entire FS in a separate table A table entry contains the number of the next block of the file The last block in a file and empty blocks are marked using reserved values The table is stored on the disk and is replicated in memory âœ… â€‹Random access is fast (following the in-memory list) âŒ Requires a lot of memory for large disks âŒ Free block lookup is slow Dynamical Allocation: inode-based FS structureIdea: separate table (index-node or i-node) for each file. Only keep table for open files in memory Fast random access The most popular FS structure today âŒ i-nodes occupy one or several disk areas âŒ i-nodes are allocated dynamically, hence free-space management is required for i-nodes Use fixed-size i-nodes to simplify dynamic allocation Reserve the last i-node entry for a pointer (a block number) to an extension i-node. i-node implementation issues Free-space management Approach 1: linked list of free blocks in free blocks on disk List of all unallocated blocks Background jobs can re-order list for better contiguity Store in free blocks themselves Does not reduce disk capacity Only one block of pointers need be kept in the main memory Approach 2: keep bitmaps of free blocks and free i-nodes on disk Individual bits in a bit vector flags used/free blocks May be too large to hold in main memory Expensive to search Concentrating (de)allocations in a portion of the bitmap has desirable effect of concentrating access Simple to find contiguous free space Implementing directories Directories are stored like normal files directory entries are contained inside data blocks The FS assigns special meaning to the content of these files a directory file is a list of directory entries a directory entry contains file name, attributes, and the file i-node number maps human-oriented file name to a system-oriented name Fixed-size vs variable-size directory entries Fixed-size directory entries Either too small Or waste too much space Variable-size directory entries Freeing variable length entries can create external fragmentation in directory blocks Can compact when block is in RAM Searching Directory Listings Locating a file in a directory Linear scan Implement a directory cache in software to speed-up search Hash lookup B-tree (100â€™s of thousands entries) Storing file attributes (a) disk addresses and attributes in directory entry â€“FAT (b) directory in which each entry just refers to an i-node â€“UNIX Inode Contents Size Offset of the highest byte written Block count Number of disk blocks used by the file. Note that number of blocks can be much less than expected given the file size Files can be sparsely populated E.g. write(f,â€œhelloâ€); lseek(f, 1000000); write(f, â€œworldâ€); Only needs to store the start and end of file, not all the empty blocks in between. Size = 1000005 Blocks = 2 + any indirect blocks Direct Blocks Block numbers of first 12 blocks in the file Most files are small â€“ We can find blocks of file directly from the inode Single Indirect Block â€“ Block number of a block containing block numbers Requires two disk access to read â€“ One for the indirect block; one for the target block Max File Size â€“ Assume 1Kbyte block size, 4 byte block numbers 12 * 1K + 1K/4 * 1K = 268 KiB For large majority of files (&lt; 268 KiB), given the inode, only one or two further accesses required to read any block in file Double Indirect Block â€“ Block number of a block containing block numbers of blocks containing block numbers Triple Indirect Hard Links Note that inodes can have more than one name -â€“ Called a Hard Link Symbolic links A symbolic link is a file that contains a reference to another file or directory Has its own inode and data block, which contains a path to the target file Marked by a special file attribute Transparent for some operations Can point across FS boundaries FS reliabilityDisk writes are buffered in RAM OS crash or power outage ==&gt; lost data Commit writes to disk periodically Use the sync command to force a FS flush FS operations are non-atomic Incomplete transaction can leave the FS in an inconsistent state Journaling file systems Keep a journal of FS updates Before performing an atomic update sequence write it to the journal Replay the last journal entries upon an unclean shutdown The ext3 journalOption1: Journal FS data structure updates âœ… Efficient use of journal space; hence faster journaling âŒ Individual updates are applied separately âŒ The journaling layer must understand FS semantics Option2: Journal disk block updates Ext3 implements Option 2 âŒ Even a small update adds a whole block to the journal âœ… Multiple updates to the same block can be aggregated into a single update âœ… The journaling layer is FSindependent (easier to implement) Journaling Block Device (JBD)JBD interface (continued) Commit: write transaction data to the journal (persistent storage) Multiple FS transactions are committed in one go Checkpoint: flush the journal to the disk Used when the journal is full or the FS is being unmounted Trade-off in FS block size File systems deal with 2 types of blocks Disk blocks or sectors (usually 512 bytes) File system blocks 512 * 2^N bytes Larger blocks require less FS metadata Smaller blocks waste less disk space (less internal fragmentation) Sequential Access The larger the block size, the fewer I/O operations required Random Access The larger the block size, the more unrelated data loaded Spatial locality of access improves the situation Choosing an appropriate block size is a compromise Virtual File System(VFS) Provides single system call interface for many file systems Transparent handling of network file systems File-based interface to arbitrary device drivers File-based interface to kernel data structures Provides an indirection layer for system calls File operation table set up at file open time Points to actual handling code for particular type Further file operations redirected to those functions VFS InterfaceTwo major data typesVFS Represents all file system types Contains pointers to functions to manipulate each file system as a whole (e.g. mount, unmount) Vnode Represents a file (inode) in the underlying filesystem Points to the real inode Contains pointers to functions to manipulate files/inodes (e.g. open, close, read, write,â€¦) File Descriptors Each open file has a file descriptor Read/Write/lseek/â€¦. use them to specify which file to operate on. State associated with a file descriptor File pointer Determines where in the file the next read or write is performed Mode Was the file opened read-only, etcâ€¦. Memory ManagementOS Memory Management Keeps track of what memory is in use and what memory is free Allocates free memory to process when needed And deallocates it when they donâ€™t Manages the transfer of memory between RAM and disk. Two broad classes of memory management systems Those that transfer processes to and from external storage during execution. Called swapping or paging Those that donâ€™t Simple Might find this scheme in an embedded device, dumb phone, or smartcard. Basic Memory ManagementMonoprogramming without Swapping or Paging Three simple ways of organizing memory MonoprogrammingOkay if Only have one thing to do Memory available approximately equates to memory required Otherwise, Poor CPU utilisation in the presence of I/O waiting Poor memory utilisation with a varied job mix Fixed partitioningSimple MM: Fixed, equal-sized partitions Any unused space in the partition is wasted Called internal fragmentation Processes smaller than main memory, but larger than a partition cannot run Simple MM: Fixed, variable-sized partitions Divide memory at boot time into a selection of different sized partitions Can base sizes on expected workload Each partition has queue: Place process in queue for smallest partition that it fits in. Processes wait for when assigned partition is empty to start Issue Some partitions may be idle Small jobs available, but only large partition free Workload could be unpredictable Alternative queue strategySingle queue, search for any jobs that fit Small jobs in large partition if necessary Increases internal memory fragmentation Fixed Partition Summary Simple Easy to implement Can result in poor memory utilisation Due to internal fragmentation Used on IBM System 360 operating system (OS/MFT) Announced 6 April, 1964 Still applicable for simple embedded systems Static workload known in advance Dynamic Partitioning Partitions are of variable length Allocated on-demand from ranges of free memory Process is allocated exactly what it needs Assumes a process knows what it needs We end up with unusable holes (external fragmentation) Classic ApproachRepresent available memory as a linked list of available â€œholesâ€ (free memory ranges). Base, size Kept in order of increasing address Simplifies merging of adjacent holes into larger holes. List nodes be stored in the â€œholesâ€ themselves Dynamic Partitioning Placement AlgorithmFirst-fit algorithm Scan the list for the first entry that fits If greater in size, break it into an allocated and free part Intent: Minimise amount of searching performed Aims to find a match quickly Biases allocation to one end of memory Tends to preserve larger blocks at the end of memory Next-fit Like first-fit, except it begins its search from the point in list where the last request succeeded instead of at the beginning. (Flawed) Intuition: spread allocation more uniformly over entire memory to avoid skipping over small holes at start of memory Performs worse than first-fit as it breaks up the large free space at end of memory. Best-fit algorithm Chooses the block that is closest in size to the request Performs worse than first-fit Has to search complete list does more work than first-fit Since smallest block is chosen for a process, the smallest amount of external fragmentation is left Create lots of unusable holes Worst-fit algorithm Chooses the block that is largest in size (worst-fit) (whimsical) idea is to leave a usable fragment left over Poor performer Has to do more work (like best fit) to search complete list Does not result in significantly less fragmentation Dynamic Partition Allocation Algorithm Summary First-fit generally better than the others and easiest to implement Compaction We can reduce external fragmentation by compaction Shuffle memory contents to place all free memory together in one large block. Only if we can relocate running programs? Pointers? Generally requires hardware support When are memory addresses bound? Compile/link time Compiler/Linker binds the addresses Must know â€œrunâ€ location at compile time Recompile if location changes Load time Compiler generates relocatable code Loader binds the addresses at load time Run time Logical compile-time addresses translated to physical addresses by special hardware. Hardware Support for Runtime Binding and Protection For process B to run using logical addresses Process B expects to access addresses from zero to some limit of memory size Need to add an appropriate offset to its logical addresses Achieve relocation Protect memory â€œlowerâ€ than B Must limit the maximum logical address B can generate Protect memory â€œhigherâ€ than B Base and Limit Register Also called Base and bound registers Relocation and limit registers Base and limit registers Restrict and relocate the currently active process Base and limit registers must be changed at Load time Relocation (compaction time) On a context switch Pro Supports protected multi-processing (-tasking) Cons Physical memory allocation must still be contiguous The entire process must be in memory Do not support partial sharing of address spaces No shared code, libraries, or data structures between processes Thus far, we have a system suitable for a batch system Limited number of dynamically allocated processes Enough to keep CPU utilised Relocated at runtime Protected from each other Swapping A process can be swapped temporarily out of memory to a backing store, and then brought back into memory for continued execution. Backing store â€“ fast disk large enough to accommodate copies of all memory images for all users; must provide direct access to these memory images. Can prioritize â€“ lower-priority process is swapped out so higher-priority process can be loaded and executed. Major part of swap time is transfer time; total transfer time is directly proportional to the amount of memory swapped. slow we assume a process is smaller than memory Virtual Memory â€“ Paging Overview Partition physical memory into small equal sized chunks Called frames Divide each processâ€™s virtual (logical) address space into same size chunks Called pages Virtual memory addresses consist of a page number and offset within the page OS maintains a page table contains the frame location for each page Used by hardware to translate each virtual address to physical address The relation between virtual addresses and physical memory addresses is given by page table Processâ€™s physical memory does not have to be contiguous No external fragmentation Small internal fragmentation (in last page) Allows sharing by mapping several pages to the same frame Abstracts physical organisation Programmer only deal with virtual addresses Minimal support for logical organisation Each unit is one or more pages Virtual MemoryMemory Management Unit(or TLB) Page-based VMVirtual Memory Divided into equalsized pages A mapping is a translation between A page and a frame A page and null Mappings defined at runtime â€“ They can change Address space can have holes Process does not have to be contiguous in physical memory Physical Memory Divided into equal-sized frames Typical Address Space Layout Stack region is at top, and can grow down Heap has free space to grow up Text is typically read-only Kernel is in a reserved, protected, shared region 0-th page typically not used Page Faults Referencing an invalid page triggers a page fault â€“ An exception handled by the OS Broadly, two standard page fault types Illegal Address (protection error) Signal or kill the process Page not resident Get an empty frame Load page from disk Update page (translation) table (enter frame #, set valid bit, etc.) Restart the faulting instruction Shared Pages Private code and data Each process has own copy of code and data Code and data can appear anywhere in the address space Shared code Single copy of code shared between all processes executing it Code must not be self modifying Code must appear at same address in all processes Page Table StructurePage table is (logically) an array of frame numbers Index by page number Each page-table entry (PTE) also has other bits Present/Absent bit Also called valid bit, it indicates a valid mapping for the page Modified bit Also called dirty bit, it indicates the page may have been modified in memory Reference bit Indicates the page has been accessed Protection bits Read permission, Write permission, Execute permission Or combinations of the above Caching bit Use to indicate processor should bypass the cache when accessing memory Example: to access device registers or memory Address TranslationEvery (virtual) memory address issued by the CPU must be translated to physical memory Every load and every store instruction Every instruction fetch Need Translation Hardware In paging system, translation involves replace page number with a frame number Page Tables Page tables are implemented as data structures in main memory Most processes do not use the full 4GB address space We need a compact representation that does not waste space Three basic schemes Use data structures that adapt to sparsity Use data structures which only represent resident pages Use VM techniques for page tables (details left to extended OS) Two-level Page Table2nd â€“level page tables representing unmapped pages are not allocated â€“ Null in the top-level page table Alternative: Inverted Page Table â€œInverted page tableâ€ is an array of page numbers sorted (indexed) by frame number (itâ€™s a frame table). Algorithm Compute hash of page number Extract index from hash table Use this to index into inverted page table Match the PID and page number in the IPT entry If match, use the index value as frame # for translation If no match, get next candidate IPT entry from chain field If NULL chain entry $\\Rightarrow$ page fault Properties of IPTs IPT grows with size of RAM, NOT virtual address space Frame table is needed anyway (for page replacement, more later) Need a separate data structure for non-resident pages Saves a vast amount of space (especially on 64-bit systems) Used in some IBM and HP workstations Improving the IPT: Hashed Page Table Retain fast lookup of IPT â€“ A single memory reference in best case Retain page table sized based on physical memory size (not virtual) Enable efficient frame sharing Support more than one mapping for same frame Sizing the Hashed Page Table HPT sized based on physical memory size With sharing Each frame can have more than one PTE More sharing increases number of slots used â€“ Increases collision likelihood However, we can tune HPT size based on: Physical memory size Expected sharing Hash collision avoidance. HPT a power of 2 multiple of number of physical memory frame VM Implementation IssuePerformanceEach virtual memory reference can cause two physical memory accesses One to fetch the page table entry One to fetch/store the data $\\Rightarrow$ Intolerable performance impact!! Solution: High-speed cache for page table entries (PTEs) Called a translation look-aside buffer (TLB) Contains recently used page table entries Associative, high-speed memory, similar to cache memory May be under OS control (unlike memory cache) TLB Translation Lookaside Buffer Given a virtual address, processor examines the TLB If matching PTE found (TLB hit), the address is translated Otherwise (TLB miss), the page number is used to index the processâ€™s page table If PT contains a valid entry, reload TLB and restart Otherwise, (page fault) check if page is on disk If on disk, swap it in Otherwise, allocate a new page or raise an exception TLB properties Page table is (logically) an array of frame numbers TLB holds a (recently used) subset of PT entries Each TLB entry must be identified (tagged) with the page # it translates Access is by associative lookup: All TLB entriesâ€™ tags are concurrently compared to the page # TLB is associative (or content-addressable) memory TLB may or may not be under direct OS control Hardware-loaded TLB On miss, hardware performs PT lookup and reloads TLB Example: x86, ARM Software-loaded TLB On miss, hardware generates a TLB miss exception, and exception handler reloads TLB Example: MIPS, Itanium (optionally) TLB size: typically 64-128 entries Can have separate TLBs for instruction fetch and data access TLBs can also be used with inverted page tables (and others) TLB and context switching TLB is a shared piece of hardware Normal page tables are per-process (address space) TLB entries are process-specific On context switch need to flush the TLB (invalidate all entries) high context-switching overhead (Intel x86) or tag entries with address-space ID (ASID) called a tagged TLB used (in some form) on all modern architectures TLB entry: ASID, page #, frame #, valid and write-protect bits TLB effect Without TLB Average number of physical memory references per virtual reference = 2 With TLB (assume 99% hit ratio) Average number of physical memory references per virtual reference = 0.99 * 1+ 0.01 * 2 = 1.01 Demand Paging/Segmentation With VM, only parts of the program image need to be resident in memory for execution Can transfer presently unused pages/segments to disk Reload non-resident pages/segment on demand. Reload is triggered by a page or segment fault Faulting process is blocked and another scheduled When page/segment is resident, faulting process is restarted May require freeing up memory first Replace current resident page/segment How determine replacement â€œvictimâ€? If victim is unmodified (â€œcleanâ€) can simply discard it This is reason for maintaining a â€œdirtyâ€ bit in the PT Why does demand paging/segmentation work? Program executes at full speed only when accessing the resident set. TLB misses introduce delays of several microseconds Page/segment faults introduce delays of several milliseconds Answer Less physical memory required per process Can fit more processes in memory Improved chance of finding a runnable one Principle of locality Principle of Locality An important observation comes from empirical studies of the properties of programs. Programs tend to reuse data and instructions they have used recently. 90/10 rule â€“ â€œA program spends 90% of its time in 10% of its codeâ€ We can exploit this locality of references An implication of locality is that we can reasonably predict what instructions and data a program will use in the near future based on its accesses in the recent past Two different types of locality have been observed: Temporal locality: states that recently accessed items are likely to be accessed in the near future Spatial locality: says that items whose addresses are near one another tend to be referenced close together in time Working Set The pages/segments required by an application in a time window ($\\Delta$)is called its memory working set. Working set is an approximation of a programsâ€™ locality if $\\Delta$ too small will not encompass entire locality if $\\Delta$ too large will encompass several localities if $\\Delta = \\infin\\Rightarrow$ will encompass entire program. $\\Delta$â€™s size is an application specific tradeoff System should keep resident at least a processâ€™s working set â€“ Process executes while it remains in its working set Working set tends to change gradually Get only a few page/segment faults during a time window Possible (but hard) to make intelligent guesses about which pieces will be needed in the future â€“ May be able to pre-fetch page/segments ThrashingCPU utilisation tends to increase with the degree of multiprogramming â€“ number of processes in system Higher degrees of multiprogramming â€“ less memory available per process Some processâ€™s working sets may no longer fit in RAM â€“ Implies an increasing page fault rate Eventually many processes have insufficient memory Canâ€™t always find a runnable process Decreasing CPU utilisation System become I/O limited This is called thrashing $ \\sum$working set sizes &gt; total physical memory size Recovery From ThrashingIn the presence of increasing page fault frequency and decreasing CPU utilisation Suspend a few processes to reduce degree of multiprogramming Resident pages of suspended processes will migrate to backing store More physical memory becomes available Resume suspended processes later when memory pressure eases VM Management PoliciesOperation and performance of VM system is dependent on a number of policies: Page table format (may be dictated by hardware) Multi-level Inverted/Hashed Page size (may be dictated by hardware) Fetch Policy Replacement policy Resident set size Minimum allocation Local versus global allocation Page cleaning policy Page SizeIncreasing page size âŒ Increases internal fragmentation â€“ ï‚§ reduces adaptability to working set size âœ… Decreases number of pages â€“ Reduces size of page tables âœ… Increases TLB coverage â€“ Reduces number of TLB misses âŒ Increases page fault latency â€“ Need to read more from disk before restarting process âœ… Increases swapping I/O throughput â€“ Small I/O are dominated by seek/rotation delays Optimal page size is a (work-load dependent) trade-off Multiple page sizes provide flexibility to optimise the use of the TLB Large page sizes can be use for code Small page size for thread stacks Most operating systems support only a single page size Dealing with multiple page sizes is hard! Fetch Policy Determines when a page should be brought into memory Demand paging only loads pages in response to page faults â€“ Many page faults when a process first starts Pre-paging brings in more pages than needed at the moment â€‹ âœ… Improves I/O performance by reading in larger chunks â€‹ âŒ Wastes I/O bandwidth if pre-fetched pages arenâ€™t used â€‹ âŒ Especially bad if we eject pages in working set in order to pre-fetch unused pages. â€‹ âŒ Hard to get right in practice Replacement Policy Page removed should be the page least likely to be references in the near future Most policies attempt to predict the future behaviour on the basis of past behaviour Constraint: locked frames Kernel code Main kernel data structure I/O buffers Performance-critical user-pages (e.g. for DBMS) Frame table has a lock (or pinned) bit Optimal Replacement policy Toss the page that wonâ€™t be used for the longest time âŒ Impossible to implement âŒ Only good as a theoretic reference point:The closer a practical algorithm gets to optimal, the better FIFO Replacement Policy First-in, first-out: Toss the oldest page âœ… Easy to implement âŒ Age of a page is isnâ€™t necessarily related to usage Least Recently Used (LRU) Toss the least recently used page Assumes that page that has not been referenced for a long time is unlikely to be referenced in the near future âšª Will work if locality holds âšª Implementation requires a time stamp to be kept for each page, updated on every reference âŒ Impossible to implement efficiently âšª Most practical algorithms are approximations of LRU Clock Page Replacement Clock policy, also called second chance. Employs a usage or reference bit in the frame table. Set to one when page is used While scanning for a victim, reset all the reference bits Toss the first page with a zero reference bit Issue How do we know when a page is referenced? Use the valid bit in the PTE: When a page is mapped (valid bit set), set the reference bit When resetting the reference bit, invalidate the PTE entry On page fault Turn on valid bit in PTE Turn on reference bit We thus simulate a reference bit in software PerformanceIt terms of selecting the most appropriate replacement, they rank as follows: Optimal LRU Clock FIFO Resident Set Size How many frames should each process have? Fixed Allocation Gives a process a fixed number of pages within which to execute. Isolates process memory usage from each other When a page fault occurs, one of the pages of that process must be replaced. âŒ Achieving high utilisation is an issue â€“ Some processes have high fault rate while others donâ€™t use their allocation. Variable Allocation Number of pages allocated to a process varies over the lifetime of the process Global Scope Operating system keeps global list of free frames Free frame is added to resident set of process when a page fault occurs If no free frame, replaces one from any process âœ… Easiest to implement âœ… Adopted by many operating systems âœ… Automatic balancing across system âŒ Does not provide guarantees for important activities Local Scope Allocate number of page frames to a new process based on Application type Program request Other criteria (priority) When a page fault occurs, select a page from among the resident set of the process that suffers the page fault Re-evaluate allocation from time to time! Cleaning PolicyObservation â€“ Clean pages are much cheaper to replace than dirty pages Demand cleaning A page is written out only when it has been selected for replacement High latency between the decision to replace and availability of free frame. Precleaning Pages are written out in batches (in the background, the pagedaemon) Increases likelihood of replacing clean frames Overlap I/O with current activity Multiprocessor SystemsAmdahlâ€™s lawGiven a proportion P of a program that can be made parallel, and the remaining serial portion (1-P), speedup by using N processors$\\frac{1}{(1-P)+\\frac{P}{N}}$ Types of Multiprocessors (MPs) UMA MP (Uniform Memory Access) Access to all memory occurs at the same speed for all processors. NUMA MP (Non-uniform memory access) Access to some parts of memory is faster for some processors than other parts of memory Bus Based UMA Simplest MP is more than one processor on a single bus connect to memory Bus bandwidth becomes a bottleneck with more than just a few CPUs Each processor has a cache to reduce its need for access to memory Hope is most accesses are to the local cache Bus bandwidth still becomes a bottleneck with many CPUs With only a single shared bus, scalability can be limited by the bus bandwidth of the single bus - Caching only helps so much Alternative bus architectures do exist. They improve bandwidth available Donâ€™t eliminate constraint that bandwidth is limited Multi-core Processor(Multi-core) share the same Bus interfaceMultiprocessors can Increase computation power beyond that available from a single CPU Share resources such as disk and memory However Assumes parallelizable workload to be effective Assumes not I/O bound Shared buses (bus bandwidth) limits scalability Can be reduced via hardware design Can be reduced by carefully crafted software behaviour Good cache locality together with limited data sharing where possible How do we construct an OS for a multiprocessor?Each CPU has its own OS? Statically allocate physical memory to each CPU Each CPU runs its own independent OS Share peripherals Each CPU (OS) handles its processes system calls Used in early multiprocessor systems to â€˜get them goingâ€™ Simpler to implement Avoids CPU-based concurrency issues by not sharing Scales â€“ no shared serial sections Modern analogy, virtualisation in the cloud. Issues Each processor has its own scheduling queue We can have one processor overloaded, and the rest idle Each processor has its own memory partition We can a one processor thrashing, and the others with free memory No way to move free memory from one OS to another Symmetric Multiprocessors (SMP) OS kernel run on all processors Load and resource are balance between all processors Including kernel execution Issue: Real concurrency in the kernel Need carefully applied synchronisation primitives to avoid disaster One alternative: A single mutex that make the entire kernel a large critical section Only one CPU can be in the kernel at a time The â€œbig lockâ€ becomes a bottleneck when in-kernel processing exceeds what can be done on a single CPU Better alternative: identify largely independent parts of the kernel and make each of them their own critical section Allows more parallelism in the kernel Issue: Difficult task Code is mostly similar to uniprocessor code Hard part is identifying independent parts that donâ€™t interfere with each other Remember all the inter-dependencies between OS subsystems. Lock contention can limit overall system performance Test-and-Set Hardware guarantees that the instruction executes atomically on a CPU.Atomically: As an indivisible unit.The instruction can not stop half way through Test-and-Set on SMPIt does not work without some extra hardware support A solution: Hardware blocks all other CPUs from accessing the bus during the TSL instruction to prevent memory accesses by any other CPU. TSL has mutually exclusive access to memory for duration of instruction. Test-and Set is a busy-wait synchronisation primitive * Called a spinlock Issue: Lock contention leads to spinning on the lock Spinning on a lock requires blocking the bus which slows all other CPUs down Independent of whether other CPUs need a lock or no Caching does not help reduce bus contention Either TSL still blocks the bus Or TSL requires exclusive access to an entry in the local cache Reducing Bus Contention Read before TSL Spin reading the lock variable waiting for it to change When it does, use TSL to acquire the lock Allows lock to be shared read-only in all caches until its released no bus traffic until actual release No race conditions, as acquisition is still with TSL. Test and set performs poorly once there is enough CPUs to cause contention for lock Expected Read before Test and Set performs better Performance less than expected Still significant contention on lock when CPUs notice release and all attempt acquisition Critical section performance degenerates Critical section requires bus traffic to modify shared structure Lock holder competes with CPU thatâ€™s waiting as they test and set, so the lock holder is slower Slower lock holder results in more contention Cache ConsistencyCache consistency is usually handled by the hardware. Spinning versus Blocking and Switching Spinning (busy-waiting) on a lock makes no sense on a uniprocessor The was no other running process to release the lock Blocking and (eventually) switching to the lock holder is the only sensible option. On SMP systems, the decision to spin or block is not as clear. The lock is held by another running CPU and will be freed without necessarily switching away from the requestor Spinning versus SwitchingBlocking and switching to another process takes time Save context and restore another Cache contains current process not new process Adjusting the cache working set also takes time TLB is similar to cache Switching back when the lock is free encounters the same again Spinning wastes CPU time directly Trade offIf lock is held for less time than the overhead of switching to and back â€‹ $\\Rightarrow$ Itâ€™s more efficient to spin $\\Rightarrow$ Spinlocks expect critical sections to be shortâ€‹ $\\Rightarrow$ No waiting for I/O within a spinlockâ€‹ $\\Rightarrow$ No nesting locks within a spinlock Preemption and Spinlocks Critical sections synchronised via spinlocks are expected to be short Avoid other CPUs wasting cycles spinning What happens if the spinlock holder is preempted at end of holderâ€™s timeslice Mutual exclusion is still guaranteed Other CPUs will spin until the holder is scheduled again!!!!! $\\Rightarrow$ Spinlock implementations disable interrupts in addition to acquiring locks to avoid lock-holder preemption Scheduling The scheduler decides who to run next. This process is sheduling Application behaviourBursts of CPU usage alternate with periods of I/O waita) CPU-Bound process Spends most of its computing Time to completion largely determined by received CPU time b) I/O-Bound process Spend most of its time waiting for I/O to complete Small bursts of CPU to process I/O and request next I/O Time to completion largely determined by I/O request time We need a mix of CPU-bound and I/O-bound processes to keep both CPU and I/O systems busy Process can go from CPU- to I/O-bound (or vice versa) in different phases of execution Key InsightsChoosing to run an I/O-bound process delays a CPU-bound process by very littleChoosing to run a CPU-bound process prior to an I/O-bound process delays the next I/O request significantly No overlap of I/O waiting with computation Results in device (disk) not as busy as possible Generally, favour I/O-bound processes over CPU-bound processes Generally, a scheduling decision is required when a process (or thread) can no longer continue, or when an activity results in more than one ready process Preemptive versus Non-preemptive SchedulingNon-preemptive Once a thread is in the running state, it continues until it completes, blocks on I/O, or voluntarily yields the CPU A single process can monopolised the entire systemPreemptive Scheduling (responsive system) Current thread can be interrupted by OS and moved to ready state. Usually after a timer interrupt and process has exceeded its maximum run time Can also be as a result of higher priority process that has become ready (after I/O interrupt). Ensures fairer service as single thread canâ€™t monopolise the system Requires a timer interrupt Categories of Scheduling AlgorithmsBatch Systems No users directly waiting, can optimise for overall machine performance Interactive Systems Users directly waiting for their results, can optimise for users perceived performance Realtime Systems Jobs have deadlines, must schedule such that all jobs (predictably) meet their deadlines Goals of Scheduling AlgorithmsAll Algorithms Fairness Give each process a fair share of the CPU Policy Enforcement What ever policy chosen, the scheduler should ensure it is carried out Balance/Efficiency Try to keep all parts of the system busy Interactive Algorithms Minimise response time Response time is the time difference between issuing a command and getting the result E.g selecting a menu, and getting the result of that selection Response time is important to the userâ€™s perception of the performance of the system. Provide Proportionality Proportionality is the user expectation that short jobs will have a short response time, and long jobs can have a long response time. Generally, favour short jobs Real-time Algorithms Must meet deadlines Each job/task has a deadline. A missed deadline can result in data loss or catastrophic failure Aircraft control system missed deadline to apply brakes Provide Predictability For some apps, an occasional missed deadline is okay â€“ E.g. DVD decoder Predictable behaviour allows smooth DVD decoding with only rare skips Interactive schedulingRound Robin Scheduling Each process is given a timeslice to run in When the timeslice expires, the next process preempts the current process, and runs for its timeslice, and so on The preempted process is placed at the end of the queue Implemented with A ready queue A regular timer interrupt Pros â€“ Fair, easy to implementCon â€“ Assumes everybody is equal What should the timeslice be? Too short Waste a lot of time switching between processes Example: timeslice of 4ms with 1 ms context switch = 20% round robin overhead Too long System is not responsive Example: timeslice of 100ms â€“ If 10 people hit â€œenterâ€ key simultaneously, the last guy to run will only see progress after 1 second. Degenerates into FCFS if timeslice longer than burst length Priorities Each Process (or thread) is associated with a priority Provides basic mechanism to influence a scheduler decision: Scheduler will always chooses a thread of higher priority over lower priority Priorities can be defined internally or externally Internal: e.g. I/O bound or CPU bound External: e.g. based on importance to the user Usually implemented by multiple priority queues, with round robin on each queue Con Low priorities can starve Need to adapt priorities periodically Based on ageing or execution history Traditional UNIX SchedulerTwo-level scheduler High-level scheduler schedules processes between memory and disk Low-level scheduler is CPU scheduler Based on a multilevel queue structure with round robin at each level The highest priority (lower number) is scheduled Priorities are re-calculated once per second, and re-inserted in appropriate queue Avoid starvation of low priority threads Penalise CPU-bound threads Priority = CPU_usage +nice +base CPU_usage = number of clock ticks Nice is a value given to the process by a user to permanently boost or reduce its priority ( Reduce priority of background jobs) Base is a set of hardwired, negative values used to boost priority of I/O bound system activities Single Shared Ready QueuePros Simple Automatic load balancing Cons Lock contention on the ready queue can be a major bottleneck Due to frequent scheduling or many CPUs or both Not all CPUs are equal The last CPU a process ran on is likely to have more related entries in the cache Affinity SchedulingBasic Idea â€“ Try hard to run a process on the CPU it ran on last time One approach: Multiple Queue Multiprocessor Scheduling Multiple Queue SMP Scheduling Each CPU has its own ready queue Coarse-grained algorithm assigns processes to CPUs Defines their affinity, and roughly balances the load The bottom-level fine-grained scheduler: Is the frequently invoked scheduler (e.g. on blocking on I/O, a lock, or exhausting a timeslice) Runs on each CPU and selects from its own ready queue Ensures affinity If nothing is available from the local ready queue, it runs a process from another CPUs ready queue rather than go idle Termed â€œWork stealingâ€ âœ… No lock contention on per-CPU ready queues in the (hopefully) common caseâœ… Load balancing to avoid idle queuesâœ… Automatic affinity to a single CPU for more cache friendly behaviour I/O ManagementI/O DevicesThere exists a large variety of I/O devices: Many of them with different properties They seem to require different interfaces to manipulate and manage them We donâ€™t want a new interface for every device Diverse, but similar interfaces leads to code duplication Challenge: Uniform and efficient approach to I/O Device DriversDrivers classified into similar categories Block devices and character (stream of data) device OS defines a standard (internal) interface to the different classes of devices Example: USB Human Input Device (HID) class specifications human input devices follow a set of rules making it easier to design a standard interface. Device drivers job translate request through the device-independent standard interface (open, close, read, write) into appropriate sequence of commands (register manipulations) for the particular hardware Initialise the hardware at boot time, and shut it down cleanly at shutdown After issuing the command to the device, the device either Completes immediately and the driver simply returns to the caller Or, device must process the request and the driver usually blocks waiting for an I/O complete interrupt. Drivers are thread-safe as they can be called by another process while a process is already blocked in the driver Thead-safe: Synchronisedâ€¦ Device-Independent I/O SoftwareThere is commonality between drivers of similar classes Divide I/O software into device-dependent and device-independent I/O software Device independent software includes Buffer or Buffer-cache management TCP/IP stack Managing access to dedicated devices Error reporting I/O Device Handling Data rate May be differences of several orders of magnitude between the data transfer rates Driver $\\Leftrightarrow$ Kernel InterfaceMajor Issue is uniform interfaces to devices and kernel Uniform device interface for kernel code Allows different devices to be used the same way Allows internal changes to device driver with fear of breaking kernel code Uniform kernel interface for device code Drivers use a defined interface to kernel services (e.g. kmalloc, install IRQ handler, etc.) Allows kernel to evolve without breaking existing drivers Together both uniform interfaces avoid a lot of programming implementing new interfaces Retains compatibility as drivers and kernels change over time. Interrupts Devices connected to an Interrupt Controller via lines on an I/O bus (e.g. PCI) Interrupt Controller signals interrupt to CPU and is eventually acknowledged. Exact details are architecture specific. I/O InteractionProgrammed I/O Also called polling, or busy waiting I/O module (controller) performs the action, not the processor Sets appropriate bits in the I/O status register No interrupts occur Processor checks status until operation is complete Wastes CPU cycles Interrupt-Driven I/O Processor is interrupted when I/O module (controller) ready to exchange data Processor is free to do other work No needless waiting Consumes a lot of processor time because every word read or written passes through the processor Direct Memory Access (DMA) Transfers data directly between Memory and Device CPU not needed for copying Transfers a block of data directly to or from memory An interrupt is sent when the task is complete The processor is only involved at the beginning and end of the transfer DMA Considerationsâœ… Reduces number of interrupts Less (expensive) context switches or kernel entry-exits ïƒ» âŒ Requires contiguous regions (buffers) Copying Some hardware supports â€œScatter-gatherâ€ Synchronous/Asynchronous Shared bus must be arbitrated (hardware) â€“ CPU cache reduces (but not eliminates) CPU need for bus I/O Management SoftwareI/O Software Layers Interrupt Handlers Interrupt handlers Can execute at (almost) any time Raise (complex) concurrency issues in the kernel Can propagate to userspace (signals, upcalls), causing similar issues Generally structured so I/O operations block until interrupts notify them of completion kern/dev/lamebus/lhd.c Interrupt Handler Steps Save Registers not already saved by hardware interrupt mechanism (Optionally) set up context for interrupt service procedure Typically, handler runs in the context of the currently running process No expensive context switch Set up stack for interrupt service procedure Handler usually runs on the kernel stack of current process Or â€œnestsâ€ if already in kernel mode running on kernel stack Ack/Mask interrupt controller, re-enable other interrupts Implies potential for interrupt nesting. Run interrupt service procedure Acknowledges interrupt at device level Figures out what caused the interrupt Received a network packet, disk read finished, UART transmit queue empty If needed, it signals blocked device driver In some cases, will have woken up a higher priority blocked thread Choose newly woken thread to schedule next. Set up MMU context for process to run next What if we are nested? Load new/original processâ€™ registers Re-enable interrupt; Start running the new process Sleeping in Interrupts An interrupt generally has no context (runs on current kernel stack) Unfair to sleep on interrupted process (deadlock possible) Where to get context for long running operation? What goes into the ready queue? What to do? Top and Bottom Half Linux implements with tasklets and workqueues Generically, in-kernel thread(s) handle long running kernel operations. Top/Half Bottom Half Top Half Interrupt handler remains short Bottom half Is preemptable by top half (interrupts) performs deferred work (e.g. IP stack processing) Is checked prior to every kernel exit signals blocked processes/threads to continue Enables low interrupt latency Bottom half canâ€™t block Deferring Work on In-kernel Threads Interrupt handler defers work onto in-kernel thread In-kernel thread handles deferred work (DW) Scheduled normally Can block Both low interrupt latency and blocking operations BufferingNo Buffering Process must read/write a device a byte/word at a time Each individual system call adds significant overhead Process must what until each I/O is complete Blocking/interrupt/waking adds to overhead. Many short runs of a process is inefficient (poor CPU cache temporal locality) User-level Buffering Process specifies a memory buffer that incoming data is placed in until it fills Filling can be done by interrupt service routine Only a single system call, and block/wakeup per data buffer Much more efficient Issues What happens if buffer is paged out to disk Could lose data while unavailable buffer is paged in Could lock buffer in memory (needed for DMA), however many processes doing I/O reduce RAM available for paging. Can cause deadlock as RAM is limited resource Consider write case When is buffer available for re-use? Either process must block until potential slow device drains buffer or deal with asynchronous signals indicating buffer drained Single Buffer Operating system assigns a buffer in kernelâ€™s memory for an I/O request In a stream-oriented scenario Used a line at time User input from a terminal is one line at a time with carriage return signaling the end of the line Output to the terminal is one line at a time Block-oriented Input transfers made to buffer Block copied to user space when needed Another block is written into the buffer Read ahead User process can process one block of data while next block is read in Swapping can occur since input is taking place in system memory, not user memory Operating system keeps track of assignment of system buffers to user processes Assume T is transfer time for a block from device C is computation time to process incoming block M is time to copy kernel buffer to user buffer Computation and transfer can be done in parallel Speed up with buffering What happens if kernel buffer is full the user buffer is swapped out, or The application is slow to process previous buffer and more data is received??? =&gt; We start to lose characters or drop network packets Double Buffer Use two system buffers instead of one A process can transfer data to or from one buffer while the operating system empties or fills the other buffer Double Buffer Speed Up Computation and Memory copy can be done in parallel with transfer Speed up with double buffering Usually M is much less than T giving a favourable result May be insufficient for really bursty traffic Lots of application writes between long periods of computation Long periods of application computation while receiving data Might want to read-ahead more than a single block for disk Circular Buffer More than two buffers are used Each individual buffer is one unit in a circular buffer Used when I/O operation must keep up with process Notice that buffering, double buffering, and circular buffering are all Bounded-Buffer Producer-Consumer Problems Reference:UNSW COMP3231 2020 lecture slides","link":"/2020/08/11/OperatingSystems/"},{"title":"Haskell and functional programming","text":"Enjoy learning haskell but not only haskell. Haskell IntroductionIn this course we use Haskell, because it is the most widespread language with good support for mathematically structured programming. 12f :: Int -&gt; Boolf x = (x &gt; 0) First $x$ is the input and the RHS of the equation is the Output. CurryingIn mathematics, we treat $log_{10}(x)$ and $log_2 (x)$ and ln(x) as separate functions. In Haskell, we have a single function logBase that, given a number n, produces a function for $log_n(x)$ 12345678910log10 :: Double -&gt; Doublelog10 = logBase 10log2 :: Double -&gt; Doublelog2 = logBase 2ln :: Double -&gt; Doubleln = logBase 2.71828logBase :: Double -&gt; Double -&gt; Double Function application associates to the left in Haskell, so: logBase 2 64 â‰¡ (logBase 2) 64 Functions of more than one argument are usually written this way in Haskell, but it is possible to use tuples insteadâ€¦ TuplesTuples are another way to take multiple inputs or produce multiple outputs: 1234toCartesian :: (Double, Double) -&gt; (Double, Double)toCartesian (r, theta) = (x, y) where x = r * cos theta y = r * sin theta N.B: The order of bindings doesnâ€™t matter. Haskell functions have no side effects, they just return a result. Thereâ€™s no notion of time(no notion of sth happen before sth else). Higher Order FunctionsIn addition to returning functions, functions can take other functions as arguments: 12345678twice :: (a -&gt; a) -&gt; (a -&gt; a)twice f a = f (f a)double :: Int -&gt; Intdouble x = x * 2quadruple :: Int -&gt; Intquadruple = twice double Haskell concrete types are written in upper case like Int, Bool, and in lower case if they stand for any type. 12345678{- twice twice double 3 == (twice twice double) 3 == (twice (twice double)) 3 == (twice quadruple) 3 == quadrauple (quadruple 3) == 48-} ListsHaskell makes extensive use of lists, constructed using square brackets. Each list element must be of the same type. 1234[True, False, True] :: [Bool][3, 2, 5+1] :: [Int][sin, cos] :: [Double -&gt; Double][ (3,â€™aâ€™),(4,â€™bâ€™) ] :: [(Int, Char)] MapA useful function is map, which, given a function, applies it to each element of a list: 123map not [True, False, True] = [False, True, False]map negate [3, -2, 4] = [-3, 2, -4]map (\\x -&gt; x + 1) [1, 2, 3] = [2, 3, 4] The last example here uses a lambda expression to define a one-use function without giving it a name. Whatâ€™s the type of map? 1map :: (a -&gt; b) -&gt; [a] -&gt; [b] StringsThe type String in Haskell is just a list of characters: 1type String = [Char] This is a type synonym, like a typedef in C. Thus: &quot;hi!&quot; == ['h', 'i', '!'] Practice Word Frequencies Given a number $n$ and a string $s$, generate a report (in String form) that lists the $n$ most common words in the string $s$. We must: Break the input string into words. Convert the words to lowercase. Sort the words. Count adjacent runs of the same word. Sort by size of the run. Take the first $n$ runs in the sorted list. Generate a report. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import Data.Char(toLower)import Data.List(group,sort,sortBy)breakIntoWords :: String -&gt; [String]breakIntoWords = wordsconvertIntoLowercase :: [[Char]] -&gt; [String]convertIntoLowercase = map (map toLower)sortWords :: [String] -&gt; [String]sortWords = sorttype Run = (Int, String)countAdjacentRuns :: [String] -&gt; [Run]countAdjacentRuns = convertToRuns . groupAdjacentRuns -- [\"hello\",\"hello\",\"world\"] --&gt; [[\"hello\",\"hello\"],[\"world\"]]groupAdjacentRuns :: [String] -&gt; [[String]]groupAdjacentRuns = group-- head :: [a] -&gt; aconvertToRuns :: [[String]] -&gt; [Run]convertToRuns = map (\\ls-&gt; (length ls, head ls))sortByRunSize :: [Run] -&gt; [Run]sortByRunSize = sortBy (\\(l1, w1) (l2, w2) -&gt; compare l2 l1)takeFirst :: Int -&gt; [Run] -&gt; [Run]takeFirst = take generateReport :: [Run] -&gt; StringgenerateReport = unlines . map (\\(l,w) -&gt; w ++ \":\" ++ show l ) -- (\\x -&gt; f x) == fmostCommonWords :: Int -&gt; (String -&gt; String)mostCommonWords n = generateReport . takeFirst n . sortByRunSize . countAdjacentRuns . sortWords . convertIntoLowercase . breakIntoWords Functional CompositionWe used function composition to combine our functions together. The mathematical $(f â—¦ g)(x)$ is written $(f . g) x$ in Haskell. In Haskell, operators like function composition are themselves functions. You can define your own! 1234-- Vector addition(.+) :: (Int, Int) -&gt; (Int, Int) -&gt; (Int, Int)(x1, y1) .+ (x2, y2) = (x1 + x2, y1 + y2)(2,3) .+ (1,1) == (3,4) You could even have defined function composition yourself if it didnâ€™t already exist: 12(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)(f . g) x = f (g x) ListsHow were all of those list functions we just used implemented? Lists are singly-linked lists in Haskell. The empty list is written as [] and a list node is written as x : xs. The value x is called the head and the rest of the list xs is called the tail. Thus: 12\"hi!\" == ['h', 'i', '!'] == 'h':('i':('!':[])) == 'h' : 'i' : '!' : [] When we define recursive functions on lists, we use the last form for pattern matching: 123map :: (a -&gt; b) -&gt; [a] -&gt; [b]map f [] = []map f (x:xs) = f x : map f xs We can evaluate programs equationally: 12345678910111213{-map toUpper \"hi!\" â‰¡ map toUpper (â€™hâ€™:\"i!\") â‰¡ toUpper â€™hâ€™ : map toUpper \"i!\" â‰¡ â€™Hâ€™ : map toUpper \"i!\" â‰¡ â€™Hâ€™ : map toUpper (â€™iâ€™:\"!\") â‰¡ â€™Hâ€™ : toUpper â€™iâ€™ : map toUpper \"!\" â‰¡ â€™Hâ€™ : â€™Iâ€™ : map toUpper \"!\" â‰¡ â€™Hâ€™ : â€™Iâ€™ : map toUpper (â€™!â€™:\"\") â‰¡ â€™Hâ€™ : â€™Iâ€™ : â€™!â€™ : map toUpper \"\" â‰¡ â€™Hâ€™ : â€™Iâ€™ : â€™!â€™ : map toUpper [] â‰¡ â€™Hâ€™ : â€™Iâ€™ : â€™!â€™ : [] â‰¡ \"HI!\"-} List Functions1234567891011121314151617181920212223242526272829303132333435-- in maths: f(g(x)) == (f o g)(x)myMap :: (a -&gt; b) -&gt; [a] -&gt; [b]myMap f [] = []myMap f (x:xs) = (f x) : (myMap f xs)-- 1 : 2 : 3 : []-- 1 + 2 + 3 + 0sum' :: [Int] -&gt; Intsum' [] = 0sum' (x:xs) = x + sum xs-- [\"hello\",\"world\",\"!\"] -&gt; \"helloworld!\"-- \"hello\":\"world\":\"!\":[]-- \"hello\"++\"world\"++\"!\"++[]concat' :: [[a]] -&gt; [a]concat' [] = []concat' (xs:xss) = xs ++ concat xssfoldr' :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; bfoldr' f z [] = zfoldr' f z (x:xs) = x `f` (foldr' f z xs)sum'' = foldr' (+) 0concat'' = foldr' (++) []filter' :: (a -&gt; Bool) -&gt; [a] -&gt; [a]filter' p [] = []-- filter' p (x:xs) = if p x then x : filter' p xs -- else filter' p xsfilter' p (x:xs) | p x = x : filter' p xs | otherwise = filter' p xs InductionSuppose we want to prove that a property $P(n)$ holds for all natural numbers $n$. Remember that the set of natural numbers $N$ can be defined as follows: Definition of Natural Numbers Inductive definition of Natural numbers 0 is a natural number. For any natural number $n, n + 1$ is also a natural number Therefore, to show $P(n)$ for all $n$, it suffices to show: $P(0)$ (the base case), and assuming $P(k)$ (the inductive hypothesis), $â‡’ P(k + 1)$ (the inductive case). Induction on Lists Haskell lists can be defined similarly to natural numbers Definition of Haskell Lists [] is a list. For any list xs, x:xs is also a list (for any item x). This means, if we want to prove that a property P(ls) holds for all lists ls, it suffices to show: P([]) (the base case) P(x:xs) for all items x, assuming the inductive hypothesis P(xs) Data TypesSo far, we have seen type synonyms using the type keyword. For a graphics library, we might define: 123456type Point = (Float, Float)type Vector = (Float, Float)type Line = (Point, Point)type Colour = (Int, Int, Int, Int) -- RGBAmovePoint :: Point -&gt; Vector -&gt; PointmovePoint (x,y) (dx,dy) = (x + dx, y + dy) But these definitions allow Points and Vectors to be used interchangeably, increasing the likelihood of errors. We can define our own compound types using the data keyword: 12data Point = Point Float Floatderiving (Show, Eq) First Point is the type name Second Point is Constructor name Floats are Constructor argument types 12345data Vector = Vector Float Floatderiving (Show, Eq)movePoint :: Point -&gt; Vector -&gt; PointmovePoint (Point x y) (Vector dx dy)= Point (x + dx) (y + dy) RecordsWe could define Colour similarly: 1data Colour = Colour Int Int Int Int But this has so many parameters, itâ€™s hard to tell which is which. Haskell lets us declare these types as records, which is identical to the declaration style on the previous slide, but also gives us projection functions and record syntax: 12345data Colour = Colour { redC :: Int , greenC :: Int , blueC :: Int , opacityC :: Int } deriving (Show, Eq) Here, the code redC (Colour 255 128 0 255) gives 255. Enumeration TypesSimilar to enums in C and Java, we can define types to have one of a set of predefined values: 123456data LineStyle = Solid | Dashed | Dotted deriving (Show, Eq)data FillStyle = SolidFill | NoFill deriving (Show, Eq) Types with more than one constructor are called sum types Algebraic Data TypesJust as the Point constructor took two Float arguments, constructors for sum types can take parameters too, allowing us to model different kinds of shape: 12345678data PictureObject = Path [Point] Colour LineStyle | Circle Point Float Colour LineStyle FillStyle | Polygon [Point] Colour LineStyle FillStyle | Ellipse Point Float Float Float Colour LineStyle FillStyle deriving (Show, Eq)type Picture = [PictureObject] Recursive and Parametric TypesData types can also be defined with parameters, such as the well known Maybe type, defined in the standard library: 1data Maybe a = Just a | Nothing Types can also be recursive. If lists werenâ€™t already defined in the standard library, we could define them ourselves: 1data List a = Nil | Cons a (List a) We can even define natural numbers, where 2 is encoded as Succ(Succ Zero): 1data Natural = Zero | Succ Natural Types in Design Make illegal states unrepresentable. â€‹ â€“ Yaron Minsky (of Jane Street) Choose types that constrain your implementation as much as possible. Then failure scenarios are eliminated automatically. Partial Functions A partial function is a function not defined for all possible inputs. Partial functions are to be avoided, because they cause your program to crash if undefined cases are encountered. To eliminate partiality, we must either: enlarge the codomain, usually with a Maybe type: 123safeHead :: [a] -&gt; Maybe a safeHead (x:xs) = Just xsafeHead [] = Nothing constrain the domain to be more specific: 1234safeHead' :: NonEmpty a -&gt; asafeHead' (One a) = asafeHead' (Cons a _) = adata NonEmpty a = One a | Cons a (NonEmpty a) Type ClassesYou have already seen functions such as: compare, (==), (+), (show) that work on multiple types, and their corresponding constraints on type variables Ord, Eq, Num and Show. These constraints are called type classes, and can be thought of as a set of types for which certain operations are implemented. Show The Show type class is a set of types that can be converted to strings. 12class Show a where -- nothing to do with OOP show :: a -&gt; String Types are added to the type class as an instance like so: 123instance Show Bool where show True = \"True\" show False = \"False\" We can also define instances that depend on other instances: 123instance Show a =&gt; Show (Maybe a) where show (Just x) = \"Just \" ++ show x show Nothing = \"Nothing\" Fortunately for us, Haskell supports automatically deriving instances for some classes, including Show. Read Type classes can also overload based on the type returned. 12class Read a where read :: String -&gt; a Semigroup A semigroup is a pair of a set S and an operation â€¢ : S â†’ S â†’ S where the operation s associative. Associativity is defined as, for all a, b, c: (a â€¢ (b â€¢ c)) = ((a â€¢ b) â€¢ c) Haskell has a type class for semigroups! The associativity law is enforced only by programmer discipline: 123class Semigroup s where(&lt;&gt;) :: s -&gt; s -&gt; s-- Law: (&lt;&gt;) must be associative. What instances can you think of? Lists &amp; ++, numbers and +, numbers and * Example: Lets implement additive colour mixing: 12345678instance Semigroup Colour whereColour r1 g1 b1 a1 &lt;&gt; Colour r2 g2 b2 a2 = Colour (mix r1 r2) (mix g1 g2) (mix b1 b2) (mix a1 a2)where mix x1 x2 = min 255 (x1 + x2) Observe that associativity is satisfied. Moniod A monoid is a semigroup (S, â€¢) equipped with a special identity element z : S such that x â€¢ z = x and z â€¢ y = y for all x, y. 12345class (Semigroup a) =&gt; Monoid a where mempty :: aFor colours, the identity element is transparent black:instance Monoid Colour where mempty = Colour 0 0 0 0 For each of the semigroups discussed previously(lists, num and +, num and *): Are they monoids? Yes If so, what is the identity element? [], 0, 1 Are there any semigroups that are not monoids? Maximum NewtypesThere are multiple possible monoid instances for numeric types like Integer: The operation (+) is associative, with identity element 0 The operation (*) is associative, with identity element 1 Haskell doesnâ€™t use any of these, because there can be only one instance per type per class in the entire program (including all dependencies and libraries used). A common technique is to define a separate type that is represented identically to the original type, but can have its own, different type class instances. In Haskell, this is done with the newtype keyword. A newtype declaration is much like a data declaration except that there can be only one constructor and it must take exactly one argument: 12345newtype Score = S Integerinstance Semigroup Score where S x &lt;&gt; S y = S (x + y)instance Monoid Score where mempty = S 0 Here, Score is represented identically to Integer, and thus no performance penalty is incurred to convert between them. In general, newtypes are a great way to prevent mistakes. Use them frequently! Ord Ord is a type class for inequality comparison 12class Ord a where (&lt;=) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x &lt;= x Transitivity: If x &lt;= y and y &lt;= z then x &lt;= z Antisymmetry: If x &lt;= y and y &lt;= x then x == y. Totality: Either x &lt;= y or y &lt;= x Relations that satisfy these four properties are called total orders(most are total orders). Without the fourth (totality), they are called partial orders(e.g. division). Eq Eq is a type class for equality or equivalence 12class Eq a where (==) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x == x. Transitivity: If x == y and y == z then x == z. Symmetry: If x == y then y == x. Relations that satisfy these are called equivalence relations. Some argue that the Eq class should be only for equality, requiring stricter laws like: If x == y then f x == f y for all functions f But this is debated. Funtors Types and ValuesHaskell is actually comprised of two languages. The value-level language, consisting of expressions such as if, let, 3 etc. The type-level language, consisting of types Int, Bool, synonyms like String, and type constructors like Maybe, (-&gt;), [ ] etc This type level language itself has a type system! KindsJust as terms in the value level language are given types, terms in the type level language are given kinds. The most basic kind is written as *. Types such as Int and Bool have kind *. Seeing as Maybe is parameterised by one argument, Maybe has kind * -&gt; *: given a type (e.g. Int), it will return a type (Maybe Int). ListsSuppose we have a function: 1toString :: Int -&gt; String And we also have a function to give us some numbers: 1getNumbers :: Seed -&gt; [Int] How can I compose toString with getNumbers to get a function f of type Seed -&gt; [String]? we use map: f = map toString . getNumbers. What about return a Maybe Int? we can use maybe map 1234567maybeMap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe bmaybeMap f Nothing = NothingmaybeMap f (Just x) = Just (f x)maybeMap f mx = case mx of Nothing -&gt; Nothing Just x -&gt; Just (f x) We can generalise this using functor. FunctorAll of these functions are in the interface of a single type class, called Functor. 12class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Unlike previous type classes weâ€™ve seen like Ord and Semigroup, Functor is over types of kind * -&gt; *. 1234567891011121314151617181920-- Instance for tuples-- type level:-- (,) :: * -&gt; (* -&gt; *)-- (,) x :: * -&gt; *instance Functor ((,) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (,) x a -&gt; (,) x b-- fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x, b) fmap f (x,a) = (x, f a) -- instance for functions-- type level:-- (-&gt;) :: * -&gt; (* -&gt; *)-- (-&gt;) x :: * -&gt; *instance Functor ((-&gt;) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (-&gt;) x a -&gt; (-&gt;) x b-- fmap :: (a -&gt; b) -&gt; (x -&gt; a) -&gt; (x -&gt; b) fmap = (.) Functor LawsThe functor type class must obey two laws: fmap id == id(indentity law) fmap f . fmap g == fmap (f . g)(composition law, haskell use this law to do optimisation) In Haskellâ€™s type system itâ€™s impossible to make a total fmap function that satisfies the first law but violates the second. This is due to parametricity. Property Based TestingFree PropertiesHaskell already ensures certain properties automatically with its language design and type system. Memory is accessed where and when it is safe and permitted to be accessed (memory safety). Values of a certain static type will actually have that type at run time. Programs that are well-typed will not lead to undefined behaviour (type safety). All functions are pure: Programs wonâ€™t have side effects not declared in the type. (purely functional programming) â‡’ Most of our properties focus on the logic of our program. Logical PropertiesWe have already seen a few examples of logical properties. Example: reverse is an involution: reverse (reverse xs) == xs right identity for (++): xs ++ [] == xs transitivity of (&gt;): (a &gt; b) âˆ§ (b &gt; c) â‡’ (a &gt; c) The set of properties that capture all of our requirements for our program is called the functional correctness specification of our software. This defines what it means for software to be correct. ProofsLast week we saw some proof methods for Haskell programs. We could prove that our implementation meets its functional correctness specification. Such proofs certainly offer a high degree of assurance, but: Proofs must make some assumptions about the environment and the semantics of the software. Proof complexity grows with implementation complexity, sometimes drastically. If software is incorrect, a proof attempt might simply become stuck: we do not always get constructive negative feedback. Proofs can be labour and time intensive ($$$), or require highly specialised knowledge ($$$). TestingCompared to proofs: Tests typically run the actual program, so requires fewer assumptions about the language semantics or operating environment. Test complexity does not grow with implementation complexity, so long as the specification is unchanged. Incorrect software when tested leads to immediate, debuggable counter examples. Testing is typically cheaper and faster than proving. Tests care about efficiency and computability, unlike proofs(e.g. termination is provable but not computable). We lose some assurance, but gain some convenience ($$$). Property Based Testing Key idea: Generate random input values, and test properties by running them. Example(QuickCheck Property) 1234567891011121314151617181920import Test.QuickCheckimport Data.Charimport Data.List-- Testable -- Arbitrary Testable-- Arbitrary Testableprop_reverseApp :: [Int] -&gt; ([Int] -&gt; Bool)prop_reverseApp xs ys = reverse (xs ++ ys) == reverse ys ++ reverse xsdivisible :: Int -&gt; Int -&gt; Booldivisible x y = x `mod` y == 0-- or select different generators with modifier newtypes.prop_refl :: Positive Int -&gt; Bool prop_refl (Positive x) = divisible x x-- Encode pre-conditions with the (==&gt;) operator:prop_unwordsWords s = unwords (words s) == sprop_wordsUnwords l = all (\\w -&gt; all (not . isSpace) w &amp;&amp; w /= []) l ==&gt; words (unwords l) == l PBT vs. Unit Testing Properties are more compact than unit tests, and describe more cases. â‡’ Less testing code Property-based testing heavily depends on test data generation: Random inputs may not be as informative as hand-crafted inputs â‡’ use shrinking(When a test fails, it finds the smallest test case still falls) Random inputs may not cover all necessary corner cases: â‡’ use a coverage checker Random inputs must be generated for user-defined types: â‡’ QuickCheck includes functions to build custom generators By increasing the number of random inputs, we improve code coverage in PBT. Test Data GenerationData which can be generated randomly is represented by the following type class: 123class Arbitrary a where arbitrary :: Gen a -- more on this later shrink :: a -&gt; [a] Most of the types we have seen so far implement Arbitrary. Shrinking The shrink function is for when test cases fail. If a given input x fails, QuickCheck will try all inputs in shrink x; repeating the process until the smallest possible input is found Testable TypesThe type of the quickCheck function is: 12-- more on IO laterquickCheck :: (Testable a) =&gt; a -&gt; IO () The Testable type class is the class of things that can be converted into properties. This includes: Bool values QuickCheckâ€™s built-in Property type Any function from an Arbitrary input to a Testable output: 12instance (Arbitrary i, Testable o) =&gt; Testable (i -&gt; o) ... Thus the type [Int] -&gt; [Int] -&gt; Bool (as used earlier) is Testable. Examples12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455split :: [a] -&gt; ([a],[a])split [] = ([],[])split [a] = ([a],[])split (x:y:xs) = let (l,r) = split xs in (x:l,y:r)prop_splitPerm xs = let (l,r) = split (xs :: [Int]) in permutation xs (l ++ r)permutation :: (Ord a) =&gt; [a] -&gt; [a] -&gt; Boolpermutation xs ys = sort xs == sort yspermutation' :: (Eq a) =&gt; [a] -&gt; [a] -&gt; (a -&gt; Bool)permutation' xs ys = \\x -&gt; count x xs == count x ys where count x l = length (filter (== x) l)merge :: (Ord a) =&gt; [a] -&gt; [a] -&gt; [a]merge [] ys = ysmerge xs [] = xsmerge (x:xs) (y:ys) | x &lt;= y = x : merge xs (y:ys) | otherwise = y : merge (x:xs) ysprop_mergePerm xs ys = permutation (xs ++ (ys :: [Int] )) (merge xs ys)prop_mergeSorted (Ordered xs) (Ordered ys) = sorted (merge (xs :: [Int]) ys)sorted :: Ord a =&gt; [a] -&gt; Boolsorted [] = True sorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)mergeSort :: (Ord a) =&gt; [a] -&gt; [a]mergeSort [] = []mergeSort [x] = [x]mergeSort xs = let (l,r) = split xs in merge (mergeSort l) (mergeSort r)prop_mergeSortSorts xs = sorted (mergeSort (xs :: [Int])) prop_mergeSortPerm xs = permutation xs (mergeSort (xs :: [Int]))prop_mergeSortExtra xs = mergeSort (xs :: [Int]) == sort xsprop_mergeSortUnit = mergeSort [3,2,1] == [1,2,3]main = do quickCheck prop_mergeSortUnit quickCheck prop_mergeSortSorts quickCheck prop_mergeSortPerm Redundant PropertiesSome properties are technically redundant (i.e. implied by other properties in the specification), but there is some value in testing them anyway: They may be more efficient than full functional correctness tests, consuming less computing resources to test. They may be more fine-grained to give better test coverage than random inputs for full functional correctness tests. They provide a good sanity check to the full functional correctness properties. Sometimes full functional correctness is not easily computable but tests of weaker properties are. These redundant properties include unit tests. We can (and should) combine both approaches! Lazy Evaluation It never evaluate anything unless it has to 123sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n This crashes when given a large number. Why? Because of the growing stack frame. 1234567891011121314sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = sumTo' (a+n) (n-1)sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n-- sumTo' 0 5-- sumTo' (0+5) (5-1)-- sumTo' (0+5) 4-- sumTo' (0+5+4) (4-1)-- sumTo' (0+5+4) 3-- sumTo' (0+5+4+3) (3-1)-- sumTo' (0+5+4+3) 2 -&gt; never evaluate the first argument-- .. This still crashes when given a large number. Why? This is called a space leak, and is one of the main drawbacks of Haskellâ€™s lazy evaluation method. Haskell is lazily evaluated, also called call-by-need. This means that expressions are only evaluated when they are needed to compute a result for the user. We can force the previous program to evaluate its accumulator by using a bang pattern, or the primitive operation seq: 1234567{-# LANGUAGE BangPatterns #-}sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' !a 0 = asumTo' !a n = sumTo' (a+n) (n-1)sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = let a' = a + n in a' `seq` sumTo' a' (n-1) AdvantagesLazy Evaluation has many advantages: It enables equational reasoning even in the presence of partial functions and non-termination It allows functions to be decomposed without sacrificing efficiency, for example: minimum = head . sort is, depending on sorting algorithm, possibly O(n). It allows for circular programming and infinite data structures, which allow us to express more things as pure functions. Infinite Data StructuresLaziness lets us define data structures that extend infinitely. Lists are a common example, but it also applies to trees or any user-defined data type: 1ones = 1 : ones Many functions such as take, drop, head, tail, filter and map work fine on infinite lists. 12345naturals = 0 : map (1+) naturals--ornaturals = map sum (inits ones)-- fibonacci numbersfibs = 1:1:zipWith (+) fibs (tail fibs) Data Invariants and ADTsStructure of a ModuleA Haskell program will usually be made up of many modules, each of which exports one or more data types. Typically a module for a data type X will also provide a set of functions, called operations, on X. to construct the data type: c :: Â· Â· Â· â†’ X to query information from the data type: q :: X â†’ Â· Â· Â· to update the data type: u :: Â· Â· Â· X â†’ X A lot of software can be designed with this structure. Example: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869module Dictionary ( Word , Definition , Dict , emptyDict , insertWord , lookup ) whereimport Prelude hiding (Word, lookup)import Test.QuickCheckimport Test.QuickCheck.Modifiers-- lookup :: [(a,b)] -&gt; a -&gt; Maybe btype Word = Stringtype Definition = Stringnewtype Dict = D [DictEntry] deriving (Show, Eq)emptyDict :: DictemptyDict = D []insertWord :: Word -&gt; Definition -&gt; Dict -&gt; DictinsertWord w def (D defs) = D (insertEntry (Entry w def) defs) where insertEntry wd (x:xs) = case compare (word wd) (word x) of GT -&gt; x : (insertEntry wd xs) EQ -&gt; wd : xs LT -&gt; wd : x : xs insertEntry wd [] = [wd]lookup :: Word -&gt; Dict -&gt; Maybe Definitionlookup w (D es) = search w es where search w [] = Nothing search w (e:es) = case compare w (word e) of LT -&gt; Nothing EQ -&gt; Just (defn e) GT -&gt; search w essorted :: (Ord a) =&gt; [a] -&gt; Boolsorted [] = Truesorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)wellformed :: Dict -&gt; Boolwellformed (D es) = sorted esprop_insert_wf dict w d = wellformed dict ==&gt; wellformed (insertWord w d dict)data DictEntry = Entry { word :: Word , defn :: Definition } deriving (Eq, Show)instance Ord DictEntry where Entry w1 d1 &lt;= Entry w2 d2 = w1 &lt;= w2instance Arbitrary DictEntry where arbitrary = Entry &lt;$&gt; arbitrary &lt;*&gt; arbitraryinstance Arbitrary Dict where arbitrary = do Ordered ds &lt;- arbitrary pure (D ds)prop_arbitrary_wf dict = wellformed dict Data Invariants Data invariants are properties that pertain to a particular data type. Whenever we use operations on that data type, we want to know that our data invariants are maintained. For a given data type X, we define a wellformedness predicate $$\\text{wf :: X â†’ Bool}$$For a given value x :: X, wf x returns true iff our data invariants hold for the value x For each operation, if all input values of type X satisfy wf, all output values will satisfy wf. In other words, for each constructor operation c :: Â· Â· Â· â†’ X, we must show wf (c Â· Â· Â·), and for each update operation u :: X â†’ X we must show wf x =â‡’ wf(u x) Abstract Data Types An abstract data type (ADT) is a data type where the implementation details of the type and its associated operations are hidden. 123456newtype Dicttype Word = Stringtype Definition = StringemptyDict :: DictinsertWord :: Word -&gt; Definition -&gt; Dict -&gt; Dictlookup :: Word -&gt; Dict -&gt; Maybe Definition If we donâ€™t have access to the implementation of Dict, then we can only access it via the provided operations, which we know preserve our data invariants. Thus, our data invariants cannot be violated if this module is correct. In general, abstraction is the process of eliminating detail. The inverse of abstraction is called refinement. Abstract data types like the dictionary above are abstract in the sense that their implementation details are hidden, and we no longer have to reason about them on the level of implementation. ValidationSuppose we had a sendEmail function 123sendEmail :: String -- email address -&gt; String -- message -&gt; IO () -- action (more in 2 wks) It is possible to mix the two String arguments, and even if we get the order right, itâ€™s possible that the given email address is not valid. We could define a tiny ADT for validated email addresses, where the data invariant is that the contained email address is valid: 1234567module EmailADT(Email, checkEmail, sendEmail) newtype Email = Email String checkEmail :: String -&gt; Maybe Email checkEmail str | '@' `elem` str = Just (Email str) | otherwise = Nothing-- Then, change the type of sendEmail: sendEmail :: Email -&gt; String -&gt; IO() The only way (outside of the EmailADT module) to create a value of type Email is to use checkEmail. checkEmail is an example of what we call a smart constructor: a constructor that enforces data invariants. Data RefinementReasoning about ADTsConsider the following, more traditional example of an ADT interface, the unbounded queue: 123456data QueueemptyQueue :: Queueenqueue :: Int -&gt; Queue -&gt; Queuefront :: Queue -&gt; Int -- partialdequeue :: Queue -&gt; Queue -- partialsize :: Queue -&gt; Int We could try to come up with properties that relate these functions to each other without reference to their implementation, such as: dequeue (enqueue x emptyQueue) == emptyQueue However these do not capture functional correctness (usually). Models for ADTsWe could imagine a simple implementation for queues, just in terms of lists: 12345emptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = length But this implementation is O(n) to enqueue! Unacceptable! However!(This is out mental model) This is a dead simple implementation, and trivial to see that it is correct. If we make a better queue implementation, it should always give the same results as this simple one. Therefore: This implementation serves as a functional correctness specification for our Queue type! Refinement Relations The typical approach to connect our model queue to our Queue type is to define a relation, called a refinement relation, that relates a Queue to a list and tells us if the two structures represent the same queue conceptually: 1234rel :: Queue -&gt; [Int] -&gt; Boolprop_empty_r = rel emptyQueue emptyQueueLprop_size_r fq lq = rel fq lq ==&gt; size fq == sizeL lqprop_enq_ref fq lq x = rel fq lq ==&gt; rel (enqueue x fq) (enqueueL x lq) Abstraction FunctionsThese refinement relations are very difficult to use with QuickCheck because the rel fq lq preconditions are very hard to satisfy with randomly generated inputs. For this example, itâ€™s a lot easier if we define an abstraction function that computes the corresponding abstract list from the concrete Queue. 1toAbstract :: Queue â†’ [Int] Conceptually, our refinement relation is then just: 1\\fq lq â†’ absfun fq == lq However, we can re-express our properties in a much more QC-friendly format 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import Test.QuickCheckemptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = lengthtoAbstract :: Queue -&gt; [Int]toAbstract (Q f sf r sr) = f ++ reverse rprop_empty_ref = toAbstract emptyQueue == emptyQueueLprop_enqueue_ref fq x = toAbstract (enqueue x fq) == enqueueL x (toAbstract fq)prop_size_ref fq = size fq == sizeL (toAbstract fq)prop_front_ref fq = size fq &gt; 0 ==&gt; front fq == frontL (toAbstract fq)prop_deq_ref fq = size fq &gt; 0 ==&gt; toAbstract (dequeue fq) == dequeueL (toAbstract fq)prop_wf_empty = wellformed emptyQueueprop_wf_enq x q = wellformed q ==&gt; wellformed (enqueue x q)prop_wf_deq x q = wellformed q &amp;&amp; size q &gt; 0 ==&gt; wellformed (dequeue q)data Queue = Q [Int] -- front of the queue Int -- size of the front [Int] -- rear of the queue Int -- size of the rear deriving (Show, Eq)wellformed :: Queue -&gt; Boolwellformed (Q f sf r sr) = length f == sf &amp;&amp; length r == sr &amp;&amp; sf &gt;= srinstance Arbitrary Queue where arbitrary = do NonNegative sf' &lt;- arbitrary NonNegative sr &lt;- arbitrary let sf = sf' + sr f &lt;- vectorOf sf arbitrary r &lt;- vectorOf sr arbitrary pure (Q f sf r sr)inv3 :: Queue -&gt; Queueinv3 (Q f sf r sr) | sf &lt; sr = Q (f ++ reverse r) (sf + sr) [] 0 | otherwise = Q f sf r sremptyQueue :: QueueemptyQueue = Q [] 0 [] 0enqueue :: Int -&gt; Queue -&gt; Queueenqueue x (Q f sf r sr) = inv3 (Q f sf (x:r) (sr+1))front :: Queue -&gt; Int -- partialfront (Q (x:f) sf r sr) = xdequeue :: Queue -&gt; Queue -- partialdequeue (Q (x:f) sf r sr) = inv3 (Q f (sf -1) r sr)size :: Queue -&gt; Intsize (Q f sf r sr) = sf + sr Data RefinementThese kinds of properties establish what is known as a data refinement from the abstract, slow, list model to the fast, concrete Queue implementation. Refinement and Specifications In general, all functional correctness specifications can be expressed as: all data invariants are maintained, and the implementation is a refinement of an abstract correctness model. There is a limit to the amount of abstraction we can do before they become useless for testing (but not necessarily for proving). Effects Effects are observable phenomena from the execution of a program. Internal vs. External EffectsExternal Observability An external effect is an effect that is observable outside the function. Internal effects are not observable from outside. Example Console, file and network I/O; termination and non-termination; non-local control flow; etc. Are memory effects external or internal? Depends on the scope of the memory being accessed. Global variable accesses are external. PurityA function with no external effects is called a pure function. A pure function is the mathematical notion of a function. That is, a function of type a -&gt; b is fully specified by a mapping from all elements of the domain type a to the codomain type b. Consequences: Two invocations with the same arguments result in the same value. No observable trace is left beyond the result of the function. No implicit notion of time or order of execution. Haskell FunctionsHaskell functions are technically not pure. They can loop infinitely. They can throw exceptions (partial functions) They can force evaluation of unevaluated expressions. Caveat Purity only applies to a particular level of abstraction. Even ignoring the above, assembly instructions produced by GHC arenâ€™t really pure. Despite the impurity of Haskell functions, we can often reason as though they are pure. Hence we call Haskell a purely functional language. The Danger of Implicit Side Effects They introduce (often subtle) requirements on the evaluation order. They are not visible from the type signature of the function. They introduce non-local dependencies which is bad for software design, increasing coupling. They interfere badly with strong typing, for example mutable arrays in Java, or reference types in ML. We canâ€™t, in general, reason equationally about effectful programs! Can we program with pure functions?Typically, a computation involving some state of type s and returning a result of type a can be expressed as a function: 1s -&gt; (s, a) Rather than change the state, we return a new copy of the state. All that copying might seem expensive, but by using tree data structures, we can usually reduce the cost to an O(log n) overhead. StateState Passing12345678910111213data Tree a = Branch a (Tree a) (Tree a) | Leaf-- Given a tree, label each node with an ascending number in infix order:label :: Tree () -&gt; Tree Intlable t = snd (go t 1) where go :: Tree() -&gt; Int -&gt; (Int, Tree Int) go Leaf c = (c, Leaf) go (Branch () l r) c = let (c', l') = go l c v = c' (c'', r') = go r (c'+1) in (c'', Branch v l' r')-- it works but not pretty Letâ€™s use a data type to simplify this! State12345678910111213141516newtype State s a = A procedure that, manipulating some state of type s, returns a-- State Operationsget :: State s sput :: s -&gt; State s ()pure :: a -&gt; State s aevalState :: State s a -&gt; s -&gt; a-- Sequential Composition-- Do one state action after another with do blocks:do put 42 desugars put 42 &gt;&gt; put Truepure True(&gt;&gt;) :: State s a -&gt; State s b -&gt; State s b-- Bind-- The 2nd step can depend on the first with bind:do x &lt;- get desugars get &gt;&gt;= \\x -&gt; pure (x + 1)pure (x+1)(&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b Example 12345678910111213141516171819202122232425262728293031323334353637383940{-modify :: (s -&gt; s) -&gt; State s ()modify f = do s &lt;- get put (f s)-}label' :: Tree () -&gt; Tree Intlabel' t = evalState (go t) 1 where go :: Tree () -&gt; State Int (Tree Int) go Leaf = pure Leaf go (Branch () l r) = do l' &lt;- go l v &lt;- get put (v + 1) r' &lt;- go r pure (Branch v l' r')newtype State' s a = State (s -&gt; (s, a))get' :: State' s sget' = (State $ \\s -&gt; (s, s)) put' :: s -&gt; State' s ()put' s = State $ \\_ -&gt; (s,())pure' :: a -&gt; State' s apure' a = State $ \\s -&gt; (s, a)evalState' :: State' s a -&gt; s -&gt; aevalState (State f) s = snd (f s) (&gt;&gt;=!) :: State' s a -&gt; (a -&gt; State' s b) -&gt; State' s b(State c) &gt;&gt;=! f = State $ \\s -&gt; let (s', a) = c s (State c') = f a in c' s'(&gt;&gt;!) :: State' s a -&gt; State' s b -&gt; State' s b(&gt;&gt;!) a b = a &gt;&gt;=! \\_ -&gt; b IO A procedure that performs some side effects, returning a result of type a is written as IO a. IO a is an abstract type. But we can think of it as a function: RealWorld -&gt; (RealWorld, a) (thatâ€™s how itâ€™s implemented in GHC) 123456(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO bpure :: a -&gt; IO agetChar :: IO CharreadLine :: IO StringputStrLn :: String -&gt; IO () -- return a procedure Infectious IOWe can convert pure values to impure procedures with pure: 1pure :: a -&gt; IO a But we canâ€™t convert impure procedures to pure values The only function that gets an a from an IO a is &gt;&gt;=: 1(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b But it returns an IO procedure as well. If a function makes use of IO effects directly or indirectly, it will have IO in its type! Haskell Design StrategyWe ultimately â€œrunâ€ IO procedures by calling them from main: 1main :: IO () Example 12345678-- Given an input number n, print a triangle of * characters of base width n.printTriangle :: Int -&gt; IO ()printTriangle 0 = pure ()printTriangle n = do putStrLn (replicate n '*') printTriangle (n - 1)main = printTriangle 9 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108{-Design a game that reads in a n Ã— n maze from a file. The player starts at position(0, 0) and must reach position (n âˆ’ 1, n âˆ’ 1) to win. The game accepts keyboard inputto move the player around the maze.-}import Data.List import System.IOmazeSize :: Int mazeSize = 10data Tile = Wall | Floor deriving (Show, Eq)type Point = (Int, Int)lookupMap :: [Tile] -&gt; Point -&gt; TilelookupMap ts (x,y) = ts !! (y * mazeSize + x)addX :: Int -&gt; Point -&gt; PointaddX dx (x,y) = (x + dx, y)addY :: Int -&gt; Point -&gt; PointaddY dy (x,y) = (x, y + dy)data Game = G { player :: Point , map :: [Tile] }invariant :: Game -&gt; Bool invariant (G (x,y) ts) = x &gt;= 0 &amp;&amp; x &lt; mazeSize &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; mazeSize &amp;&amp; lookupMap ts (x,y) /= WallmoveLeft :: Game -&gt; Game moveLeft (G p m) = let g' = G (addX (-1) p) m in if invariant g' then g' else G p mmoveRight :: Game -&gt; GamemoveRight (G p m) = let g' = G (addX 1 p) m in if invariant g' then g' else G p mmoveUp :: Game -&gt; GamemoveUp (G p m) = let g' = G (addY (-1) p) m in if invariant g' then g' else G p mmoveDown :: Game -&gt; GamemoveDown (G p m) = let g' = G (addY 1 p) m in if invariant g' then g' else G p mwon :: Game -&gt; Bool won (G p m) = p == (mazeSize-1,mazeSize-1)main :: IO () main = do str &lt;- readFile \"input.txt\" let initial = G (0,0) (stringToMap str) gameLoop initial where gameLoop :: Game -&gt; IO () gameLoop state | won state = putStrLn \"You win!\" | otherwise = do display state c &lt;- getChar' case c of 'w' -&gt; gameLoop (moveUp state) 'a' -&gt; gameLoop (moveLeft state) 's' -&gt; gameLoop (moveDown state) 'd' -&gt; gameLoop (moveRight state) 'q' -&gt; pure () _ -&gt; gameLoop state stringToMap :: String -&gt; [Tile]stringToMap [] = []stringToMap ('#':xs) = Wall : stringToMap xs stringToMap (' ':xs) = Floor : stringToMap xs stringToMap (c:xs) = stringToMap xsdisplay :: Game -&gt; IO () display (G (px,py) m) = printer (0,0) m where printer (x,y) (t:ts) = do if (x,y) == (px,py) then putChar '@' else if t == Wall then putChar '#' else putChar ' ' if (x == mazeSize - 1) then do putChar '\\n' printer (0,y+1) ts else printer (x+1,y) ts printer (x,y) [] = putChar '\\n'getChar' :: IO Char getChar' = do b &lt;- hGetBuffering stdin e &lt;- hGetEcho stdin hSetBuffering stdin NoBuffering hSetEcho stdin False x &lt;- getChar hSetBuffering stdin b hSetEcho stdin e pure x Benefits of an IO Type Absence of effects makes type system more informative: A type signatures captures entire interface of the function All dependencies are explicit in the form of data dependencies. All dependencies are typed It is easier to reason about pure code and it is easier to test Testing is local, doesnâ€™t require complex set-up and tear-down. Reasoning is local, doesnâ€™t require state invariants Type checking leads to strong guarantees. Mutable VariablesWe can have honest-to-goodness mutability in Haskell, if we really need it, using IORef. 1234data IORef anewIORef :: a -&gt; IO (IORef a)readIORef :: IORef a -&gt; IO awriteIORef :: IORef a -&gt; a -&gt; IO () Example 1234567891011121314151617181920212223242526import Data.IORef import Test.QuickCheck.Monadic import Test.QuickCheckaverageListIO :: [Int] -&gt; IO IntaverageListIO ls = do sum &lt;- newIORef 0 count &lt;- newIORef 0 let loop :: [Int] -&gt; IO () loop [] = pure () loop (x:xs) = do s &lt;- readIORef sum writeIORef sum (s + x) c &lt;- readIORef count writeIORef count (c + 1) loop xs loop ls s &lt;- readIORef sum c &lt;- readIORef count pure (s `div` c) prop_average :: [Int] -&gt; Propertyprop_average ls = monadicIO $ do pre (length ls &gt; 0) avg &lt;- run (averageListIO ls) assert (avg == (sum ls `div` length ls)) Mutable Variables, LocallySomething like averaging a list of numbers doesnâ€™t require external effects, even if we use mutation internally. 12345data STRef s anewSTRef :: a -&gt; ST (STRef s a)readSTRef :: STRef s a -&gt; ST s awriteSTRef :: STRef s a -&gt; a -&gt; ST s ()runST :: (forall s. ST s a) -&gt; a The extra s parameter is called a state thread, that ensures that mutable variables donâ€™t leak outside of the ST computation. The ST type is not assessable in this course, but it is useful sometimes in Haskell programming. QuickChecking EffectsQuickCheck lets us test IO (and ST) using this special property monad interface: 1234monadicIO :: PropertyM IO () -&gt; Propertypre :: Bool -&gt; PropertyM IO ()assert :: Bool -&gt; PropertyM IO ()run :: IO a -&gt; PropertyM IO a Do notation and similar can be used for PropertyM IO procedures just as with State s and IO procedures. Example 12345678910111213-- GNU Factorimport Test.QuickCheck import Test.QuickCheck.Modifiersimport Test.QuickCheck.Monadic import System.Process-- readProcess :: FilePath -&gt; [String] -&gt; String -&gt; IO Stringtest_gnuFactor :: Positive Integer -&gt; Propertytest_gnuFactor (Positive n) = monadicIO $ do str &lt;- run (readProcess \"gfactor\" [show n] \"\") let factors = map read (tail (words str)) assert (product factors == n) FunctorRecall the type class defined over type constructors called Functor. Weâ€™ve seen instances for lists, Maybe, tuples and functions. Other instances include: IO (how?) States (how?) Gen 123456789101112131415ioMap :: (a -&gt; b) -&gt; IO a -&gt; IO bioMap f act = do a &lt;- act pure (f a) stateMap :: (a -&gt; b) -&gt; State s a -&gt; State s bstateMap f act = do a &lt;- act pure (f a)-- more generalmonadMap :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m bmonadMap f act = do a &lt;- act pure (f a) QuickCheck Generators12345678910{-class Arbitrary a where arbitray :: Gen a shrink :: a -&gt; [a]-- Gen a ~=~ random generator of values type a.-}sortedLists :: (Arbitrary a, Ord a) =&gt; Gen [a]sortedLists = fmap sort arbitrary :: Gen a-- listOf :: Gen a -&gt; Gen [a] Applicative FunctorsBinary FunctionsSuppose we want to look up a studentâ€™s zID and program code using these functions: 1234lookupID :: Name -&gt; Maybe ZIDlookupProgram :: Name -&gt; Maybe Program-- we had a function:makeRecord :: ZID -&gt; Program -&gt; StudentRecord How can we combine these functions to get a function of type Name -&gt; Maybe StudentRecord? 1234lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in ? Binary Map? We could imagine a binary version of the maybeMap function: 12maybeMap2 :: (a -&gt; b -&gt; c) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c But then, we might need a trinary version. 12maybeMap3 :: (a -&gt; b -&gt; c -&gt; d) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c -&gt; Maybe d Or even a 4-ary version, 5-ary, 6-ary. . . this would quickly become impractical! Using Functor? Using fmap gets us part of the way there: 12345lookupRecord' :: Name -&gt; Maybe (Program -&gt; StudentRecord)lookupRecord' n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid -- what about program? But, now we have a function inside a Maybe. Applicative This is encapsulated by a subclass of Functor called Applicative. 123class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b Maybe is an instance, so we can use this for lookupRecord: 12345lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid &lt;*&gt; program -- or pure makeRecord &lt;*&gt; zid &lt;*&gt; program Using ApplicativeIn general, we can take a regular function application:$$\\text{f a b c d}$$And apply that function to Maybe (or other Applicative) arguments using this pattern (where &lt;*&gt; is left-associative): $$\\text{pure f &lt;*&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; }$$ Relationship to FunctorAll law-abiding instances of Applicative are also instances of Functor, by defining: 1fmap f x = pure f &lt;*&gt; x Sometimes this is written as an infix operator, &lt;$&gt;, which allows us to write: 123pure f &lt;*&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md-- as:f &lt;$&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md Applicative laws12345678-- Identitypure id &lt;*&gt; v = v-- Homomorphismpure f &lt;*&gt; pure x = pure (f x)-- Interchangeu &lt;*&gt; pure y = pure ($ y) &lt;*&gt; u-- Compositionpure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w) Example 1234567891011121314151617181920212223242526272829type Name = Stringtype ZID = Intdata Program = COMP | SENG | BINF | CENG deriving (Show, Eq)type StudentRecord = (Name, ZID, Program)lookupID :: Name -&gt; Maybe ZIDlookupID \"Liam\" = Just 3253158lookupID \"Unlucky\" = Just 4444444lookupID \"Prosperous\" = Just 8888888lookupID _ = NothinglookupProgram :: Name -&gt; Maybe ProgramlookupProgram \"Liam\" = Just COMPlookupProgram \"Unlucky\" = Just SENGlookupProgram \"Prosperous\" = Just CENGlookupProgram _ = NothingmakeRecord :: ZID -&gt; Program -&gt; Name -&gt; StudentRecordmakeRecord zid pr name = (name,zid,pr)liam :: Maybe StudentRecordliam = let mzid = lookupID \"Liam\" mprg = lookupProgram \"Liam\" in pure makeRecord &lt;*&gt; mzid &lt;*&gt; mprg &lt;*&gt; pure \"Liam\"-- pure :: a -&gt; Maybe a-- fmap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b Functor Laws for ApplicativeThese are proofs not Haskell code: 123456789101112131415fmap f x = pure f &lt;*&gt; x-- The two functor laws are:1. fmap id x == x2. fmap f (fmap g x) == fmap (f.g) x-- Proof:1) pure id &lt;*&gt; x == x -- true by Identity law2) pure f &lt;*&gt; (pure g &lt;*&gt; x) == pure (.) &lt;*&gt; pure f &lt;*&gt; pure g &lt;*&gt; x --Composition == pure ((.) f) &lt;*&gt; pure g &lt;*&gt; x --Homomorphism == pure (f.g) &lt;*&gt; x --Homomorphism Applicative ListsThere are two ways to implement Applicative for lists: 1(&lt;*&gt;) :: [a -&gt; b] -&gt; [a] -&gt; [b] Apply each of the given functions to each of the given arguments, concatenating all the results Apply each function in the list of functions to the corresponding value in the list of arguments The second one is put behind a newtype (ZipList) in the Haskell standard library. 1234567891011121314pureZ :: a -&gt; [a]pureZ a = a:pureZ aapplyListsZ :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsZ (f:fs) (x:xs) = f x : applyListsZ fs xsapplyListsZ [] _ = []applyListsZ _ [] = []pureC :: a -&gt; [a]pureC a = [a] applyListsC :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsC (f:fs) args = map f args ++ applyListsC fs argsapplyListsC [] args = [] Other instancesQuickCheck generators: Gen1234data Concrete = C [Char] [Char] deriving (Show, Eq)instance Arbitrary Concrete where arbitrary = C &lt;$&gt; arbitrary &lt;*&gt; arbitrary Functions: ((-&gt;) x The Applicative instance for functions lets us pass the same argument into multiple functions without repeating ourselves. 1234567891011instance Applicative ((-&gt;) x) where pure :: a -&gt; x -&gt; a pure a x = a (&lt;*&gt;) :: (x -&gt; (a -&gt; b)) -&gt; (x -&gt; a) -&gt; (x -&gt; b) (&lt;*&gt;) xab xa x = xab x (xa x)-- f (g x) (h x) (i x)---- Can be written as: -- (pure f &lt;*&gt; g &lt;*&gt; h &lt;*&gt; i) x Tuples: ((,) x)We canâ€™t implement pure without an extra constraint! The tuple instance for Applicative lets us combine secondary outputs from functions into one secondary output without manually combining them. 1234567891011instance Functor ((,) x) where fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x,b) fmap f (x,a) = (x,f a)-- monoid has an identity elementinstance Monoid x =&gt; Applicative ((,) x) where pure :: a -&gt; (x,a) pure a = (mempty ,a) (&lt;*&gt;) :: (x,a -&gt; b) -&gt; (x, a) -&gt; (x, b) (&lt;*&gt;) (x, f) (x',a) = (x &lt;&gt; x', f a) It requires Monoid here to combine the values, and to provide a default value for pure. 12345678910111213f :: A -&gt; (Log, B)g :: X -&gt; (Log, Y)a :: Ax :: Xcombine :: B -&gt; Y -&gt; Z-- combine the logs silentlytest :: (Log, Z)test = combine &lt;$&gt; f a &lt;*&gt; g x-- instead of -- let (l1, b) = f a-- (l2, y) = g x-- in (l1 &lt;&gt; l2, combine b y) IO and State s123456789instance Applicative IO where pure :: a -&gt; IO a pure a = pure a (&lt;*&gt;) :: IO (a -&gt; b) -&gt; IO a -&gt; IO b pf &lt;*&gt; pa = do f &lt;- pf a &lt;- pa pure (f a) Monads Monads are types m where we can sequentially compose functions of the form a -&gt; m b 12class Applicative m =&gt; Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b Sometimes in old documentation the function return is included here, but it is just an alias for pure. It has nothing to do with return as in C/Java/Python etc. Example 123456789101112131415161718192021222324-- Maybe Monad(&gt;&gt;=) :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b(&gt;&gt;=) Nothing f = Nothing(&gt;&gt;=) (Just a) f = f a-- List Monad(&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b](&gt;&gt;=) as f = concatMap f as-- function(&gt;&gt;=) :: (x -&gt; a) -&gt; (a -&gt; x -&gt;b) -&gt; b(&gt;&gt;=) xa axb x = axb (xa x) x-- function monad example(reader monad)f :: A -&gt; Config -&gt; Bf :: X -&gt; Config -&gt; Yf :: B -&gt; Config -&gt; Ccombine :: (A, X) -&gt; Config -&gt; (Y, C)combine (a, x) = do b &lt;- f a y &lt;- g x c &lt;- h b pure(y, c) Monad LawWe can define a composition operator with (&gt;&gt;=): 12(&lt;=&lt;) :: (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) (f &lt;=&lt; g) x = g x &gt;&gt;= f Monad Laws 123f &lt;=&lt; (g &lt;=&lt; x) == (f &lt;=&lt; g) &lt;=&lt; x -- associativitypure &lt;=&lt; f == f -- left identityf &lt;=&lt; pure == f -- right identity These are similar to the monoid laws, generalised for multiple types inside the monad. This sort of structure is called a category in mathematics. Relationship to ApplicativeAll Monad instances give rise to an Applicative instance, because we can define &lt;*&gt; in terms of &gt;&gt;=. 1mf &lt;*&gt; mx = mf &gt;&gt;= \\f -&gt; mx &gt;&gt;= \\x -&gt; pure (f x) This implementation is already provided for Monads as the ap function in Control.Monad Do notationWorking directly with the monad functions can be unpleasant. As weâ€™ve seen, Haskell has some notation to increase niceness: 1234567do x &lt;- y becomes y &gt;&gt;= \\x -&gt; do z zdo x becomes x &gt;&gt;= \\_ -&gt; do y y Examples 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071{- (Dice Rolls) Roll two 6-sided dice, if the difference is &lt; 2, reroll the second die. Final score is the difference of the two die. What score is most common?-}roll :: [Int]roll = [1, 2, 3, 4, 5, 6]diceGame = do d1 &lt;- roll d2 &lt;- roll if (abs (d1 - d2) &lt; 2) then do d2' &lt;- roll pure (abs (d1 - d2')) else pure (abs (d1 - d2)) {- Partial Functions We have a list of student names in a database of type [(ZID, Name)]. Given a list of zIDâ€™s, return a Maybe [Name], where Nothing indicates that a zID could not be found-}db :: [(ZID, Name)]db = [(3253158, \"Liam\"), (8888888, \"Rich\"), (4444444, \"Mort\")]studentNames :: [ZID] -&gt; Maybe [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- lookup z db ns &lt;- studentNames zs pure (n:ns)-- briefer but less clear with applicative notation:-- studentNames (z:zs) = (:) &lt;$&gt; lookup z db &lt;*&gt; studentNames zs{- Arbitrary Instances Define a Tree type and a generator for search trees: searchTrees :: Int -&gt; Int -&gt; Generator Tree-}data Tree a = Leaf | Branch a (Tree a) (Tree a) deriving (Show, Eq)instance Arbitrary (Tree Int) where arbitrary = do mn &lt;- (arbitrary :: Gen Int) Positive delta &lt;- arbitrary let mx = mn + delta searchTree mn mx where searchTree :: Int -&gt; Int -&gt; Gen (Tree Int) searchTree mn mx | mn &gt;= mx = pure Leaf | otherwise = do v &lt;- choose (mn,mx) l &lt;- searchTree mn v r &lt;- searchTree (v+1) mx pure (Branch v l r)-- The Either MonadstudentNames :: [ZID] -&gt; Either ZID [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- case lookup z db of Just v -&gt; Right v Nothing -&gt; Left z ns &lt;- studentNames zs pure (n:ns) Static Assurance with TypesStatic AssureanceMethods of Assurance Static means of assurance analyse a program without running it. Static vs. Dynamic Static checks can be exhaustive. Exhaustivity An exhaustive check is a check that is able to analyse all possible executions of a program. However, some properties cannot be checked statically in general (halting problem), or are intractable to feasibly check statically (state space explosion). Dynamic checks cannot be exhaustive, but can be used to check some properties where static methods are unsuitable. Compiler IntegrationMost static and all dynamic methods of assurance are not integrated into the compilation process. You can compile and run your program even if it fails tests You can change your program to diverge from your model checker model. Your proofs can diverge from your implementation. Types Because types are integrated into the compiler, they cannot diverge from the source code. This means that type signatures are a kind of machine-checked documentation for your code. Types are the most widely used kind of formal verification in programming today. They are checked automatically by the compiler. They can be extended to encompass properties and proof systems with very high expressivity (covered next week). They are an exhaustive analysis Phantom Types A type parameter is phantom if it does not appear in the right hand side of the type definition. 1newtype Size x = S Int Lets examine each one of the following use cases: We can use this parameter to track what data invariants have been established about a value. We can use this parameter to track information about the representation (e.g. units of measure). We can use this parameter to enforce an ordering of operations performed on these values (type state). ValidationSuppose we have 123data UG -- empty typedata PGdata StudentID x = SID Int We can define a smart constructor that specialises the type parameter: 12sid :: Int -&gt; Either (StudentID UG) (StudentID PG) Define functions: 12enrolInCOMP3141 :: StudentID UG -&gt; IO ()lookupTranscript :: StudentID x -&gt; IO String Units of Measure123456789data Kilometresdata Milesdata Value x = U IntsydneyToMelbourne = (U 877 :: Value Kilometres)losAngelesToSanFran = (U 383 :: Value Miles)-- Note the arguments to area must have the same unitdata Square aarea :: Value m -&gt; Value m -&gt; Value (Square m)area (U x) (U y) = U (x * y) Type State123456789101112{- A Socket can either be ready to recieve data, or busy. If the socket is busy, the user must first use the wait operation, which blocks until the socket is ready. If the socket is ready, the user can use the send operation to send string data, which will make the socket busy again.-}data Busydata Readynewtype Socket s = Socket ...wait :: Socket Busy -&gt; IO (Socket Ready)send :: Socket Ready -&gt; String -&gt; IO (Socket Busy)-- assumption: use the socket in a linear way(only use once) Linearity and Type StateThe previous code assumed that we didnâ€™t re-use old Sockets: 12345678910send2 :: Socket Ready -&gt; String -&gt; String -&gt; IO (Socket Busy)send2 s x y = do s' &lt;- send s x s'' &lt;- wait s' s''' &lt;- send s'' y pure s'''-- But we can just re-use old values to send without waiting:send2' s x y = do _ &lt;- send s x s' &lt;- send s y pure s' Linear type systems can solve this, but not in Haskell (yet) Datatype Promotion123data UGdata PGdata StudentID x = SID Int Defining empty data types for our tags is untyped. We can have StudentID UG, but also StudentID String. The DataKinds language extension lets us use data types as kinds: 1234567891011121314{-# LANGUAGE DataKinds, KindSignatures #-}data Stream = UG | PGdata StudentID (x :: Stream) = SID Intpostgrad :: [Int]postgrad = [3253158]makeStudentID :: Int -&gt; Either (StudentID UG) (StudentID PG)makeStudentID i | i `elem` postgrad = Right (SID i) | otherwise = Left (SID i)enrollInCOMP3141 :: StudentID UG -&gt; IO ()enrollInCOMP3141 (SID x) = putStrLn (show x ++ \" enrolled in COMP3141!\") GADTsUntyped Evaluator 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950data Expr t = BConst Bool | IConst Int | Times (Expr Int) (Expr Int) | Less (Expr Int) (Expr Int) | And (Expr Bool) (Expr Bool) | If (Expr Bool) (Expr t) (Expr t) deriving (Show, Eq)data Value = BVal Bool | IVal Int deriving (Show, Eq)eval :: Expr -&gt; Valueeval (BConst b) = BVal beval (IConst i) = IVal ieval (Times e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; IVal (i1 * i2)eval (Less e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; BVal (i1 &lt; i2) eval (And e1 e2) = case (eval e1, eval e2) of (BVal b1, BVal b2) -&gt; BVal (b1 &amp;&amp; b2) eval (If ec et ee) = case eval ec of BVal True -&gt; eval et BVal False -&gt; eval ee-- partial functioneval :: Expr -&gt; Maybe Valueeval (BConst b) = pure (BVal b)eval (IConst i) = pure (IVal i)eval (Times e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (IVal (v1' * v2')) _ -&gt; Nothingeval (Less e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (BVal (v1' &lt; v2')) _ -&gt; Nothingeval (And e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (BVal v1', BVal v2') -&gt; pure (BVal (v1' &amp;&amp; v2')) _ -&gt; Nothingeval (If ec et ee) = do v1 &lt;- eval ec case v1 of (BVal True) -&gt; eval et (BVal False) -&gt; eval ee GADTs Generalised Algebraic Datatypes (GADTs) is an extension to Haskell that, among other things, allows data types to be specified by writing the types of their constructors. 1234567{-# LANGUAGE GADTs, KindSignatures #-}-- Unary natural numbers, e.g. 3 is S (S (S Z))data Nat = Z | S Nat-- is the same asdata Nat :: * whereZ :: NatS :: Nat -&gt; Nat When combined with the type indexing trick of phantom types, this becomes very powerful! Typed Evaluator There is now only one set of precisely-typed constructors. 12345678910111213141516{-# LANGUAGE GADTs, KindSignatures #-}data Expr :: * -&gt; * where BConst :: Bool -&gt; Expr Bool IConst :: Int -&gt; Expr Int Times :: Expr Int -&gt; Expr Int -&gt; Expr Int Less :: Expr Int -&gt; Expr Int -&gt; Expr Bool And :: Expr Bool -&gt; Expr Bool -&gt; Expr Bool If :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr aeval :: Expr t -&gt; teval (IConst i) = ieval (BConst b) = beval (Times e1 e2) = eval e1 * eval e2eval (Less e1 e2) = eval e1 &lt; eval e2eval (And e1 e2) = eval e1 &amp;&amp; eval e2eval (If ec et ee) = if eval ec then eval et else eval ee ListsWe could define our own list type using GADT syntax as follows: 123456data List (a :: *) :: * whereNil :: List aCons :: a -&gt; List a -&gt; List a-- head (hd) and tail (tl) functions are partial hd (Cons x xs) = xtl (Cons x xs) = xs We will constrain the domain of these functions by tracking the length of the list on the type level. Vectors123456789101112131415161718192021222324252627282930313233{-# LANGUAGE GADTs, KindSignatures #-}{-# LANGUAGE DataKinds, StandaloneDeriving, TypeFamilies #-}data Nat = Z | S Natplus :: Nat -&gt; Nat -&gt; Nat plus Z n = nplus (S m) n = S (plus m n)type family Plus (m :: Nat) (n :: Nat) :: Nat where Plus Z n = n Plus (S m) n = S (Plus m n)data Vec (a :: *) :: Nat -&gt; * where Nil :: Vec a Z Cons :: a -&gt; Vec a n -&gt; Vec a (S n)deriving instance Show a =&gt; Show (Vec a n)appendV :: Vec a m -&gt; Vec a n -&gt; Vec a (Plus m n)appendV Nil ys = ysappendV (Cons x xs) ys = Cons x (appendV xs ys)-- 0: Z-- 1: S Z-- 2: S (S Z)hd :: Vec a (S n) -&gt; ahd (Cons x xs) = xmapVec :: (a -&gt; b) -&gt; Vec a n -&gt; Vec b nmapVec f Nil = NilmapVec f (Cons x xs) = Cons (f x) (mapVec f xs) TradeoffsThe benefits of this extra static checking are obvious, however: It can be difficult to convince the Haskell type checker that your code is correct, even when it is. Type-level encodings can make types more verbose and programs harder to understand. Sometimes excessively detailed types can make type-checking very slow, hindering productivity Pragmatism We should use type-based encodings only when the assurance advantages outweigh the clarity disadvantages. The typical use case for these richly-typed structures is to eliminate partial functions from our code base. If we never use partial list functions, length-indexed vectors are not particularly useful Theory of TypesLogic We can specify a logical system as a _deductive system_ by providing a set of rules and axioms that describe how to prove various connectives. Natural Deduction A way we can specify logic Each connective typically has introduction and elimination rules. For example, to prove an implication A â†’ B holds, we must show that B holds assuming A. This introduction rule is written as: More rulesImplication also has an elimination rule, that is also called modus ponens:$$\\frac{\\ulcorner \\vdash A\\rightarrow B \\space\\space\\space\\space\\space\\space\\space\\space\\ulcorner \\vdash A}{\\ulcorner \\vdash B}\\rightarrow-E$$Conjunction (and) has an introduction rule that follows our intuition:$$\\frac{\\ulcorner \\vdash A\\space\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner \\vdash B}{\\ulcorner \\vdash A\\land B}\\land-I_1$$It has two elimination rules:$$\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash A}\\land-E_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash B}\\land-E_2$$Disjunction (or) has two introduction rules:$$\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_2$$Disjunction elimination is a little unusual:$$\\frac{\\ulcorner \\vdash A\\lor B\\space\\space\\space\\space\\space\\space\\space A,\\ulcorner \\vdash P \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner \\vdash P}{\\ulcorner \\vdash P}\\lor-E$$The true literal, written T, has only an introduction:$$\\frac{}{\\ulcorner \\vdash \\top}$$And false, written âŠ¥, has just elimination (ex falso quodlibet):$$\\frac{\\ulcorner \\vdash \\bot}{\\ulcorner \\vdash P}$$Typically we just defineï¼š$$\\neg A \\equiv(A\\rightarrow\\bot)$$ Constructive LogicThe logic we have expressed so far does not admit the law of the excluded middle:$$P\\lor\\neg P$$Or the equivalent double negation elimination:$$(\\neg\\neg P)\\rightarrow P$$This is because it is a constructive logic that does not allow us to do proof by contradiction. Typed Lambda CalculusBoiling Haskell DownThe theoretical properties we will describe also apply to Haskell, but we need a smaller language for demonstration purposes. No user-defined types, just a small set of built-in types. No polymorphism (type variables) Just lambdas (Î»x.e) to define functions or bind variables. This language is a very minimal functional language, called the simply typed lambda calculus, originally due to Alonzo Church. Our small set of built-in types are intended to be enough to express most of the data types we would otherwise define. We are going to use logical inference rules to specify how expressions are given types (typing rules). Function TypesWe create values of a function type A â†’ B using lambda expressions:$$\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$The typing rule for function application is as follows:$$\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2:: B}$$ Composite Data TypesIn addition to functions, most programming languages feature ways to compose types together to produce new types, such as: Classes, Tuples, Structs, Unions, Recordsâ€¦ Product TypesFor simply typed lambda calculus, we will accomplish this with tuples, also called product types. (A, B) We wonâ€™t have type declarations, named fields or anything like that. More than two values can be combined by nesting products, for example a three dimensional vector:$$\\text{(Int, (Int, Int))}$$ Constructors and EliminatorsWe can construct a product type the same as Haskell tuples:$$\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}$$The only way to extract each component of the product is to use the fst and snd eliminators:$$\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}$$ Unit TypesCurrently, we have no way to express a type with just one value. This may seem useless at first, but it becomes useful in combination with other types. Weâ€™ll introduce the unit type from Haskell, written (), which has exactly one inhabitant, also written ():$$\\frac{}{\\ulcorner\\vdash ():: ()}$$ Disjunctive CompositionWe canâ€™t, with the types we have, express a type with exactly three values. 1data TrafficLight = Red | Amber | Green In general we want to express data that can be one of multiple alternatives, that contain different bits of data. 12345type Length = Inttype Angle = Intdata Shape = Rect Length Length | Circle Length | Point | Triangle Angle Length Length Sum TypesWeâ€™ll build in the Haskell Either type to express the possibility that data may be one of two forms.$$\\text{Either } A \\space B$$These types are also called sum types. Our TrafficLight type can be expressed (grotesquely) as a sum of units:$$\\text{TrafficLight } \\simeq \\text{Either () (Either () ())}$$ Constructors and Eliminators for SumsTo make a value of type Either A B, we invoke one of the two constructors:$$\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}$$We can branch based on which alternative is used using pattern matching:$$\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}$$Example Our traffic light type has three values as required:$$\\begin{align*}\\text{TrafficLight } &amp;\\simeq \\text{Either () (Either () ())}\\\\\\text{Red } &amp;\\simeq \\text{Left ()}\\\\\\text{Amber } &amp;\\simeq \\text{Right (Left ())}\\\\\\text{Green } &amp;\\simeq \\text{Right (Right (Left ()}\\\\\\end{align*}$$ The Empty TypeWe add another type, called Void, that has no inhabitants. Because it is empty, there is no way to construct it. We do have a way to eliminate it, however:$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}$$If I have a variable of the empty type in scope, we must be looking at an expression that will never be evaluated. Therefore, we can assign any type we like to this expression, because it will never be executed. Gathering Rules$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ():: ()}\\\\\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2::B}\\space\\space\\space\\space\\space\\space\\space\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$ Removing Terms. . .$$\\frac{\\ulcorner\\vdash\\text{Void}}{\\ulcorner\\vdash P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ()}\\\\\\frac{\\ulcorner\\vdash A}{\\ulcorner\\vdash\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash B}{\\ulcorner\\vdash\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash \\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space A,\\ulcorner\\vdash P\\space \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner\\vdash P}{\\ulcorner\\vdash P}\\\\\\frac{\\ulcorner\\vdash A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash B}{\\ulcorner\\vdash(A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash A}\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash B}\\\\\\frac{\\ulcorner\\vdash A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash A}{\\ulcorner\\vdash B}\\space\\space\\space\\space\\space\\space\\space\\frac{ A, \\ulcorner\\vdash B}{\\ulcorner\\vdash A\\rightarrow B}$$ This looks exactly like constructive logic! If we can construct a program of a certain type, we have also created a proof of a program The Curry-Howard CorrespondenceThis correspondence goes by many names, but is usually attributed to Haskell Curry and William Howard. It is a very deep result: Programming Logic Types Propositions Programs Proofs Evaluation Proof Simplification It turns out, no matter what logic you want to define, there is always a corresponding Î»-calculus, and vice versa. Î»-calculus Logic Typed Î»-Calculus Constructive Logic Continuations Classical Logic Monads Modal Logic Linear Types, Session Types Linear Logic Region Types Separation Logic TranslatingWe can translate logical connectives to types and back: Types logical connectives Tuples Conjuction($\\land$) Either Disjunction (âˆ¨) Functions Implication () True Void False We can also translate our equational reasoning on programs into proof simplification on proofs! Proof SimplificationAssuming A $âˆ§$ B, we want to prove B $âˆ§$ A. We have this unpleasant proof: Translating to types, we get: Assuming x :: (A, B), we want to construct (B, A). We know that$$\\text{(snd x, snd (fst x, fst x)) = (snd x, fst x)}$$Assuming x :: (A, B), we want to construct (B, A). Back to logic: ApplicationsAs mentioned before, in dependently typed languages such as Agda and Idris, the distinction between value-level and type-level languages is removed, allowing us to refer to our program in types (i.e. propositions) and then construct programs of those types (i.e. proofs). Generally, dependent types allow us to use rich types not just for programming, but also for verification via the Curry-Howard correspondence. CaveatsAll functions we define have to be total and terminating. Otherwise we get an inconsistent logic that lets us prove false things. Most common calculi correspond to constructive logic, not classical ones, so principles like the law of excluded middle or double negation elimination do not hold. Algebraic Type IsomorphismSemiring Structure These types we have defined form an algebraic structure called a commutative semiring Laws for Either and Void: Associativity: Either (Either A B) C $\\simeq$ Either A (Either B C) Identity: Either Void A $\\simeq$ A Commutativity: Either A B $\\simeq$ Either B A Laws for tuples and 1: Associativity: ((A, B), C) $\\simeq$ (A,(B, C)) Identity: ((), A) $\\simeq$ A Commutativity: (A, B) $\\simeq$ (B, A) Combining the two: Distributivity: (A, Either B C) $\\simeq$ Either (A, B) (A, C) Absorption: (Void, A) $\\simeq$ Void What does $\\simeq$ mean here? Itâ€™s more than logical equivalence. Distinguish more things. Isomorphism Two types A and B are isomorphic, written A $\\simeq$ B, if there exists a bijection between them. This means that for each value in A we can find a unique value in B and vice versa. Example: 12data Switch = On Name Int | Off Name Can be simplified to the isomorphic (Name, Maybe Int). Generic Programming Representing data types generically as sums and products is the foundation for generic programming libraries such as GHC generics. This allows us to define algorithms that work on arbitrary data structures. Polymorphism and ParametrictyType QuantifiersConsider the type of fst: 1fst :: (a,b) -&gt; a This can be written more verbosely as: 1fst :: forall a b. (a,b) -&gt; a Or, in a more mathematical notation:$$\\text{fst}::\\forall a\\space b(a, b)\\rightarrow a$$This kind of quantification over type variables is called parametric polymorphism or just polymorphism for short. (Itâ€™s also called generics in some languages, but this terminology is bad) Curry-HowardThe type quantifier âˆ€ corresponds to a universal quantifier âˆ€, but it is not the same as the âˆ€ from first-order logic. Whatâ€™s the difference? First-order logic quantifiers range over a set of individuals or values, for example the natural numbers:$$\\forall x. x+1 &gt;x$$These quantifiers range over propositions (types) themselves. It is analogous to second-order logic, not first-order:$$\\forall A.\\forall B\\space A\\land B \\rightarrow B\\land A\\\\\\forall A.\\forall B\\space (A, B) \\rightarrow (B, A)$$The first-order quantifier has a type-theoretic analogue too (type indices), but this is not nearly as common as polymorphism. Generality A type A is more general than a type B, often written A $\\sqsubseteq$ B, if type variables in A can be instantiated to give the type B. If we need a function of type Int â†’ Int, a polymorphic function of type âˆ€a. a â†’ a will do just fine, we can just instantiate the type variable to Int. But the reverse is not true. This gives rise to an ordering. Example$$\\text{Int}\\rightarrow\\text{Int} \\sqsupseteq \\forall z. z\\rightarrow z\\sqsupseteq \\forall x\\space y. x\\rightarrow y\\sqsupseteq\\forall a. a$$ Constraining ImplementationsHow many possible total, terminating implementations are there of a function of the following type? Many$$\\text{Int}\\rightarrow\\text{Int}$$How about this type? 1$$\\forall a. a\\rightarrow a$$ Polymorphic type signatures constrain implementations. Parametricity The principle of parametricity states that the result of polymorphic functions cannot depend on values of an abstracted type. More formally, suppose I have a polymorphic function g that is polymorphic on type a. If run any arbitrary function f :: a â†’ a on all the a values in the input of g, that will give the same results as running g first, then f on all the a values of the output. Example:$$foo :: âˆ€a. [a] â†’ [a]$$We know that every element of the output occurs in the input. The parametricity theorem we get is, for all f :$$\\text{foo }\\circ(\\text{map f}) = (\\text{map f})\\circ \\text{foo}$$ $$head :: âˆ€a. [a] â†’ a\\\\\\text{ f (head l) = head (map f l)}$$ $$(++) :: âˆ€a. [a] â†’ [a] â†’ [a]\\\\\\text{map f (a ++ b) = map f a ++ map f b}$$ $$concat :: âˆ€a. [[a]] â†’ [a]\\\\\\text{map f (concat ls) = concat (map (map f ) ls)}$$ Higher Order Functions$$filter :: âˆ€a. (a â†’ Bool) â†’ [a] â†’ [a]\\\\\\text{filter p (map f ls) = map f (filter (p â—¦ f ) ls)}$$ Parametricity TheoremsFollow a similar structure. In fact it can be mechanically derived, using the relational parametricity framework invented by John C. Reynolds, and popularised by Wadler in the famous paper, â€œTheorems for Free!â€1 . Upshot: We can ask lambdabot on the Haskell IRC channel for these theorems.","link":"/2020/08/01/Haskell/"}],"tags":[{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"IR","slug":"IR","link":"/tags/IR/"},{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"spin","slug":"spin","link":"/tags/spin/"},{"name":"concurrency","slug":"concurrency","link":"/tags/concurrency/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"notes","slug":"notes","link":"/tags/notes/"},{"name":"record","slug":"record","link":"/tags/record/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"Haskell","slug":"Haskell","link":"/tags/Haskell/"},{"name":"Functional Programming","slug":"Functional-Programming","link":"/tags/Functional-Programming/"}],"categories":[{"name":"notes","slug":"notes","link":"/categories/notes/"},{"name":"æ•™ç¨‹","slug":"æ•™ç¨‹","link":"/categories/%E6%95%99%E7%A8%8B/"},{"name":"Paper Review","slug":"Paper-Review","link":"/categories/Paper-Review/"},{"name":"projects","slug":"projects","link":"/categories/projects/"}]}