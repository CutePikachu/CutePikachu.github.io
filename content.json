{"pages":[{"title":"Projects","text":"My journey to the Computer Science World starts in July 2018. The followings are the projects I have at least made a fair amount of contribution. Some of them might not be open sourced, but they will shine and work as expected in the future. 2020.7 - Now awesome-unswLink: https://github.com/UNSWEEB/awesome-unswJust to collect class notes of UNSW computer science courses for students. Looking for more notes. We are welcome and seeking for contribution. 2020.7 - Now XYZSASStudent Afair System using Aliyun table storage and function compute. 2020.7 - Now AauthWeb link: https://aauth.link/Repo link: https://github.com/yzITI/Aauth“Auth with Anything: Fourth Party Login Service.” The goal is to make it is easier for other web developers to use third party login service. I have only contributed to front-end page yet. 2020.4 - Now AcewordsLink: https://acewords.topAn web app to help Chinese memorize foreign language words.This one is actually version2.0. I have made more than half of its backend(written in NodeJs) and about one third of its front-end(still VueJs).My journey to the backend world starts now. 2019.12 - 2020.2 YZSA-feStudent Affair system for Yangzhou Highschool of Jiangsu Province front-end.I have paticipated in this project which is currently using by Yangzhou Highschool of Jiangsu Province. This project is not open sourced and I won’t provide the link to the website because it requires students or staff account to login. 2019.9 - Now YZZX-techWebsite link: https://yzzx.techRepo link(deprecated): https://github.com/CutePikachu/yzzx-tech-deprecated-Repo link(in use): https://github.com/yzITI/yzzx-techThis is the web page for ITI(website language in Chinese).The deprecated version is not fully written by me, though I have made it suitable to view in phone. In the current version, I have written some pages(in about early 2020). 2019.7 Little littersLink: https://github.com/dsksi/cse-2019-hackathonThis was made during UNSW(university of New South Wales) cse hackathon, and our group got 3rd prize(3/40). The project itself was not complicated but it was impressive because we made it in 24 hours and only part of our team actually had some web programming experience. I wrote few components or functions in react and this was my first time coding using ReactJs. 2019.6 Building MazeLink: https://github.com/CutePikachu/BuildingMaze-FEIt is a maze game built using vue.This project was somewhat incomplete and can be inproved. However, I don’t want to change it and have decided to leave it like that.This is the project opening the door of front-end development for me. Guided by Phantomlsh.","link":"/projects/index.html"}],"posts":[{"title":"Foundation of Concurrency","text":"Gain more insight about concurrency. Semantics#mermaid-1597051225361 .label{font-family:'trebuchet ms', verdana, arial;color:#333}#mermaid-1597051225361 .node rect,#mermaid-1597051225361 .node circle,#mermaid-1597051225361 .node ellipse,#mermaid-1597051225361 .node polygon{fill:#ECECFF;stroke:#9370db;stroke-width:1px}#mermaid-1597051225361 .node.clickable{cursor:pointer}#mermaid-1597051225361 .arrowheadPath{fill:#333}#mermaid-1597051225361 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1597051225361 .edgeLabel{background-color:#e8e8e8}#mermaid-1597051225361 .cluster rect{fill:#ffffde !important;stroke:#aa3 !important;stroke-width:1px !important}#mermaid-1597051225361 .cluster text{fill:#333}#mermaid-1597051225361 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:'trebuchet ms', verdana, arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1597051225361 .actor{stroke:#ccf;fill:#ECECFF}#mermaid-1597051225361 text.actor{fill:#000;stroke:none}#mermaid-1597051225361 .actor-line{stroke:grey}#mermaid-1597051225361 .messageLine0{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1597051225361 .messageLine1{stroke-width:1.5;stroke-dasharray:'2 2';stroke:#333}#mermaid-1597051225361 #arrowhead{fill:#333}#mermaid-1597051225361 #crosshead path{fill:#333 !important;stroke:#333 !important}#mermaid-1597051225361 .messageText{fill:#333;stroke:none}#mermaid-1597051225361 .labelBox{stroke:#ccf;fill:#ECECFF}#mermaid-1597051225361 .labelText{fill:#000;stroke:none}#mermaid-1597051225361 .loopText{fill:#000;stroke:none}#mermaid-1597051225361 .loopLine{stroke-width:2;stroke-dasharray:'2 2';stroke:#ccf}#mermaid-1597051225361 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1597051225361 .noteText{fill:black;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:14px}#mermaid-1597051225361 .activation0{fill:#f4f4f4;stroke:#666}#mermaid-1597051225361 .activation1{fill:#f4f4f4;stroke:#666}#mermaid-1597051225361 .activation2{fill:#f4f4f4;stroke:#666}#mermaid-1597051225361 .section{stroke:none;opacity:0.2}#mermaid-1597051225361 .section0{fill:rgba(102,102,255,0.49)}#mermaid-1597051225361 .section2{fill:#fff400}#mermaid-1597051225361 .section1,#mermaid-1597051225361 .section3{fill:#fff;opacity:0.2}#mermaid-1597051225361 .sectionTitle0{fill:#333}#mermaid-1597051225361 .sectionTitle1{fill:#333}#mermaid-1597051225361 .sectionTitle2{fill:#333}#mermaid-1597051225361 .sectionTitle3{fill:#333}#mermaid-1597051225361 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1597051225361 .grid .tick{stroke:#d3d3d3;opacity:0.3;shape-rendering:crispEdges}#mermaid-1597051225361 .grid path{stroke-width:0}#mermaid-1597051225361 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1597051225361 .task{stroke-width:2}#mermaid-1597051225361 .taskText{text-anchor:middle;font-size:11px}#mermaid-1597051225361 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1597051225361 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1597051225361 .taskText0,#mermaid-1597051225361 .taskText1,#mermaid-1597051225361 .taskText2,#mermaid-1597051225361 .taskText3{fill:#fff}#mermaid-1597051225361 .task0,#mermaid-1597051225361 .task1,#mermaid-1597051225361 .task2,#mermaid-1597051225361 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1597051225361 .taskTextOutside0,#mermaid-1597051225361 .taskTextOutside2{fill:#000}#mermaid-1597051225361 .taskTextOutside1,#mermaid-1597051225361 .taskTextOutside3{fill:#000}#mermaid-1597051225361 .active0,#mermaid-1597051225361 .active1,#mermaid-1597051225361 .active2,#mermaid-1597051225361 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1597051225361 .activeText0,#mermaid-1597051225361 .activeText1,#mermaid-1597051225361 .activeText2,#mermaid-1597051225361 .activeText3{fill:#000 !important}#mermaid-1597051225361 .done0,#mermaid-1597051225361 .done1,#mermaid-1597051225361 .done2,#mermaid-1597051225361 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1597051225361 .doneText0,#mermaid-1597051225361 .doneText1,#mermaid-1597051225361 .doneText2,#mermaid-1597051225361 .doneText3{fill:#000 !important}#mermaid-1597051225361 .crit0,#mermaid-1597051225361 .crit1,#mermaid-1597051225361 .crit2,#mermaid-1597051225361 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1597051225361 .activeCrit0,#mermaid-1597051225361 .activeCrit1,#mermaid-1597051225361 .activeCrit2,#mermaid-1597051225361 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1597051225361 .doneCrit0,#mermaid-1597051225361 .doneCrit1,#mermaid-1597051225361 .doneCrit2,#mermaid-1597051225361 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1597051225361 .doneCritText0,#mermaid-1597051225361 .doneCritText1,#mermaid-1597051225361 .doneCritText2,#mermaid-1597051225361 .doneCritText3{fill:#000 !important}#mermaid-1597051225361 .activeCritText0,#mermaid-1597051225361 .activeCritText1,#mermaid-1597051225361 .activeCritText2,#mermaid-1597051225361 .activeCritText3{fill:#000 !important}#mermaid-1597051225361 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1597051225361 g.classGroup text{fill:#9370db;stroke:none;font-family:'trebuchet ms', verdana, arial;font-size:10px}#mermaid-1597051225361 g.classGroup rect{fill:#ECECFF;stroke:#9370db}#mermaid-1597051225361 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1597051225361 .classLabel .box{stroke:none;stroke-width:0;fill:#ECECFF;opacity:0.5}#mermaid-1597051225361 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1597051225361 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1597051225361 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #compositionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #aggregationStart{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #aggregationEnd{fill:#ECECFF;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #dependencyStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #dependencyEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 #extensionEnd{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1597051225361 .commit-id,#mermaid-1597051225361 .commit-msg,#mermaid-1597051225361 .branch-label{fill:lightgrey;color:lightgrey} #mermaid-1597051225361 { color: rgb(0, 0, 0); font: normal normal 400 normal 16px / normal \"Times New Roman\"; }PseudocodeCCSLTSKSbehavioursLTL Notes Notes made by Liam. To use as a guide for revision. Bridging the GapWe have seen several semantic models for concurrent systems in this course. Initially, when we introduced Linear Temporal Logic, we viewed concurrent processes as sets of behaviours, where behaviours were infinite sequences of states. This model is very nice for specifying the semantics of LTL, which we did in Week 1. Later on, though, we used different semantic models, such as Transition Diagrams and Ben-Ari’s pseudocode. We introduced the Calculus of Communicating Systems (CCS), a type of process algebra, to construct Labelled Transition Systems, analogous to our transition diagrams. This was intended to give us a formal language that is semantically connected to our diagrammatic notation. There is a missing link, however, between these two semantic notions: The world of CCS and labelled transition systems on one hand, and the world of behaviours and LTL properties on the other. We examined the two CCS processes $a ⋅ (b + c)$ and $a ⋅ b + a ⋅ c$ and concluded that LTL would not be able to distinguish these two processes, even though they are not considered equal (bisimilar) in CCS. If we were to try to add the equation $a⋅(P + Q) = a ⋅ P + a ⋅ Q$ into CCS, then we would identify those two processes above, but we would also identify $a ⋅ b + a$ with $a ⋅ b$, and clearly one of these satisfies $◊b$ and the other doesn’t! This is because the semantic equivalence we get by adding that equation is called partial trace equivalence. As mentioned in lectures, partial trace equivalence is like looking at all the possible sequences of actions (traces) in the process, including those sequences that do not take available transitions. So, the traces of $a⋅b+a$ are $ϵ$ (the empty trace), $a$, and $ab$ – exactly the same as the partial traces of $a⋅b$. Because partial trace equivalence includes these incomplete traces, using it is basically giving up on the vital progress assumption. Progress is the assumption that a process will take an action if one is available. To get the set of behaviours we expect for our LTL semantics, we need some way to choose which traces we want to keep and which ones we don’t. This is the definition of what counts as a completed trace. There are many different completeness criteria. Progress is the weakest, and the most common. Other completeness criteria include weak and strong fairness, which we have seen already, and justness which I will describe below. ProgressA simple way to view progress as a completeness criteria is that it says a path is complete iff it is infinite or if it ends in a state with no outgoing transitions. This rules out all paths which end in states where they could take an outgoing transition but don’t. Weak FairnessWeak and strong fairness are defined in terms of tasks. A task is a set of transitions. Weak fairness as a property of paths can be expressed by the LTL formulas$$◻(◻(enabled(t))⇒◊(taken(t)))$$for each task tt. Here enabled(t)enabled(t) is an atomic proposition that is true if a transition in tt is enabled. Likewise taken(t)taken(t) holds if tt is taken in that state. Given a process $P=a⋅P+b$, weak fairness would state that eventually $b$ must occur, as we would not be able to loop infinitely around $a$, never taking the (forever enabled) $b$ transition. Viewed as a completeness criterion, weak fairness rules out all traces which do not obey the property above. Note that it implies progress, because any trace that will eventually take a forever enabled transition will surely not be able to sit in a state without taking an available transition. Strong FairnessStrong fairness is similar to weak fairness, except that the property has one extra operator:$$◻(◻◊(enabled(t))⇒◊(taken(t))))$$This is saying that our task tt does not have to be forever enabled, just enabled infinitely often. This means a process can go away and come back. For example, the process $P=a⋅a⋅P+b$ would not eventually do $b$ under weak fairness, as after one $a$ the $b$ transition is no longer enabled. However because the infinite aa path has $b$ available infinitely often, strong fairness would require that $b$ is eventually taken. Viewed as a completeness criterion, this rules out all properties that don’t obey the property above. It can be proven in LTL that strong fairness implies weak fairness. JustnessJustness is a slightly harder concept to formalise. It is weaker than both strong and weak fairness, but stronger than progress. Essentially, it is a criterion that says that individual components should be able to make progress on their own. For example, imagine a system that consists of three components, one which always does aa, another which always does bb, and another which synchronises with aa: $A=a⋅A$ $B=b⋅B$ $C=\\bar a⋅A$ System$=A|B|C$ Now, a valid trace of this system would be simply $bbbbb⋯$ forever. This would satisfy progress, as a transition is always being taken. However, the actions $a$ and $\\bar a$ that could occur, and the communication $τ$ transition that could between $A$ and $C$ are never taken in this trace. Justness says that each component of the system will always make progress if their resources are available. In other words, $B$ doing unrelated $b$ moves should never prevent $A$ and $C$ from communicating. Viewed as a completeness criterion, this is stronger than progress, as it is essentially the same as progress except applied locally, rather than globally. It is weaker than weak fairness, as justness would not require that $A=a⋅A+b$ eventually does a $b$, as all transitions are from the same component ($A$). Kripke StructuresThese traces that we have examined are still sequences of actions, not states. Behaviours, however, are sequences of states. Normally, to convert our labelled transition systems into something we can reason about in LTL, we first translate them into an automata called a Kripke Structure. A Kripke structure is a 4-tuple $(S,I,↦,L)$ which contains a set of states $S$, an initial state $I$, a transition relation $↦$ which, unlike a labelled transition system, does not have any action labels on the transitions, and a labelling function $L$ which associates to every state $S$ a (set of) atomic propositions – these are the atomic propositions we use in our LTL formulae. A Kripke structure for an OS process behaviour was actually shown in the lecture on temporal logic in Week 1, I just never told you it was a Kripke structure. Kripke Structures deal with states, not transition actions. This means, to translate from a labelled transition system to a Kripke structure, we need a way to move labels from transitions to states. The simplest translation is due to de Nicola and Vaandrager, where each transition $s_i\\xrightarrow a s_j$ in the LTS is split into two in the Kripke Structure: $s_i→X$ and $X→s_j$, where $X$ is a new state that is labelled with $a$. Because this converts existing LTS locations into blank, unlabelled states in the Kripke structure, this introduces problems with the next state operator in LTL. For this reason (and others) we usually consider LTL without the next state operator in this field. Normally with LTL, we require that Kripke structures have no deadlock states, that is, states with no outgoing transitions. The usual solution here is to add a self loop to all terminal states. We can extract our normal notion of a behaviour by using the progress completeness criterion. Because of the restriction above, the progress criterion is equivalent to examining only the infinite runs of the automata. Putting it all togetherNow we can: Translate Pseudocode to CCS or transition diagrams Translate transition systems to Kripke structures Extract behaviours from Kripke structures using various completeness criteria Specify LTL properties about those behaviours Now we have connected all the semantic models used in the course. PseudocodeCalculus of Communicating Systems(CCS)The Calculus of Communicating Systems: Is a process algebra, a simple formal language to describe concurrent systems. Is given semantics in terms of labelled transition systems. Was developed by Turing-award winner Robin Milner in the 1980s Has an abstract view of synchronization that applies well to message passing. Processes Processes in CCS are defined by equations: The equation:$$\\textbf{CLOCK} = \\text{tick}$$defines a process CLOCK that simply executes the action “tick” and then terminates. This process corresponds to the first location in this labelled transition system (LTS):$$\\bullet\\xrightarrow{\\text{tick}}\\bullet$$An LTS is like a transition diagram, save that our transitions are just abstract actions and we have no initial or final location. Action PrefixingIf a is an action and $P$ is a process, then $x.P$ is a process that executes $x$ before $P$. This brackets to the right, so:$$x.y.z.P = x.(y.(z.P))$$Example$$\\textbf{CLOCK}_2 = \\text{tick.tock}$$defines a process called CLOCK$_2$ that executes the action “tick” then the action “tock” and then terminates$$\\bullet\\xrightarrow{\\text{tick}}\\bullet\\xrightarrow{\\text{tock}}\\bullet$$The process:$$\\textbf{CLOCK}_3 = \\text{tock.tick}$$has the same actions as CLOCK$_2$ but arranges them in another order. StoppingMore precisely, we should write:$$\\textbf{CLOCK}_2 = \\text{tick.tock.}\\textbf{STOP}$$where STOP is the trivial process with no transitions. LoopsUp to now, all processes make a finite number of transitions and then terminate. Processes that can make a infinite number of transitions can be pictured by allowing loops: CLOCK$_4 =$ tick.CLOCK$_4$ CLOCK$5_ =$ tick.tick,CLOCK$_5$ We accomplish loops in CCS using recursion. Equality of ProcessesThese two processes(CLOCK$_4$ and CLOCK$_5$) are physically different: But they both have the same behaviour — an infinite sequence of “tick” transitions. (Informal definition) We consider two process to be equal if an external observer cannot distinguish them by their actions. We will refine this definition later. Choice If $P$ and $Q$ are processes then $P + Q$ is a process which can either behave as the process $P$ or the process $Q$. Choice Equalities$$\\begin{align*}&amp;P + (Q + R) &amp;= \\space&amp;(P + Q) + R &amp;(\\text{associativity})\\\\&amp;P + Q &amp;= \\space&amp;Q + P &amp;(\\text{commutativity})\\\\&amp;P + STOP &amp;= \\space&amp;P &amp;(\\text{neutral element})\\\\&amp;P + P &amp;= \\space&amp;P &amp;(\\text{idempotence})\\end{align*}$$ What about the equation:$$a.(P + Q)=^?(a.P) + (a.Q)$$ Branching TimeExmaple: $\\textbf{VM}_1 = \\text{in}50¢.(\\text{outCoke + outPepsi})$ $ \\textbf{VM}_2 = (\\text{in}50¢.\\text{outCoke}) + (\\text{in}50¢.\\text{outPepsi})$ Reactive Systems VM$_1$ allows the customer to choose which drink to vend after inserting 50¢. In VM$_2$ however, the machine makes the choice when the customer inserts a coin. They different in this reactive view, but they have the same behaviours! EquivalencesThe equation$$a.(P + Q) = (a.P) + (a.Q)$$is usually not admitted for this reason. If we do admit it, then our notion of equality is very coarse (it is called partial trace equivalence). This is enough if we want to prove safety properties, but progress is not guaranteed. Our notion of equality without this equation is called (strong) bisimulation equivalence or (strong) bisimilarity. Parallel Composition If $P$ and $Q$ are processes then $P | Q$ is the parallel composition of their processes — i.e. the non-deterministic interleaving of their actions $$\\textbf{ACLOCK}=\\text{tick.beep | tock}$$ SynchronizationIn CCS, every action a has an opposing coaction $\\bar a$ (and $\\bar{\\bar a}$ = a): It is a convention to think of an action as an output event and a coaction as an input event. If a system can execute both an action and its coaction, it may execute them both simultaneously by taking an internal transition marked by the special action $τ$ . Expansion TheoremLet $P$ and $Q$ be processes. By expanding recursive definitions and using our existing equations for choice we can express $P$ and $Q$ as n-ary choices of action prefixes:$$P =\\sum_{i∈I}\\alpha_i. P_i \\text{ and } Q =\\sum_{j∈J}β_j. Q_j$$Then, the parallel composition can be expressed as follows:$$P | Q =\\sum_{i∈I}αi.(P_i| Q) +\\sum_{j∈J}β_j.(P | Q_j) + \\sum_{i∈I, j∈J, α_i=\\barβ_j}τ.(P_i| Q_j)$$From this, many useful equations are derivable:$$\\begin{align*}&amp;P|Q &amp;= \\space&amp;Q|P\\\\&amp;P|(Q|R) &amp;=\\space &amp;(P|Q)|R\\\\&amp;P|\\text{STOP} &amp;=\\space &amp;P\\\\\\end{align*}$$ Restriction If $P$ is a process and a is an action (not $τ$ ), then $P \\ a$ is the same as the process $P$ except that the actions a and a may not be executed. We have $(a.P)$ \\ $ b = a.(P $\\ $ b) \\text{ if } a \\not\\in {b, \\bar b}$ Example:$$\\begin{align}&amp;\\textbf{CLOCK}_4 &amp;= \\space&amp;\\text{tick}\\textbf{.CLOCK}_4\\\\&amp;\\textbf{MAN} &amp;=\\space &amp;\\bar {\\text{tick}}\\text{.eat.}\\textbf{MAN} \\\\&amp;\\textbf{EXAMPLE} &amp;=\\space &amp;(\\textbf{MAN|CLOCK}_4)\\text{\\ tick}\\\\\\end{align}$$ SemanticsUp until now, our semantics were given informally in terms of pictures. Now we will formalise our semantic intuitions. Our set of locations in our labelled transition system will be the set of all CCS processes. Locations can now be labelled with what process they are: We will now define what transitions exist in our LTS by means of a set of inference rules. This technique is called operational semantics. Inference RulesIn logic we often write:$$\\frac{A_1, A_2, … A_n}{C}$$To indicate that C can be proved by proving all assumptions A1 through An. For example, the classical logical rule of modus ponens is written as follows:$$\\frac{A\\Rightarrow B\\space\\space\\space\\space\\space\\space A}{B}\\text{Modus Ponens}$$ Operational Semantics$$\\frac{}{a.P\\xrightarrow{a} P}\\text{ACT}\\space\\space\\space\\space\\space\\frac{P\\xrightarrow{a} P’}{P+Q\\xrightarrow{a} P’}\\text{CHOICE}_1\\space\\space\\space\\space\\space\\frac{Q\\xrightarrow{a} Q’}{P+Q\\xrightarrow{a} Q’}\\text{CHOICE}_2$$ $$\\frac{P\\xrightarrow{a} P’}{P|Q\\xrightarrow{a} P’|Q}\\text{PAR}_1\\space\\space\\space\\space\\space\\frac{Q\\xrightarrow{a} Q’}{P|Q\\xrightarrow{a} P|Q’}\\text{PAR}_2\\space\\space\\space\\space\\space\\frac{P\\xrightarrow{a} P’\\space\\space\\space Q\\xrightarrow{a} Q’}{P|Q\\xrightarrow{τ} P’|Q’}\\text{SYNC}$$ $$\\frac{P\\xrightarrow{a} P’\\space\\space\\space a\\not\\in{b,\\bar b}}{P\\text{ \\ }b\\xrightarrow{a} P’\\text{ \\ }b}\\text{RESTRICT}$$ Bisimulation Equivalence Two processes (or locations) P and Q are bisimilar iff they can do the same actions and those actions themselves lead to bisimilar processes. All of our previous equalities can be proven by induction on the semantics here. Proof TreesThe advantages of this rule presentation is that they can be “stacked” to give a neat tree like derivation of proofs. Value PassingWe introduce synchronous channels into CCS by allowing actions and coactions to take parameters.$$\\begin{align}&amp;\\text{Actions:}&amp;a(3)\\space\\space\\space\\space &amp;c(15) &amp;x(True)…\\&amp;\\text{Coactions:}&amp;\\bar a(x)\\space\\space\\space\\space &amp;\\bar c(y) &amp;\\bar c(z)…\\end{align}$$The parameter of an action is the value to be sent, and the parameter of a coaction is the variable in which the received value is stored. Example A one-cell sized buffer is implemented as:$$\\textbf{BUFF}=\\bar{\\text{in}}(x).\\text{out}(x).\\textbf{BUFF}$$Larger buffers can be made by stitching multiple BUFF processes together! This is how we model asynchronous communication in CCS. Merge and GuardsGuard: If $P$ is a value-passing CCS process and $ϕ$ is a formula about the variables in scope, then $[ϕ]P$ is a process that executes just like $P$ if $ϕ$ is holds for the current state and like STOP otherwise We can define an if statement like so:$$\\textbf{if } ϕ \\textbf{ then } P \\textbf{ else } Q≡ ([ϕ].P) + ([¬ϕ].Q)$$ Assignment If $P$ is a process and $x$ is a variable in the state, and $e$ is an expression, then $[\\![x := e]\\!]$ $P$ is is the same as $P$ except that it first updates the variable $x$ to have the value $e$ before making a transition. Some presentations of value passing CCS also include assignment to update variables in the state. With this, our value-passing CCS is now just as expressive as Ben-Ari’s pseudocode. Moreover, the connection between CCS and transition diagrams is formalised, enabling us to reason symbolically about processes rather than semantically. Process AlgebraThis is an example of a process algebra. There are many such algebras and they have been very influential on the design of concurrent programming languages. Kripke Structures(KS)Linear Temporal Logic(LTL)Logic A logic is a formal language designed to express logical reasoning. Like any formal languages, logics have a syntax and semantics(meaning of the value). Example: Proposition Logic Syntax A set of atomic propositions $P = {a, b, c, …}$ An inductively defined set of formulae: Each $p \\in P$ is a formula If $P$ and $Q$ are formulae, then $P \\land Q$ is a formula If $P$ is a formula, then $\\neg P$ is a formula (Other connectives(like or) are just sugar for these, so we omit them) Sematics Semantics are a mathematical representation of the meaning of a piece of syntax. There are many ways of giving a logic semantics, but we will use models. Example (Propositional Logic Semantics) A model for propositional logic is a valuation $V ⊆ P$, a set of “true” atomic propositions. We can extend a valuation over an entire formula, giving us a satisfaction relation:$$\\begin{align} V ⊨ p &amp;⇔ p \\in V\\\\ V ⊨ \\varphi \\land \\psi &amp;⇔ V ⊨ \\varphi\\text{ and }V ⊨ \\psi\\\\ V ⊨ \\neg \\varphi &amp;⇔ V |\\ne \\varphi\\end{align}$$We read $V ⊨ φ$ as $V$ “satisfies” $φ$. LTL Linear temporal logic (LTL) is a logic designed to describe linear time properties. Linear temporal logic syntax We have normal propositional operators: $p ∈ P$ is an LTL formula. If $ϕ$, $ψ$ are LTL formulae, then $ϕ ∧ ψ$ is an LTL formula. If $ϕ$ is an LTL formula, $¬ϕ$ is an LTL formula. We also have modal or temporal operators: If $ϕ$ is an LTL formula, then $\\circ ϕ$ is an LTL formula. The circle is read as ‘next’ If $ϕ$, $ψ$ are LTL formulae, then $ϕ U ψ$ is an LTL formula. The U is read as until. LTL Semanticslet $\\sigma = \\sigma_0\\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5…$ be a behavior. Then define the notation: $\\sigma|_0 = \\sigma$ $\\sigma|_1 = \\sigma_1\\sigma_2\\sigma_3\\sigma_4\\sigma_5$ $\\sigma|_{n+1} = (\\sigma|_1)|_n$ Semantics The models of LTL are behaviours. For atomic propositions, we just look at the first state. We often identify states with the set of atomic propositions they satisfy.$$\\begin{align}&amp;\\sigma \\vDash p &amp;\\Leftrightarrow\\space &amp;p\\in \\sigma_0\\\\&amp;\\sigma \\vDash \\varphi\\land\\psi &amp;\\Leftrightarrow \\space&amp;\\sigma \\vDash \\varphi\\space \\text{and} \\space \\sigma\\vDash\\psi\\\\&amp;\\sigma \\vDash \\neg\\varphi &amp;\\Leftrightarrow \\space&amp;\\sigma \\not\\vDash \\varphi\\\\&amp;\\sigma \\vDash \\bigcirc\\varphi &amp;\\Leftrightarrow\\space &amp;\\sigma_1 \\vDash \\varphi\\\\&amp;\\sigma \\vDash \\varphi U\\psi &amp;\\Leftrightarrow\\space &amp;\\text{There exists an $i$ such that} \\text{$\\sigma_i\\vDash\\varphi$ and for all $j&lt;i$, $\\sigma|_j\\vDash\\varphi$}\\end{align}$$We say $P\\vDash\\varphi$ iff $\\forall\\sigma\\in[\\![P]\\!]$. $\\sigma\\vDash\\varphi$. Derived Operators The operator $\\Diamond\\varphi$(“finally” or “eventually”) says that $\\varphi$ will be true at some point. The operator$\\square\\varphi$(“global” or “always”) says that $\\varphi$ is always true from now on. Fairness The fairness assumption means that if a process can always make a move, it will eventually be scheduled to make that move. Expressing Fairness in LTLWeak fairness for action $π$ is then expressible as:$$\\square(\\square \\text{enabled}(π) ⇒ \\Diamond\\text{taken}(π))$$Strong fairness for action π is then expressible as:$$\\square(\\square\\Diamond\\text{enabled}(π) ⇒ \\Diamond\\text{taken}(π))$$ Critical SectionsDesiderata We want to ensure two main properties and two secondary ones: Mutual Exclusion No two processes are in their critical section at the same time. Eventual Entry (or starvation-freedom) Once it enters its pre-protocol, a process will eventually be able to execute its critical section. Absence of Deadlock The system will never reach a state where no actions can be taken from any process. Absence of Unneccessary Delay If only one process is attempting to enter its critical section, it is not prevented from doing so. Eventual Entry is liveness, the rest are safety. Dekker’s algorithm Dekker’s algorithm works well except if the scheduler pathologically tries to run the loop at q3 · · · q7 when turn = 2 over and over rather than run the process p (or vice versa). With fairness assumption, Dekker’s algorithm is correct.","link":"/2020/08/09/Foundation%20of%20Concurreny/"},{"title":"Mac装spin和ispin","text":"翻了很久也没找到Mac的安装教程(可能是太简单了)， 安装spin这个就很简单了，感谢brew, 1brew install spin 安装ispin去https://github.com/nimble-code/Spin下载，可以直接打包或者git clone。其实ispin只是一个文件，在 ‘optional_gui’这个folder里（不确定只有这个文件是不是就可以直接运行）。然后我们进入ispin文件所在的目录。 1cd Spin/optional_gui 然后在命令行输入 1wish -f ispin.tcl 就可以运行了。","link":"/2020/06/01/Mac%E8%A3%85spin%E5%92%8Cispin/"},{"title":"【双剑合璧】Git和Github使用教程","text":"是否有遇到过写着写着想回到之前版本，却又不记得具体实现；又或是想和队友共享代码，每次修改发送文件；抑或是想换个电脑写却不想用email等搬运全部文件这类烦不胜烦类似的问题，伟大的程序员们自然早就为我们造好了轮子，那就是版本控制的利器——Git以及cloud based Github。本教程也主要讲Git与Github的使用。 1. 背景介绍本来想粗暴写一下安装使用教程，想了想还是先写一点背景介绍，不感兴趣可以直接跳过。 版本控制首先我们需要了解一个概念——Version Control，也就是版本控制。当我们写代码的时候总会有意无意制造出一些bug，有时候我们会想返回前一次没有问题的时候，因此这就是我们为什么需要版本控制。 简单来说，版本控制就是在不同时间节点保存你的程序，然后你可以通过它回看甚至回到之前保存的版本。 什么是GitGit是在2005年的时候初次开发出来的版本控制利器，并风靡全球。Git是一个安装并管理本地系统的工具而且可以给你提供现有文件的你保存的不同版本。因为是本地的，下载之后就不再需要网络也可以使用。 什么是GithubGithub有一点像可视化的Git，并且是在线的服务。让你可以在线管理你的Git仓库（这个具体会在后面讲）。通过Github，你可以分享你的程序，让其他合作者一起进行编辑。它不仅保存了Git的全部功能，还进行了扩充，它可以让你在任意电脑任意地区访问，只要你有权限。它最大的优点就是它是一个很庞大的数据库，你可以搜索，阅读甚至使用别人写好的程序。当然Github还有很多替代物如Gitlab之类，本文就不赘述了。 Git vs. Github简单的说，Git是一个帮助你管理和追踪本地源码历史的版本控制工具，而Github则是一个基于云端运用Git技术的让你管理Git仓库的服务。 2. Git首先，我们来康康Git的使用。 注意：所有和Git相关的命令都是git开头噢。 安装如果已经安装过，可以跳过。 Linux 如果是Fedora或其他类似的 RPM-based distribution, 譬如RHEL 和 CentOS： 1$ sudo dnf install git-all 或Debian-based distribution像Ubuntu 1$ sudo apt install git-all macOS 可以先试 1$ git --version 如果没有安装，应该会弹出安装请求，也可以去https://git-scm.com/download/mac 下载。 Windows 这个稍微有点烦，见https://git-scm.com/book/en/v2/Getting-Started-Installing-Git 。 新建一个Git仓库安装好之后我们就可以开始用了，Git的一个仓库（Repo）就是你的一个项目。我们首先新建一个文件夹 12$ mkdir firstRepo$ cd firstRepo 初始化一个仓库 1$ git init 这时候会显示这么一行 Initialized empty Git repository in /Users/tina/Desktop/firstRepo/.git/, 后面一部分是你当前仓库的路径，会根据路径不同进行变化。通过.git我们可以知道Git其实是创建了一个隐藏文件夹，所以我们运行ls这个命令，并不会看到有关Git的信息。 添加文件我们依然先在当前目录建文件，文件类型无所谓。这里就用txt文件了。 123$ touch first.txt$ lsfirst.txt 此时我们可以看见文件里有一个文件叫first.txt，但是这个文件并不在我们Git仓库里（划重点），为了看仓库里有什么，我们使用 1234567891011$ git statusOn branch masterInitial commitUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) first.txtnothing added to commit but untracked files present (use \"git add\" to track) 这个命令我们之后具体讲，现在我们看到untracked files这里，这个的意思就是说这些个文件在当前目录下但是没有保存到我们仓库里，所以Git不会对它的改变进行追踪。把文件添加到仓库里，我们使用 1$ git add first.txt 此时first.txt已经被加进去了。如果我们想添加多个文件呢，可以把想加入的文件用空格隔开，写在后面，像这样 1$ git add file1 file2 不过我们还有一个更简单的方法，那就是. 1$ git add . 这样可以把当前目录下全部的没有track的文件都加进来（是不是很方便！） 删除文件提到添加就不得不说删除，如果一个文件不想被跟踪了怎么办呢，那就删掉啦。 1$ git rm file 注意：这个操作不会在当前目录删除文件 一个比较重要的flag --force 顾名思义, 这个flag是用来强制删除文件的。一般来说，如果你的文件没有被commit（下面就讲这个），是不可以被删除的，但是加上这个flag就可以删除了。 “截图”咳，实话实话，我也不知道commit怎么准确翻译成中文。现在我们来看Git非常非常重要的一个命令，commit。它其实就是类似一个截图，存下来你当前的项目完成情况并保存下来，以后可以回顾甚至回到当前节点。至于怎么回到我们后面再讲，现在就来进行“截图”。这个截图只是对于Git追踪的文件，所以它与add是不可分割的。 1234$ git commit -m \"Your message about the commit\"[master (root-commit) b345d9a] This is my first commit! 1 file changed, 1 insertion(+) create mode 100644 first.txt 这样就建了一个新的commit啦，看到这个b345d9a东西了么，这个是一个commit id。但是如果你对文件进行了修改，又运行了这个语句，你会看到这么一行Already Up to date. Bug？Nonono， 这就是需要我们add来参与了。因为在Git心里，你的文件还是上一次add来的，他一看，文件和上一次commit的没有变化呀，不截图不截图。所以我们要重新add一遍，这个时候的add就是一个更新的操作了，我们每次commit之前都要先add再commit。 不过，这两步某些情况下是可以合并的，变成 1$ git commit -am \"Your message about the commit\" 这样做就是更新我跟踪的所有文件并进行现有成果截图，但是如果你新建的文件还是要用单独的add来进行添加哦。 注意：这个语句千万不要瞎写哦，以后你可能会用到 分支虽然我一直刻意没有提到上面出现过几次的一个词master，它是什么呢，它是我们的主分支。想象一棵树，它就是我们的树干。分支的作用是当你想修改某个部分的代码，添加新功能，但却不想影响之前写好的代码，就可以分出一个branch，在上面进行。不同分支之间不会相互干扰，除非你进行合并等操作。 我们初始化一个Git仓库时，主分支就已经存在，在此基础上，我们可以新建自己的branch。使用 1234$ git branch branchName$ git branch* master branchName 第一个命令是新建，第二个是列出全部分支。*表示当前所在分支。切换分支我们使用 1234$ git checkout branchName$ git branch master* branchName 通常来说，我们新建一个分支就是为了切换到新分支，所以我们可以把上两步合二为一 1$ git checkout -b &lt;my branch name&gt; 注意：我们新建分支的内容是和你建分支时所在的一致，要注意新建分支时所在的branch哦，通常情况我们都是在master上加分支。 在进行写代码，修改之后，我们想把分支上内容合并至master，使用 12$ git checkout master$ git merge branchName 这里稍微有一些饶人，我们需要在master上进行merge操作来使得master与分支一致。现在我们来简单讲解下merge。 假设在master上我们有几次commit后，新建一个分支A，几次commit之后，我们想把A合并到master上。合并之前， common base—— — commit —— commit (master) ​ ｜— commit —— commit (A) 合并后 common base—— — commit —— commit (master) —— new merge commit ​ ｜— commit —— commit (A) ——｜ 这个时候master上就有了A上面的内容啦。 merge分为两种，一种是fast forward, 另外一种是3-way merge。 第一种很直接，合并前 common base—— (master) ​ ｜— commit —— commit (A) 合并后 common base—— —— new merge commit ​ ｜— commit —— commit (A) ——｜ 就不细讲Git原理，康图。 第二种其实就是第一个那个图。小朋友，你是否有很多问号，为什么就这样合并起来，而没有冲突。merge的过程很容易产生的问题就是冲突！尽管Git的merge已经很nb了，但依然不可避免产生冲突，但这些我们可以很容易地解决。 冲突这是Git里经常遇到并且要解决的问题！当你merge不同分支时，很容易遇到。为了让大家直观感受，我们创造一个冲突。 1234567891011121314151617$ git checkout master$ echo 'this is conflicted text from master' &gt; first.txt$ git commit -am 'added one line'[master 8cc7111] added one line 1 file changed, 1 insertion(+)$ git checkout branchNameSwitched to branch 'branchName'$ echo 'this is conflicted text from feature branch' &gt; first.txt$ git commit -am 'added one line'[a 23f5790] added one line 1 file changed, 1 insertion(+)$ git checkout masterSwitched to branch 'master'$ git merge branchNameAuto-merging first.txtCONFLICT (content): Merge conflict in first.txtAutomatic merge failed; fix conflicts and then commit the result. YES!!冲突出现了，现在我们要解决它。这个时候打开此文件 1$ vim first.txt vim是最好的text editor！ 我们会看到这个 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADthis is conflicted text from master=======this is conflicted text from feature branch&gt;&gt;&gt;&gt;&gt;&gt;&gt; branchName &lt;&lt;&lt;&lt;&lt;&lt;&lt; 和=======是告诉我们哪里有冲突，并且是哪个branch。就这个而言前第二行是master上内容而第四行则是branchName上的内容，我们选取需要内容后记得删除135行哦！ Vim使用i进行编辑，完成后esc然后打:q推出。这个时候冲突已经解决完啦，我们需要进行一次commit来结束这次的合并操作。 1$ git commit -am 'solve conflict' 其他git status其实算是看当前分支的一个状态吧，可以看到是否有未被追踪的文件，有没有变化没有被commit。使用 1$ git status git log简单粗暴，显示commit记录，使用 1$ git log 会显示出一串commit记录，每个包括id，作者，时间，分支，以及commit的时候的那个message。 切换到某次commit1$ git checkout specific-commit-id 这个commit id就需要我们去git log里面找来，嗯就是那一长串看不懂的hash。至于你具体想切换到哪个，就看你commit的时间以及你自己写的commit信息啦（滑稽）瞎写的话现在就作茧自缚了嘻嘻。 结语Git大致就讲这么多啦，想更了解具体的Git，阔以参考document。https://git-scm.com/doc 3. Github不得不说Github是真的好用，造好的轮子随便用，甚至还可以找到作业答案，当然要自己写作业了！，而且Github推出的Github desktop是真香。使用很简单，无师自通。言归正传，我们来康康Github怎么用。 注册及安装想注册的话你需要准备的东西有：一个或多个邮箱。一个账号可以绑定N个邮箱，然后去https://github.com/ 进行注册。如果是学生且想免费申请Github pro可以访问https://education.github.com/pack 。 申请完之后我们打开命令行 12$ git config --global user.name \"&lt;your_name_here&gt;\"$ git config --global user.email \"&lt;your_email@email.com&gt;\" 注意：写名字时去除括号但保留引号，邮箱使用申请时的邮箱。以及，如果只是想在当前仓库用的话，去掉global就ok了 现在我们连接ssh到Github，这样就可以通过ssh进行clone操作而不是http。先复制ssh key 1$ pbcopy &lt; ~/.ssh/id_rsa.pub 然后去Github网页，点击头像，出现下拉菜单，点击Settings，在用户设置的菜单栏选择SSH and GPG keys，点击New SSH key，在title栏写上你设备的名字，方便你辨认，把刚刚复制好的key复制到key那里。然后点击添加，如果出现输密码框就输入密码。 新建仓库先去Github上建立一个新仓库，点击Repositories旁New按钮，填写Repo的名字后就可以确认建立。新建完后自动跳转到这个初始页面，在这里我们可以看到这个仓库的https地址，点击旁边的复制按钮备用。 连接仓库现在，之前Git里的操作都可以正常用了，不过效果依然存在本地，想要和云端连接，我们需要学一些新操作。首先和remote连接 1$ git remote add origin remote repository URL 这里的URL就是上一步操作复制得到的。然后 1$ git push -u origin master 运行完之后，我们就可以在Github上看到啦。每当我们想要发布一个新的分支的时候，都需要运行这个命令，要将master改成分支的名字即可。 更新当remote和本地的版本不一致时，我们会需要进行更新，可能是本地快于remote也可能是remote快于本地。我们使用git pull来拉取remote的更新，使用前要记得先commit本地的版本哦。如果要更新remote版本，我们直接用git push就可以做到了。 克隆在Github上打开你要克隆的仓库主页，点击Clone or Download，然后可以选择用https还是ssh进行克隆，复制链接，打开命令行 1$ git clone url 如果不是你的仓库，只能使用https哦，或者也可以选择下载。 Github网页既然他是有网页版的，那自然不能忽略网页版提供的服务。 搜索我要说的搜索是在页面顶部Navigation bar里的那个搜索框，在哪里可以进行关键词搜索，能检索到有相关信息的公开仓库。在搜索结果的页面，我们可以选取特定语言（这个项目使用的语言），结果的排序方法等等。还有其他一些信息，就自己去看啦。 每个仓库都会显示stars，一般来说stars越多，认可度越高，也就越好。 关于Repo因为要素过多有很多内容，我就选取部分说明，其他的功能自己康康就好啦。 文件 点到自己的任意一个仓库，我们可以直接在网上编辑文件。点开文件A，然后点击Edit就可以进行修改，修改完成后使用下方的commit进行保存。也可以直接新建文件，上传文件，删除文件等。不过比较神奇的操作是新建文件夹。具体如下： 点击Create New File，在文件名的地方输入文件夹名称并加上/, 神奇的事情发生了，文件夹就建好了。不过不可以建空文件夹哦，所以它会强行让你写一个文件名。 合作 如果想和组员共同编辑一个仓库，我们点击settings，然后Manage Access，这个时候会让你输入一下密码。进去后可以修改当前仓库权限以及邀请合作者，提供输入合作人的Github名字来进行查找和添加。 其他怎么fetch一个云端的分支（不在本地的）？ 1$ git checkout --track origin/daves_branch 如果不想用命令行怎么办？ Github Desktop你值得拥有：https://desktop.github.com/ 4. 写在最后感谢大家看完我的废话教程，这大概是本人多年一年的使用经常用到的部分，希望可以给你们带来帮助。","link":"/2020/03/21/%E3%80%90%E5%8F%8C%E5%89%91%E5%90%88%E7%92%A7%E3%80%91Git%E5%92%8CGithub%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"},{"title":"Haskell and functional programming","text":"Enjoy learning haskell but not only haskell. Haskell IntroductionIn this course we use Haskell, because it is the most widespread language with good support for mathematically structured programming. 12f :: Int -&gt; Boolf x = (x &gt; 0) First $x$ is the input and the RHS of the equation is the Output. CurryingIn mathematics, we treat $log_{10}(x)$ and $log_2 (x)$ and ln(x) as separate functions. In Haskell, we have a single function logBase that, given a number n, produces a function for $log_n(x)$ 12345678910log10 :: Double -&gt; Doublelog10 = logBase 10log2 :: Double -&gt; Doublelog2 = logBase 2ln :: Double -&gt; Doubleln = logBase 2.71828logBase :: Double -&gt; Double -&gt; Double Function application associates to the left in Haskell, so: logBase 2 64 ≡ (logBase 2) 64 Functions of more than one argument are usually written this way in Haskell, but it is possible to use tuples instead… TuplesTuples are another way to take multiple inputs or produce multiple outputs: 1234toCartesian :: (Double, Double) -&gt; (Double, Double)toCartesian (r, theta) = (x, y) where x = r * cos theta y = r * sin theta N.B: The order of bindings doesn’t matter. Haskell functions have no side effects, they just return a result. There’s no notion of time(no notion of sth happen before sth else). Higher Order FunctionsIn addition to returning functions, functions can take other functions as arguments: 12345678twice :: (a -&gt; a) -&gt; (a -&gt; a)twice f a = f (f a)double :: Int -&gt; Intdouble x = x * 2quadruple :: Int -&gt; Intquadruple = twice double Haskell concrete types are written in upper case like Int, Bool, and in lower case if they stand for any type. 12345678{- twice twice double 3 == (twice twice double) 3 == (twice (twice double)) 3 == (twice quadruple) 3 == quadrauple (quadruple 3) == 48-} ListsHaskell makes extensive use of lists, constructed using square brackets. Each list element must be of the same type. 1234[True, False, True] :: [Bool][3, 2, 5+1] :: [Int][sin, cos] :: [Double -&gt; Double][ (3,’a’),(4,’b’) ] :: [(Int, Char)] MapA useful function is map, which, given a function, applies it to each element of a list: 123map not [True, False, True] = [False, True, False]map negate [3, -2, 4] = [-3, 2, -4]map (\\x -&gt; x + 1) [1, 2, 3] = [2, 3, 4] The last example here uses a lambda expression to define a one-use function without giving it a name. What’s the type of map? 1map :: (a -&gt; b) -&gt; [a] -&gt; [b] StringsThe type String in Haskell is just a list of characters: 1type String = [Char] This is a type synonym, like a typedef in C. Thus: &quot;hi!&quot; == ['h', 'i', '!'] Practice Word Frequencies Given a number $n$ and a string $s$, generate a report (in String form) that lists the $n$ most common words in the string $s$. We must: Break the input string into words. Convert the words to lowercase. Sort the words. Count adjacent runs of the same word. Sort by size of the run. Take the first $n$ runs in the sorted list. Generate a report. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import Data.Char(toLower)import Data.List(group,sort,sortBy)breakIntoWords :: String -&gt; [String]breakIntoWords = wordsconvertIntoLowercase :: [[Char]] -&gt; [String]convertIntoLowercase = map (map toLower)sortWords :: [String] -&gt; [String]sortWords = sorttype Run = (Int, String)countAdjacentRuns :: [String] -&gt; [Run]countAdjacentRuns = convertToRuns . groupAdjacentRuns -- [\"hello\",\"hello\",\"world\"] --&gt; [[\"hello\",\"hello\"],[\"world\"]]groupAdjacentRuns :: [String] -&gt; [[String]]groupAdjacentRuns = group-- head :: [a] -&gt; aconvertToRuns :: [[String]] -&gt; [Run]convertToRuns = map (\\ls-&gt; (length ls, head ls))sortByRunSize :: [Run] -&gt; [Run]sortByRunSize = sortBy (\\(l1, w1) (l2, w2) -&gt; compare l2 l1)takeFirst :: Int -&gt; [Run] -&gt; [Run]takeFirst = take generateReport :: [Run] -&gt; StringgenerateReport = unlines . map (\\(l,w) -&gt; w ++ \":\" ++ show l ) -- (\\x -&gt; f x) == fmostCommonWords :: Int -&gt; (String -&gt; String)mostCommonWords n = generateReport . takeFirst n . sortByRunSize . countAdjacentRuns . sortWords . convertIntoLowercase . breakIntoWords Functional CompositionWe used function composition to combine our functions together. The mathematical $(f ◦ g)(x)$ is written $(f . g) x$ in Haskell. In Haskell, operators like function composition are themselves functions. You can define your own! 1234-- Vector addition(.+) :: (Int, Int) -&gt; (Int, Int) -&gt; (Int, Int)(x1, y1) .+ (x2, y2) = (x1 + x2, y1 + y2)(2,3) .+ (1,1) == (3,4) You could even have defined function composition yourself if it didn’t already exist: 12(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; (a -&gt; c)(f . g) x = f (g x) ListsHow were all of those list functions we just used implemented? Lists are singly-linked lists in Haskell. The empty list is written as [] and a list node is written as x : xs. The value x is called the head and the rest of the list xs is called the tail. Thus: 12\"hi!\" == ['h', 'i', '!'] == 'h':('i':('!':[])) == 'h' : 'i' : '!' : [] When we define recursive functions on lists, we use the last form for pattern matching: 123map :: (a -&gt; b) -&gt; [a] -&gt; [b]map f [] = []map f (x:xs) = f x : map f xs We can evaluate programs equationally: 12345678910111213{-map toUpper \"hi!\" ≡ map toUpper (’h’:\"i!\") ≡ toUpper ’h’ : map toUpper \"i!\" ≡ ’H’ : map toUpper \"i!\" ≡ ’H’ : map toUpper (’i’:\"!\") ≡ ’H’ : toUpper ’i’ : map toUpper \"!\" ≡ ’H’ : ’I’ : map toUpper \"!\" ≡ ’H’ : ’I’ : map toUpper (’!’:\"\") ≡ ’H’ : ’I’ : ’!’ : map toUpper \"\" ≡ ’H’ : ’I’ : ’!’ : map toUpper [] ≡ ’H’ : ’I’ : ’!’ : [] ≡ \"HI!\"-} List Functions1234567891011121314151617181920212223242526272829303132333435-- in maths: f(g(x)) == (f o g)(x)myMap :: (a -&gt; b) -&gt; [a] -&gt; [b]myMap f [] = []myMap f (x:xs) = (f x) : (myMap f xs)-- 1 : 2 : 3 : []-- 1 + 2 + 3 + 0sum' :: [Int] -&gt; Intsum' [] = 0sum' (x:xs) = x + sum xs-- [\"hello\",\"world\",\"!\"] -&gt; \"helloworld!\"-- \"hello\":\"world\":\"!\":[]-- \"hello\"++\"world\"++\"!\"++[]concat' :: [[a]] -&gt; [a]concat' [] = []concat' (xs:xss) = xs ++ concat xssfoldr' :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; bfoldr' f z [] = zfoldr' f z (x:xs) = x `f` (foldr' f z xs)sum'' = foldr' (+) 0concat'' = foldr' (++) []filter' :: (a -&gt; Bool) -&gt; [a] -&gt; [a]filter' p [] = []-- filter' p (x:xs) = if p x then x : filter' p xs -- else filter' p xsfilter' p (x:xs) | p x = x : filter' p xs | otherwise = filter' p xs InductionSuppose we want to prove that a property $P(n)$ holds for all natural numbers $n$. Remember that the set of natural numbers $N$ can be defined as follows: Definition of Natural Numbers Inductive definition of Natural numbers 0 is a natural number. For any natural number $n, n + 1$ is also a natural number Therefore, to show $P(n)$ for all $n$, it suffices to show: $P(0)$ (the base case), and assuming $P(k)$ (the inductive hypothesis), $⇒ P(k + 1)$ (the inductive case). Induction on Lists Haskell lists can be defined similarly to natural numbers Definition of Haskell Lists [] is a list. For any list xs, x:xs is also a list (for any item x). This means, if we want to prove that a property P(ls) holds for all lists ls, it suffices to show: P([]) (the base case) P(x:xs) for all items x, assuming the inductive hypothesis P(xs) Data TypesSo far, we have seen type synonyms using the type keyword. For a graphics library, we might define: 123456type Point = (Float, Float)type Vector = (Float, Float)type Line = (Point, Point)type Colour = (Int, Int, Int, Int) -- RGBAmovePoint :: Point -&gt; Vector -&gt; PointmovePoint (x,y) (dx,dy) = (x + dx, y + dy) But these definitions allow Points and Vectors to be used interchangeably, increasing the likelihood of errors. We can define our own compound types using the data keyword: 12data Point = Point Float Floatderiving (Show, Eq) First Point is the type name Second Point is Constructor name Floats are Constructor argument types 12345data Vector = Vector Float Floatderiving (Show, Eq)movePoint :: Point -&gt; Vector -&gt; PointmovePoint (Point x y) (Vector dx dy)= Point (x + dx) (y + dy) RecordsWe could define Colour similarly: 1data Colour = Colour Int Int Int Int But this has so many parameters, it’s hard to tell which is which. Haskell lets us declare these types as records, which is identical to the declaration style on the previous slide, but also gives us projection functions and record syntax: 12345data Colour = Colour { redC :: Int , greenC :: Int , blueC :: Int , opacityC :: Int } deriving (Show, Eq) Here, the code redC (Colour 255 128 0 255) gives 255. Enumeration TypesSimilar to enums in C and Java, we can define types to have one of a set of predefined values: 123456data LineStyle = Solid | Dashed | Dotted deriving (Show, Eq)data FillStyle = SolidFill | NoFill deriving (Show, Eq) Types with more than one constructor are called sum types Algebraic Data TypesJust as the Point constructor took two Float arguments, constructors for sum types can take parameters too, allowing us to model different kinds of shape: 12345678data PictureObject = Path [Point] Colour LineStyle | Circle Point Float Colour LineStyle FillStyle | Polygon [Point] Colour LineStyle FillStyle | Ellipse Point Float Float Float Colour LineStyle FillStyle deriving (Show, Eq)type Picture = [PictureObject] Recursive and Parametric TypesData types can also be defined with parameters, such as the well known Maybe type, defined in the standard library: 1data Maybe a = Just a | Nothing Types can also be recursive. If lists weren’t already defined in the standard library, we could define them ourselves: 1data List a = Nil | Cons a (List a) We can even define natural numbers, where 2 is encoded as Succ(Succ Zero): 1data Natural = Zero | Succ Natural Types in Design Make illegal states unrepresentable. ​ – Yaron Minsky (of Jane Street) Choose types that constrain your implementation as much as possible. Then failure scenarios are eliminated automatically. Partial Functions A partial function is a function not defined for all possible inputs. Partial functions are to be avoided, because they cause your program to crash if undefined cases are encountered. To eliminate partiality, we must either: enlarge the codomain, usually with a Maybe type: 123safeHead :: [a] -&gt; Maybe a safeHead (x:xs) = Just xsafeHead [] = Nothing constrain the domain to be more specific: 1234safeHead' :: NonEmpty a -&gt; asafeHead' (One a) = asafeHead' (Cons a _) = adata NonEmpty a = One a | Cons a (NonEmpty a) Type ClassesYou have already seen functions such as: compare, (==), (+), (show) that work on multiple types, and their corresponding constraints on type variables Ord, Eq, Num and Show. These constraints are called type classes, and can be thought of as a set of types for which certain operations are implemented. Show The Show type class is a set of types that can be converted to strings. 12class Show a where -- nothing to do with OOP show :: a -&gt; String Types are added to the type class as an instance like so: 123instance Show Bool where show True = \"True\" show False = \"False\" We can also define instances that depend on other instances: 123instance Show a =&gt; Show (Maybe a) where show (Just x) = \"Just \" ++ show x show Nothing = \"Nothing\" Fortunately for us, Haskell supports automatically deriving instances for some classes, including Show. Read Type classes can also overload based on the type returned. 12class Read a where read :: String -&gt; a Semigroup A semigroup is a pair of a set S and an operation • : S → S → S where the operation s associative. Associativity is defined as, for all a, b, c: (a • (b • c)) = ((a • b) • c) Haskell has a type class for semigroups! The associativity law is enforced only by programmer discipline: 123class Semigroup s where(&lt;&gt;) :: s -&gt; s -&gt; s-- Law: (&lt;&gt;) must be associative. What instances can you think of? Lists &amp; ++, numbers and +, numbers and * Example: Lets implement additive colour mixing: 12345678instance Semigroup Colour whereColour r1 g1 b1 a1 &lt;&gt; Colour r2 g2 b2 a2 = Colour (mix r1 r2) (mix g1 g2) (mix b1 b2) (mix a1 a2)where mix x1 x2 = min 255 (x1 + x2) Observe that associativity is satisfied. Moniod A monoid is a semigroup (S, •) equipped with a special identity element z : S such that x • z = x and z • y = y for all x, y. 12345class (Semigroup a) =&gt; Monoid a where mempty :: aFor colours, the identity element is transparent black:instance Monoid Colour where mempty = Colour 0 0 0 0 For each of the semigroups discussed previously(lists, num and +, num and *): Are they monoids? Yes If so, what is the identity element? [], 0, 1 Are there any semigroups that are not monoids? Maximum NewtypesThere are multiple possible monoid instances for numeric types like Integer: The operation (+) is associative, with identity element 0 The operation (*) is associative, with identity element 1 Haskell doesn’t use any of these, because there can be only one instance per type per class in the entire program (including all dependencies and libraries used). A common technique is to define a separate type that is represented identically to the original type, but can have its own, different type class instances. In Haskell, this is done with the newtype keyword. A newtype declaration is much like a data declaration except that there can be only one constructor and it must take exactly one argument: 12345newtype Score = S Integerinstance Semigroup Score where S x &lt;&gt; S y = S (x + y)instance Monoid Score where mempty = S 0 Here, Score is represented identically to Integer, and thus no performance penalty is incurred to convert between them. In general, newtypes are a great way to prevent mistakes. Use them frequently! Ord Ord is a type class for inequality comparison 12class Ord a where (&lt;=) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x &lt;= x Transitivity: If x &lt;= y and y &lt;= z then x &lt;= z Antisymmetry: If x &lt;= y and y &lt;= x then x == y. Totality: Either x &lt;= y or y &lt;= x Relations that satisfy these four properties are called total orders(most are total orders). Without the fourth (totality), they are called partial orders(e.g. division). Eq Eq is a type class for equality or equivalence 12class Eq a where (==) :: a -&gt; a -&gt; Bool What laws should instances satisfy? For all x, y, and z: Reflexivity: x == x. Transitivity: If x == y and y == z then x == z. Symmetry: If x == y then y == x. Relations that satisfy these are called equivalence relations. Some argue that the Eq class should be only for equality, requiring stricter laws like: If x == y then f x == f y for all functions f But this is debated. Funtors Types and ValuesHaskell is actually comprised of two languages. The value-level language, consisting of expressions such as if, let, 3 etc. The type-level language, consisting of types Int, Bool, synonyms like String, and type constructors like Maybe, (-&gt;), [ ] etc This type level language itself has a type system! KindsJust as terms in the value level language are given types, terms in the type level language are given kinds. The most basic kind is written as *. Types such as Int and Bool have kind *. Seeing as Maybe is parameterised by one argument, Maybe has kind * -&gt; *: given a type (e.g. Int), it will return a type (Maybe Int). ListsSuppose we have a function: 1toString :: Int -&gt; String And we also have a function to give us some numbers: 1getNumbers :: Seed -&gt; [Int] How can I compose toString with getNumbers to get a function f of type Seed -&gt; [String]? we use map: f = map toString . getNumbers. What about return a Maybe Int? we can use maybe map 1234567maybeMap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe bmaybeMap f Nothing = NothingmaybeMap f (Just x) = Just (f x)maybeMap f mx = case mx of Nothing -&gt; Nothing Just x -&gt; Just (f x) We can generalise this using functor. FunctorAll of these functions are in the interface of a single type class, called Functor. 12class Functor f where fmap :: (a -&gt; b) -&gt; f a -&gt; f b Unlike previous type classes we’ve seen like Ord and Semigroup, Functor is over types of kind * -&gt; *. 1234567891011121314151617181920-- Instance for tuples-- type level:-- (,) :: * -&gt; (* -&gt; *)-- (,) x :: * -&gt; *instance Functor ((,) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (,) x a -&gt; (,) x b-- fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x, b) fmap f (x,a) = (x, f a) -- instance for functions-- type level:-- (-&gt;) :: * -&gt; (* -&gt; *)-- (-&gt;) x :: * -&gt; *instance Functor ((-&gt;) x) where-- fmap :: (a -&gt; b) -&gt; f a -&gt; f b-- fmap :: (a -&gt; b) -&gt; (-&gt;) x a -&gt; (-&gt;) x b-- fmap :: (a -&gt; b) -&gt; (x -&gt; a) -&gt; (x -&gt; b) fmap = (.) Functor LawsThe functor type class must obey two laws: fmap id == id(indentity law) fmap f . fmap g == fmap (f . g)(composition law, haskell use this law to do optimisation) In Haskell’s type system it’s impossible to make a total fmap function that satisfies the first law but violates the second. This is due to parametricity. Property Based TestingFree PropertiesHaskell already ensures certain properties automatically with its language design and type system. Memory is accessed where and when it is safe and permitted to be accessed (memory safety). Values of a certain static type will actually have that type at run time. Programs that are well-typed will not lead to undefined behaviour (type safety). All functions are pure: Programs won’t have side effects not declared in the type. (purely functional programming) ⇒ Most of our properties focus on the logic of our program. Logical PropertiesWe have already seen a few examples of logical properties. Example: reverse is an involution: reverse (reverse xs) == xs right identity for (++): xs ++ [] == xs transitivity of (&gt;): (a &gt; b) ∧ (b &gt; c) ⇒ (a &gt; c) The set of properties that capture all of our requirements for our program is called the functional correctness specification of our software. This defines what it means for software to be correct. ProofsLast week we saw some proof methods for Haskell programs. We could prove that our implementation meets its functional correctness specification. Such proofs certainly offer a high degree of assurance, but: Proofs must make some assumptions about the environment and the semantics of the software. Proof complexity grows with implementation complexity, sometimes drastically. If software is incorrect, a proof attempt might simply become stuck: we do not always get constructive negative feedback. Proofs can be labour and time intensive ($$$), or require highly specialised knowledge ($$$). TestingCompared to proofs: Tests typically run the actual program, so requires fewer assumptions about the language semantics or operating environment. Test complexity does not grow with implementation complexity, so long as the specification is unchanged. Incorrect software when tested leads to immediate, debuggable counter examples. Testing is typically cheaper and faster than proving. Tests care about efficiency and computability, unlike proofs(e.g. termination is provable but not computable). We lose some assurance, but gain some convenience ($$$). Property Based Testing Key idea: Generate random input values, and test properties by running them. Example(QuickCheck Property) 1234567891011121314151617181920import Test.QuickCheckimport Data.Charimport Data.List-- Testable -- Arbitrary Testable-- Arbitrary Testableprop_reverseApp :: [Int] -&gt; ([Int] -&gt; Bool)prop_reverseApp xs ys = reverse (xs ++ ys) == reverse ys ++ reverse xsdivisible :: Int -&gt; Int -&gt; Booldivisible x y = x `mod` y == 0-- or select different generators with modifier newtypes.prop_refl :: Positive Int -&gt; Bool prop_refl (Positive x) = divisible x x-- Encode pre-conditions with the (==&gt;) operator:prop_unwordsWords s = unwords (words s) == sprop_wordsUnwords l = all (\\w -&gt; all (not . isSpace) w &amp;&amp; w /= []) l ==&gt; words (unwords l) == l PBT vs. Unit Testing Properties are more compact than unit tests, and describe more cases. ⇒ Less testing code Property-based testing heavily depends on test data generation: Random inputs may not be as informative as hand-crafted inputs ⇒ use shrinking(When a test fails, it finds the smallest test case still falls) Random inputs may not cover all necessary corner cases: ⇒ use a coverage checker Random inputs must be generated for user-defined types: ⇒ QuickCheck includes functions to build custom generators By increasing the number of random inputs, we improve code coverage in PBT. Test Data GenerationData which can be generated randomly is represented by the following type class: 123class Arbitrary a where arbitrary :: Gen a -- more on this later shrink :: a -&gt; [a] Most of the types we have seen so far implement Arbitrary. Shrinking The shrink function is for when test cases fail. If a given input x fails, QuickCheck will try all inputs in shrink x; repeating the process until the smallest possible input is found Testable TypesThe type of the quickCheck function is: 12-- more on IO laterquickCheck :: (Testable a) =&gt; a -&gt; IO () The Testable type class is the class of things that can be converted into properties. This includes: Bool values QuickCheck’s built-in Property type Any function from an Arbitrary input to a Testable output: 12instance (Arbitrary i, Testable o) =&gt; Testable (i -&gt; o) ... Thus the type [Int] -&gt; [Int] -&gt; Bool (as used earlier) is Testable. Examples12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455split :: [a] -&gt; ([a],[a])split [] = ([],[])split [a] = ([a],[])split (x:y:xs) = let (l,r) = split xs in (x:l,y:r)prop_splitPerm xs = let (l,r) = split (xs :: [Int]) in permutation xs (l ++ r)permutation :: (Ord a) =&gt; [a] -&gt; [a] -&gt; Boolpermutation xs ys = sort xs == sort yspermutation' :: (Eq a) =&gt; [a] -&gt; [a] -&gt; (a -&gt; Bool)permutation' xs ys = \\x -&gt; count x xs == count x ys where count x l = length (filter (== x) l)merge :: (Ord a) =&gt; [a] -&gt; [a] -&gt; [a]merge [] ys = ysmerge xs [] = xsmerge (x:xs) (y:ys) | x &lt;= y = x : merge xs (y:ys) | otherwise = y : merge (x:xs) ysprop_mergePerm xs ys = permutation (xs ++ (ys :: [Int] )) (merge xs ys)prop_mergeSorted (Ordered xs) (Ordered ys) = sorted (merge (xs :: [Int]) ys)sorted :: Ord a =&gt; [a] -&gt; Boolsorted [] = True sorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)mergeSort :: (Ord a) =&gt; [a] -&gt; [a]mergeSort [] = []mergeSort [x] = [x]mergeSort xs = let (l,r) = split xs in merge (mergeSort l) (mergeSort r)prop_mergeSortSorts xs = sorted (mergeSort (xs :: [Int])) prop_mergeSortPerm xs = permutation xs (mergeSort (xs :: [Int]))prop_mergeSortExtra xs = mergeSort (xs :: [Int]) == sort xsprop_mergeSortUnit = mergeSort [3,2,1] == [1,2,3]main = do quickCheck prop_mergeSortUnit quickCheck prop_mergeSortSorts quickCheck prop_mergeSortPerm Redundant PropertiesSome properties are technically redundant (i.e. implied by other properties in the specification), but there is some value in testing them anyway: They may be more efficient than full functional correctness tests, consuming less computing resources to test. They may be more fine-grained to give better test coverage than random inputs for full functional correctness tests. They provide a good sanity check to the full functional correctness properties. Sometimes full functional correctness is not easily computable but tests of weaker properties are. These redundant properties include unit tests. We can (and should) combine both approaches! Lazy Evaluation It never evaluate anything unless it has to 123sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n This crashes when given a large number. Why? Because of the growing stack frame. 1234567891011121314sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = sumTo' (a+n) (n-1)sumTo :: Integer -&gt; IntegersumTo 0 = 0sumTo n = sumTo (n-1) + n-- sumTo' 0 5-- sumTo' (0+5) (5-1)-- sumTo' (0+5) 4-- sumTo' (0+5+4) (4-1)-- sumTo' (0+5+4) 3-- sumTo' (0+5+4+3) (3-1)-- sumTo' (0+5+4+3) 2 -&gt; never evaluate the first argument-- .. This still crashes when given a large number. Why? This is called a space leak, and is one of the main drawbacks of Haskell’s lazy evaluation method. Haskell is lazily evaluated, also called call-by-need. This means that expressions are only evaluated when they are needed to compute a result for the user. We can force the previous program to evaluate its accumulator by using a bang pattern, or the primitive operation seq: 1234567{-# LANGUAGE BangPatterns #-}sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' !a 0 = asumTo' !a n = sumTo' (a+n) (n-1)sumTo' :: Integer -&gt; Integer -&gt; IntegersumTo' a 0 = asumTo' a n = let a' = a + n in a' `seq` sumTo' a' (n-1) AdvantagesLazy Evaluation has many advantages: It enables equational reasoning even in the presence of partial functions and non-termination It allows functions to be decomposed without sacrificing efficiency, for example: minimum = head . sort is, depending on sorting algorithm, possibly O(n). It allows for circular programming and infinite data structures, which allow us to express more things as pure functions. Infinite Data StructuresLaziness lets us define data structures that extend infinitely. Lists are a common example, but it also applies to trees or any user-defined data type: 1ones = 1 : ones Many functions such as take, drop, head, tail, filter and map work fine on infinite lists. 12345naturals = 0 : map (1+) naturals--ornaturals = map sum (inits ones)-- fibonacci numbersfibs = 1:1:zipWith (+) fibs (tail fibs) Data Invariants and ADTsStructure of a ModuleA Haskell program will usually be made up of many modules, each of which exports one or more data types. Typically a module for a data type X will also provide a set of functions, called operations, on X. to construct the data type: c :: · · · → X to query information from the data type: q :: X → · · · to update the data type: u :: · · · X → X A lot of software can be designed with this structure. Example: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869module Dictionary ( Word , Definition , Dict , emptyDict , insertWord , lookup ) whereimport Prelude hiding (Word, lookup)import Test.QuickCheckimport Test.QuickCheck.Modifiers-- lookup :: [(a,b)] -&gt; a -&gt; Maybe btype Word = Stringtype Definition = Stringnewtype Dict = D [DictEntry] deriving (Show, Eq)emptyDict :: DictemptyDict = D []insertWord :: Word -&gt; Definition -&gt; Dict -&gt; DictinsertWord w def (D defs) = D (insertEntry (Entry w def) defs) where insertEntry wd (x:xs) = case compare (word wd) (word x) of GT -&gt; x : (insertEntry wd xs) EQ -&gt; wd : xs LT -&gt; wd : x : xs insertEntry wd [] = [wd]lookup :: Word -&gt; Dict -&gt; Maybe Definitionlookup w (D es) = search w es where search w [] = Nothing search w (e:es) = case compare w (word e) of LT -&gt; Nothing EQ -&gt; Just (defn e) GT -&gt; search w essorted :: (Ord a) =&gt; [a] -&gt; Boolsorted [] = Truesorted [x] = Truesorted (x:y:xs) = x &lt;= y &amp;&amp; sorted (y:xs)wellformed :: Dict -&gt; Boolwellformed (D es) = sorted esprop_insert_wf dict w d = wellformed dict ==&gt; wellformed (insertWord w d dict)data DictEntry = Entry { word :: Word , defn :: Definition } deriving (Eq, Show)instance Ord DictEntry where Entry w1 d1 &lt;= Entry w2 d2 = w1 &lt;= w2instance Arbitrary DictEntry where arbitrary = Entry &lt;$&gt; arbitrary &lt;*&gt; arbitraryinstance Arbitrary Dict where arbitrary = do Ordered ds &lt;- arbitrary pure (D ds)prop_arbitrary_wf dict = wellformed dict Data Invariants Data invariants are properties that pertain to a particular data type. Whenever we use operations on that data type, we want to know that our data invariants are maintained. For a given data type X, we define a wellformedness predicate $$\\text{wf :: X → Bool}$$For a given value x :: X, wf x returns true iff our data invariants hold for the value x For each operation, if all input values of type X satisfy wf, all output values will satisfy wf. In other words, for each constructor operation c :: · · · → X, we must show wf (c · · ·), and for each update operation u :: X → X we must show wf x =⇒ wf(u x) Abstract Data Types An abstract data type (ADT) is a data type where the implementation details of the type and its associated operations are hidden. 123456newtype Dicttype Word = Stringtype Definition = StringemptyDict :: DictinsertWord :: Word -&gt; Definition -&gt; Dict -&gt; Dictlookup :: Word -&gt; Dict -&gt; Maybe Definition If we don’t have access to the implementation of Dict, then we can only access it via the provided operations, which we know preserve our data invariants. Thus, our data invariants cannot be violated if this module is correct. In general, abstraction is the process of eliminating detail. The inverse of abstraction is called refinement. Abstract data types like the dictionary above are abstract in the sense that their implementation details are hidden, and we no longer have to reason about them on the level of implementation. ValidationSuppose we had a sendEmail function 123sendEmail :: String -- email address -&gt; String -- message -&gt; IO () -- action (more in 2 wks) It is possible to mix the two String arguments, and even if we get the order right, it’s possible that the given email address is not valid. We could define a tiny ADT for validated email addresses, where the data invariant is that the contained email address is valid: 1234567module EmailADT(Email, checkEmail, sendEmail) newtype Email = Email String checkEmail :: String -&gt; Maybe Email checkEmail str | '@' `elem` str = Just (Email str) | otherwise = Nothing-- Then, change the type of sendEmail: sendEmail :: Email -&gt; String -&gt; IO() The only way (outside of the EmailADT module) to create a value of type Email is to use checkEmail. checkEmail is an example of what we call a smart constructor: a constructor that enforces data invariants. Data RefinementReasoning about ADTsConsider the following, more traditional example of an ADT interface, the unbounded queue: 123456data QueueemptyQueue :: Queueenqueue :: Int -&gt; Queue -&gt; Queuefront :: Queue -&gt; Int -- partialdequeue :: Queue -&gt; Queue -- partialsize :: Queue -&gt; Int We could try to come up with properties that relate these functions to each other without reference to their implementation, such as: dequeue (enqueue x emptyQueue) == emptyQueue However these do not capture functional correctness (usually). Models for ADTsWe could imagine a simple implementation for queues, just in terms of lists: 12345emptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = length But this implementation is O(n) to enqueue! Unacceptable! However!(This is out mental model) This is a dead simple implementation, and trivial to see that it is correct. If we make a better queue implementation, it should always give the same results as this simple one. Therefore: This implementation serves as a functional correctness specification for our Queue type! Refinement Relations The typical approach to connect our model queue to our Queue type is to define a relation, called a refinement relation, that relates a Queue to a list and tells us if the two structures represent the same queue conceptually: 1234rel :: Queue -&gt; [Int] -&gt; Boolprop_empty_r = rel emptyQueue emptyQueueLprop_size_r fq lq = rel fq lq ==&gt; size fq == sizeL lqprop_enq_ref fq lq x = rel fq lq ==&gt; rel (enqueue x fq) (enqueueL x lq) Abstraction FunctionsThese refinement relations are very difficult to use with QuickCheck because the rel fq lq preconditions are very hard to satisfy with randomly generated inputs. For this example, it’s a lot easier if we define an abstraction function that computes the corresponding abstract list from the concrete Queue. 1toAbstract :: Queue → [Int] Conceptually, our refinement relation is then just: 1\\fq lq → absfun fq == lq However, we can re-express our properties in a much more QC-friendly format 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import Test.QuickCheckemptyQueueL = []enqueueL a = (++ [a])frontL = headdequeueL = tailsizeL = lengthtoAbstract :: Queue -&gt; [Int]toAbstract (Q f sf r sr) = f ++ reverse rprop_empty_ref = toAbstract emptyQueue == emptyQueueLprop_enqueue_ref fq x = toAbstract (enqueue x fq) == enqueueL x (toAbstract fq)prop_size_ref fq = size fq == sizeL (toAbstract fq)prop_front_ref fq = size fq &gt; 0 ==&gt; front fq == frontL (toAbstract fq)prop_deq_ref fq = size fq &gt; 0 ==&gt; toAbstract (dequeue fq) == dequeueL (toAbstract fq)prop_wf_empty = wellformed emptyQueueprop_wf_enq x q = wellformed q ==&gt; wellformed (enqueue x q)prop_wf_deq x q = wellformed q &amp;&amp; size q &gt; 0 ==&gt; wellformed (dequeue q)data Queue = Q [Int] -- front of the queue Int -- size of the front [Int] -- rear of the queue Int -- size of the rear deriving (Show, Eq)wellformed :: Queue -&gt; Boolwellformed (Q f sf r sr) = length f == sf &amp;&amp; length r == sr &amp;&amp; sf &gt;= srinstance Arbitrary Queue where arbitrary = do NonNegative sf' &lt;- arbitrary NonNegative sr &lt;- arbitrary let sf = sf' + sr f &lt;- vectorOf sf arbitrary r &lt;- vectorOf sr arbitrary pure (Q f sf r sr)inv3 :: Queue -&gt; Queueinv3 (Q f sf r sr) | sf &lt; sr = Q (f ++ reverse r) (sf + sr) [] 0 | otherwise = Q f sf r sremptyQueue :: QueueemptyQueue = Q [] 0 [] 0enqueue :: Int -&gt; Queue -&gt; Queueenqueue x (Q f sf r sr) = inv3 (Q f sf (x:r) (sr+1))front :: Queue -&gt; Int -- partialfront (Q (x:f) sf r sr) = xdequeue :: Queue -&gt; Queue -- partialdequeue (Q (x:f) sf r sr) = inv3 (Q f (sf -1) r sr)size :: Queue -&gt; Intsize (Q f sf r sr) = sf + sr Data RefinementThese kinds of properties establish what is known as a data refinement from the abstract, slow, list model to the fast, concrete Queue implementation. Refinement and Specifications In general, all functional correctness specifications can be expressed as: all data invariants are maintained, and the implementation is a refinement of an abstract correctness model. There is a limit to the amount of abstraction we can do before they become useless for testing (but not necessarily for proving). Effects Effects are observable phenomena from the execution of a program. Internal vs. External EffectsExternal Observability An external effect is an effect that is observable outside the function. Internal effects are not observable from outside. Example Console, file and network I/O; termination and non-termination; non-local control flow; etc. Are memory effects external or internal? Depends on the scope of the memory being accessed. Global variable accesses are external. PurityA function with no external effects is called a pure function. A pure function is the mathematical notion of a function. That is, a function of type a -&gt; b is fully specified by a mapping from all elements of the domain type a to the codomain type b. Consequences: Two invocations with the same arguments result in the same value. No observable trace is left beyond the result of the function. No implicit notion of time or order of execution. Haskell FunctionsHaskell functions are technically not pure. They can loop infinitely. They can throw exceptions (partial functions) They can force evaluation of unevaluated expressions. Caveat Purity only applies to a particular level of abstraction. Even ignoring the above, assembly instructions produced by GHC aren’t really pure. Despite the impurity of Haskell functions, we can often reason as though they are pure. Hence we call Haskell a purely functional language. The Danger of Implicit Side Effects They introduce (often subtle) requirements on the evaluation order. They are not visible from the type signature of the function. They introduce non-local dependencies which is bad for software design, increasing coupling. They interfere badly with strong typing, for example mutable arrays in Java, or reference types in ML. We can’t, in general, reason equationally about effectful programs! Can we program with pure functions?Typically, a computation involving some state of type s and returning a result of type a can be expressed as a function: 1s -&gt; (s, a) Rather than change the state, we return a new copy of the state. All that copying might seem expensive, but by using tree data structures, we can usually reduce the cost to an O(log n) overhead. StateState Passing12345678910111213data Tree a = Branch a (Tree a) (Tree a) | Leaf-- Given a tree, label each node with an ascending number in infix order:label :: Tree () -&gt; Tree Intlable t = snd (go t 1) where go :: Tree() -&gt; Int -&gt; (Int, Tree Int) go Leaf c = (c, Leaf) go (Branch () l r) c = let (c', l') = go l c v = c' (c'', r') = go r (c'+1) in (c'', Branch v l' r')-- it works but not pretty Let’s use a data type to simplify this! State12345678910111213141516newtype State s a = A procedure that, manipulating some state of type s, returns a-- State Operationsget :: State s sput :: s -&gt; State s ()pure :: a -&gt; State s aevalState :: State s a -&gt; s -&gt; a-- Sequential Composition-- Do one state action after another with do blocks:do put 42 desugars put 42 &gt;&gt; put Truepure True(&gt;&gt;) :: State s a -&gt; State s b -&gt; State s b-- Bind-- The 2nd step can depend on the first with bind:do x &lt;- get desugars get &gt;&gt;= \\x -&gt; pure (x + 1)pure (x+1)(&gt;&gt;=) :: State s a -&gt; (a -&gt; State s b) -&gt; State s b Example 12345678910111213141516171819202122232425262728293031323334353637383940{-modify :: (s -&gt; s) -&gt; State s ()modify f = do s &lt;- get put (f s)-}label' :: Tree () -&gt; Tree Intlabel' t = evalState (go t) 1 where go :: Tree () -&gt; State Int (Tree Int) go Leaf = pure Leaf go (Branch () l r) = do l' &lt;- go l v &lt;- get put (v + 1) r' &lt;- go r pure (Branch v l' r')newtype State' s a = State (s -&gt; (s, a))get' :: State' s sget' = (State $ \\s -&gt; (s, s)) put' :: s -&gt; State' s ()put' s = State $ \\_ -&gt; (s,())pure' :: a -&gt; State' s apure' a = State $ \\s -&gt; (s, a)evalState' :: State' s a -&gt; s -&gt; aevalState (State f) s = snd (f s) (&gt;&gt;=!) :: State' s a -&gt; (a -&gt; State' s b) -&gt; State' s b(State c) &gt;&gt;=! f = State $ \\s -&gt; let (s', a) = c s (State c') = f a in c' s'(&gt;&gt;!) :: State' s a -&gt; State' s b -&gt; State' s b(&gt;&gt;!) a b = a &gt;&gt;=! \\_ -&gt; b IO A procedure that performs some side effects, returning a result of type a is written as IO a. IO a is an abstract type. But we can think of it as a function: RealWorld -&gt; (RealWorld, a) (that’s how it’s implemented in GHC) 123456(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO bpure :: a -&gt; IO agetChar :: IO CharreadLine :: IO StringputStrLn :: String -&gt; IO () -- return a procedure Infectious IOWe can convert pure values to impure procedures with pure: 1pure :: a -&gt; IO a But we can’t convert impure procedures to pure values The only function that gets an a from an IO a is &gt;&gt;=: 1(&gt;&gt;=) :: IO a -&gt; (a -&gt; IO b) -&gt; IO b But it returns an IO procedure as well. If a function makes use of IO effects directly or indirectly, it will have IO in its type! Haskell Design StrategyWe ultimately “run” IO procedures by calling them from main: 1main :: IO () Example 12345678-- Given an input number n, print a triangle of * characters of base width n.printTriangle :: Int -&gt; IO ()printTriangle 0 = pure ()printTriangle n = do putStrLn (replicate n '*') printTriangle (n - 1)main = printTriangle 9 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108{-Design a game that reads in a n × n maze from a file. The player starts at position(0, 0) and must reach position (n − 1, n − 1) to win. The game accepts keyboard inputto move the player around the maze.-}import Data.List import System.IOmazeSize :: Int mazeSize = 10data Tile = Wall | Floor deriving (Show, Eq)type Point = (Int, Int)lookupMap :: [Tile] -&gt; Point -&gt; TilelookupMap ts (x,y) = ts !! (y * mazeSize + x)addX :: Int -&gt; Point -&gt; PointaddX dx (x,y) = (x + dx, y)addY :: Int -&gt; Point -&gt; PointaddY dy (x,y) = (x, y + dy)data Game = G { player :: Point , map :: [Tile] }invariant :: Game -&gt; Bool invariant (G (x,y) ts) = x &gt;= 0 &amp;&amp; x &lt; mazeSize &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; mazeSize &amp;&amp; lookupMap ts (x,y) /= WallmoveLeft :: Game -&gt; Game moveLeft (G p m) = let g' = G (addX (-1) p) m in if invariant g' then g' else G p mmoveRight :: Game -&gt; GamemoveRight (G p m) = let g' = G (addX 1 p) m in if invariant g' then g' else G p mmoveUp :: Game -&gt; GamemoveUp (G p m) = let g' = G (addY (-1) p) m in if invariant g' then g' else G p mmoveDown :: Game -&gt; GamemoveDown (G p m) = let g' = G (addY 1 p) m in if invariant g' then g' else G p mwon :: Game -&gt; Bool won (G p m) = p == (mazeSize-1,mazeSize-1)main :: IO () main = do str &lt;- readFile \"input.txt\" let initial = G (0,0) (stringToMap str) gameLoop initial where gameLoop :: Game -&gt; IO () gameLoop state | won state = putStrLn \"You win!\" | otherwise = do display state c &lt;- getChar' case c of 'w' -&gt; gameLoop (moveUp state) 'a' -&gt; gameLoop (moveLeft state) 's' -&gt; gameLoop (moveDown state) 'd' -&gt; gameLoop (moveRight state) 'q' -&gt; pure () _ -&gt; gameLoop state stringToMap :: String -&gt; [Tile]stringToMap [] = []stringToMap ('#':xs) = Wall : stringToMap xs stringToMap (' ':xs) = Floor : stringToMap xs stringToMap (c:xs) = stringToMap xsdisplay :: Game -&gt; IO () display (G (px,py) m) = printer (0,0) m where printer (x,y) (t:ts) = do if (x,y) == (px,py) then putChar '@' else if t == Wall then putChar '#' else putChar ' ' if (x == mazeSize - 1) then do putChar '\\n' printer (0,y+1) ts else printer (x+1,y) ts printer (x,y) [] = putChar '\\n'getChar' :: IO Char getChar' = do b &lt;- hGetBuffering stdin e &lt;- hGetEcho stdin hSetBuffering stdin NoBuffering hSetEcho stdin False x &lt;- getChar hSetBuffering stdin b hSetEcho stdin e pure x Benefits of an IO Type Absence of effects makes type system more informative: A type signatures captures entire interface of the function All dependencies are explicit in the form of data dependencies. All dependencies are typed It is easier to reason about pure code and it is easier to test Testing is local, doesn’t require complex set-up and tear-down. Reasoning is local, doesn’t require state invariants Type checking leads to strong guarantees. Mutable VariablesWe can have honest-to-goodness mutability in Haskell, if we really need it, using IORef. 1234data IORef anewIORef :: a -&gt; IO (IORef a)readIORef :: IORef a -&gt; IO awriteIORef :: IORef a -&gt; a -&gt; IO () Example 1234567891011121314151617181920212223242526import Data.IORef import Test.QuickCheck.Monadic import Test.QuickCheckaverageListIO :: [Int] -&gt; IO IntaverageListIO ls = do sum &lt;- newIORef 0 count &lt;- newIORef 0 let loop :: [Int] -&gt; IO () loop [] = pure () loop (x:xs) = do s &lt;- readIORef sum writeIORef sum (s + x) c &lt;- readIORef count writeIORef count (c + 1) loop xs loop ls s &lt;- readIORef sum c &lt;- readIORef count pure (s `div` c) prop_average :: [Int] -&gt; Propertyprop_average ls = monadicIO $ do pre (length ls &gt; 0) avg &lt;- run (averageListIO ls) assert (avg == (sum ls `div` length ls)) Mutable Variables, LocallySomething like averaging a list of numbers doesn’t require external effects, even if we use mutation internally. 12345data STRef s anewSTRef :: a -&gt; ST (STRef s a)readSTRef :: STRef s a -&gt; ST s awriteSTRef :: STRef s a -&gt; a -&gt; ST s ()runST :: (forall s. ST s a) -&gt; a The extra s parameter is called a state thread, that ensures that mutable variables don’t leak outside of the ST computation. The ST type is not assessable in this course, but it is useful sometimes in Haskell programming. QuickChecking EffectsQuickCheck lets us test IO (and ST) using this special property monad interface: 1234monadicIO :: PropertyM IO () -&gt; Propertypre :: Bool -&gt; PropertyM IO ()assert :: Bool -&gt; PropertyM IO ()run :: IO a -&gt; PropertyM IO a Do notation and similar can be used for PropertyM IO procedures just as with State s and IO procedures. Example 12345678910111213-- GNU Factorimport Test.QuickCheck import Test.QuickCheck.Modifiersimport Test.QuickCheck.Monadic import System.Process-- readProcess :: FilePath -&gt; [String] -&gt; String -&gt; IO Stringtest_gnuFactor :: Positive Integer -&gt; Propertytest_gnuFactor (Positive n) = monadicIO $ do str &lt;- run (readProcess \"gfactor\" [show n] \"\") let factors = map read (tail (words str)) assert (product factors == n) FunctorRecall the type class defined over type constructors called Functor. We’ve seen instances for lists, Maybe, tuples and functions. Other instances include: IO (how?) States (how?) Gen 123456789101112131415ioMap :: (a -&gt; b) -&gt; IO a -&gt; IO bioMap f act = do a &lt;- act pure (f a) stateMap :: (a -&gt; b) -&gt; State s a -&gt; State s bstateMap f act = do a &lt;- act pure (f a)-- more generalmonadMap :: Monad m =&gt; (a -&gt; b) -&gt; m a -&gt; m bmonadMap f act = do a &lt;- act pure (f a) QuickCheck Generators12345678910{-class Arbitrary a where arbitray :: Gen a shrink :: a -&gt; [a]-- Gen a ~=~ random generator of values type a.-}sortedLists :: (Arbitrary a, Ord a) =&gt; Gen [a]sortedLists = fmap sort arbitrary :: Gen a-- listOf :: Gen a -&gt; Gen [a] Applicative FunctorsBinary FunctionsSuppose we want to look up a student’s zID and program code using these functions: 1234lookupID :: Name -&gt; Maybe ZIDlookupProgram :: Name -&gt; Maybe Program-- we had a function:makeRecord :: ZID -&gt; Program -&gt; StudentRecord How can we combine these functions to get a function of type Name -&gt; Maybe StudentRecord? 1234lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in ? Binary Map? We could imagine a binary version of the maybeMap function: 12maybeMap2 :: (a -&gt; b -&gt; c) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c But then, we might need a trinary version. 12maybeMap3 :: (a -&gt; b -&gt; c -&gt; d) -&gt; Maybe a -&gt; Maybe b -&gt; Maybe c -&gt; Maybe d Or even a 4-ary version, 5-ary, 6-ary. . . this would quickly become impractical! Using Functor? Using fmap gets us part of the way there: 12345lookupRecord' :: Name -&gt; Maybe (Program -&gt; StudentRecord)lookupRecord' n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid -- what about program? But, now we have a function inside a Maybe. Applicative This is encapsulated by a subclass of Functor called Applicative. 123class Functor f =&gt; Applicative f where pure :: a -&gt; f a (&lt;*&gt;) :: f (a -&gt; b) -&gt; f a -&gt; f b Maybe is an instance, so we can use this for lookupRecord: 12345lookupRecord :: Name -&gt; Maybe StudentRecordlookupRecord n = let zid = lookupID n program = lookupProgram n in fmap makeRecord zid &lt;*&gt; program -- or pure makeRecord &lt;*&gt; zid &lt;*&gt; program Using ApplicativeIn general, we can take a regular function application:$$\\text{f a b c d}$$And apply that function to Maybe (or other Applicative) arguments using this pattern (where &lt;*&gt; is left-associative): $$\\text{pure f &lt;&gt; ma &lt;&gt; mb &lt;&gt; mc &lt;&gt; }$$ Relationship to FunctorAll law-abiding instances of Applicative are also instances of Functor, by defining: 1fmap f x = pure f &lt;*&gt; x Sometimes this is written as an infix operator, &lt;$&gt;, which allows us to write: 123pure f &lt;*&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md-- as:f &lt;$&gt; ma &lt;*&gt; mb &lt;*&gt; mc &lt;*&gt; md Applicative laws12345678-- Identitypure id &lt;*&gt; v = v-- Homomorphismpure f &lt;*&gt; pure x = pure (f x)-- Interchangeu &lt;*&gt; pure y = pure ($ y) &lt;*&gt; u-- Compositionpure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w) Example 1234567891011121314151617181920212223242526272829type Name = Stringtype ZID = Intdata Program = COMP | SENG | BINF | CENG deriving (Show, Eq)type StudentRecord = (Name, ZID, Program)lookupID :: Name -&gt; Maybe ZIDlookupID \"Liam\" = Just 3253158lookupID \"Unlucky\" = Just 4444444lookupID \"Prosperous\" = Just 8888888lookupID _ = NothinglookupProgram :: Name -&gt; Maybe ProgramlookupProgram \"Liam\" = Just COMPlookupProgram \"Unlucky\" = Just SENGlookupProgram \"Prosperous\" = Just CENGlookupProgram _ = NothingmakeRecord :: ZID -&gt; Program -&gt; Name -&gt; StudentRecordmakeRecord zid pr name = (name,zid,pr)liam :: Maybe StudentRecordliam = let mzid = lookupID \"Liam\" mprg = lookupProgram \"Liam\" in pure makeRecord &lt;*&gt; mzid &lt;*&gt; mprg &lt;*&gt; pure \"Liam\"-- pure :: a -&gt; Maybe a-- fmap :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b Functor Laws for ApplicativeThese are proofs not Haskell code: 123456789101112131415fmap f x = pure f &lt;*&gt; x-- The two functor laws are:1. fmap id x == x2. fmap f (fmap g x) == fmap (f.g) x-- Proof:1) pure id &lt;*&gt; x == x -- true by Identity law2) pure f &lt;*&gt; (pure g &lt;*&gt; x) == pure (.) &lt;*&gt; pure f &lt;*&gt; pure g &lt;*&gt; x --Composition == pure ((.) f) &lt;*&gt; pure g &lt;*&gt; x --Homomorphism == pure (f.g) &lt;*&gt; x --Homomorphism Applicative ListsThere are two ways to implement Applicative for lists: 1(&lt;*&gt;) :: [a -&gt; b] -&gt; [a] -&gt; [b] Apply each of the given functions to each of the given arguments, concatenating all the results Apply each function in the list of functions to the corresponding value in the list of arguments The second one is put behind a newtype (ZipList) in the Haskell standard library. 1234567891011121314pureZ :: a -&gt; [a]pureZ a = a:pureZ aapplyListsZ :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsZ (f:fs) (x:xs) = f x : applyListsZ fs xsapplyListsZ [] _ = []applyListsZ _ [] = []pureC :: a -&gt; [a]pureC a = [a] applyListsC :: [a -&gt; b] -&gt; [a] -&gt; [b]applyListsC (f:fs) args = map f args ++ applyListsC fs argsapplyListsC [] args = [] Other instancesQuickCheck generators: Gen1234data Concrete = C [Char] [Char] deriving (Show, Eq)instance Arbitrary Concrete where arbitrary = C &lt;$&gt; arbitrary &lt;*&gt; arbitrary Functions: ((-&gt;) x The Applicative instance for functions lets us pass the same argument into multiple functions without repeating ourselves. 1234567891011instance Applicative ((-&gt;) x) where pure :: a -&gt; x -&gt; a pure a x = a (&lt;*&gt;) :: (x -&gt; (a -&gt; b)) -&gt; (x -&gt; a) -&gt; (x -&gt; b) (&lt;*&gt;) xab xa x = xab x (xa x)-- f (g x) (h x) (i x)---- Can be written as: -- (pure f &lt;*&gt; g &lt;*&gt; h &lt;*&gt; i) x Tuples: ((,) x)We can’t implement pure without an extra constraint! The tuple instance for Applicative lets us combine secondary outputs from functions into one secondary output without manually combining them. 1234567891011instance Functor ((,) x) where fmap :: (a -&gt; b) -&gt; (x,a) -&gt; (x,b) fmap f (x,a) = (x,f a)-- monoid has an identity elementinstance Monoid x =&gt; Applicative ((,) x) where pure :: a -&gt; (x,a) pure a = (mempty ,a) (&lt;*&gt;) :: (x,a -&gt; b) -&gt; (x, a) -&gt; (x, b) (&lt;*&gt;) (x, f) (x',a) = (x &lt;&gt; x', f a) It requires Monoid here to combine the values, and to provide a default value for pure. 12345678910111213f :: A -&gt; (Log, B)g :: X -&gt; (Log, Y)a :: Ax :: Xcombine :: B -&gt; Y -&gt; Z-- combine the logs silentlytest :: (Log, Z)test = combine &lt;$&gt; f a &lt;*&gt; g x-- instead of -- let (l1, b) = f a-- (l2, y) = g x-- in (l1 &lt;&gt; l2, combine b y) IO and State s123456789instance Applicative IO where pure :: a -&gt; IO a pure a = pure a (&lt;*&gt;) :: IO (a -&gt; b) -&gt; IO a -&gt; IO b pf &lt;*&gt; pa = do f &lt;- pf a &lt;- pa pure (f a) Monads Monads are types m where we can sequentially compose functions of the form a -&gt; m b 12class Applicative m =&gt; Monad m where (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b Sometimes in old documentation the function return is included here, but it is just an alias for pure. It has nothing to do with return as in C/Java/Python etc. Example 123456789101112131415161718192021222324-- Maybe Monad(&gt;&gt;=) :: Maybe a -&gt; (a -&gt; Maybe b) -&gt; Maybe b(&gt;&gt;=) Nothing f = Nothing(&gt;&gt;=) (Just a) f = f a-- List Monad(&gt;&gt;=) :: [a] -&gt; (a -&gt; [b]) -&gt; [b](&gt;&gt;=) as f = concatMap f as-- function(&gt;&gt;=) :: (x -&gt; a) -&gt; (a -&gt; x -&gt;b) -&gt; b(&gt;&gt;=) xa axb x = axb (xa x) x-- function monad example(reader monad)f :: A -&gt; Config -&gt; Bf :: X -&gt; Config -&gt; Yf :: B -&gt; Config -&gt; Ccombine :: (A, X) -&gt; Config -&gt; (Y, C)combine (a, x) = do b &lt;- f a y &lt;- g x c &lt;- h b pure(y, c) Monad LawWe can define a composition operator with (&gt;&gt;=): 12(&lt;=&lt;) :: (b -&gt; m c) -&gt; (a -&gt; m b) -&gt; (a -&gt; m c) (f &lt;=&lt; g) x = g x &gt;&gt;= f Monad Laws 123f &lt;=&lt; (g &lt;=&lt; x) == (f &lt;=&lt; g) &lt;=&lt; x -- associativitypure &lt;=&lt; f == f -- left identityf &lt;=&lt; pure == f -- right identity These are similar to the monoid laws, generalised for multiple types inside the monad. This sort of structure is called a category in mathematics. Relationship to ApplicativeAll Monad instances give rise to an Applicative instance, because we can define &lt;*&gt; in terms of &gt;&gt;=. 1mf &lt;*&gt; mx = mf &gt;&gt;= \\f -&gt; mx &gt;&gt;= \\x -&gt; pure (f x) This implementation is already provided for Monads as the ap function in Control.Monad Do notationWorking directly with the monad functions can be unpleasant. As we’ve seen, Haskell has some notation to increase niceness: 1234567do x &lt;- y becomes y &gt;&gt;= \\x -&gt; do z zdo x becomes x &gt;&gt;= \\_ -&gt; do y y Examples 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071{- (Dice Rolls) Roll two 6-sided dice, if the difference is &lt; 2, reroll the second die. Final score is the difference of the two die. What score is most common?-}roll :: [Int]roll = [1, 2, 3, 4, 5, 6]diceGame = do d1 &lt;- roll d2 &lt;- roll if (abs (d1 - d2) &lt; 2) then do d2' &lt;- roll pure (abs (d1 - d2')) else pure (abs (d1 - d2)) {- Partial Functions We have a list of student names in a database of type [(ZID, Name)]. Given a list of zID’s, return a Maybe [Name], where Nothing indicates that a zID could not be found-}db :: [(ZID, Name)]db = [(3253158, \"Liam\"), (8888888, \"Rich\"), (4444444, \"Mort\")]studentNames :: [ZID] -&gt; Maybe [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- lookup z db ns &lt;- studentNames zs pure (n:ns)-- briefer but less clear with applicative notation:-- studentNames (z:zs) = (:) &lt;$&gt; lookup z db &lt;*&gt; studentNames zs{- Arbitrary Instances Define a Tree type and a generator for search trees: searchTrees :: Int -&gt; Int -&gt; Generator Tree-}data Tree a = Leaf | Branch a (Tree a) (Tree a) deriving (Show, Eq)instance Arbitrary (Tree Int) where arbitrary = do mn &lt;- (arbitrary :: Gen Int) Positive delta &lt;- arbitrary let mx = mn + delta searchTree mn mx where searchTree :: Int -&gt; Int -&gt; Gen (Tree Int) searchTree mn mx | mn &gt;= mx = pure Leaf | otherwise = do v &lt;- choose (mn,mx) l &lt;- searchTree mn v r &lt;- searchTree (v+1) mx pure (Branch v l r)-- The Either MonadstudentNames :: [ZID] -&gt; Either ZID [Name]studentNames [] = pure []studentNames (z:zs) = do n &lt;- case lookup z db of Just v -&gt; Right v Nothing -&gt; Left z ns &lt;- studentNames zs pure (n:ns) Static Assurance with TypesStatic AssureanceMethods of Assurance Static means of assurance analyse a program without running it. Static vs. Dynamic Static checks can be exhaustive. Exhaustivity An exhaustive check is a check that is able to analyse all possible executions of a program. However, some properties cannot be checked statically in general (halting problem), or are intractable to feasibly check statically (state space explosion). Dynamic checks cannot be exhaustive, but can be used to check some properties where static methods are unsuitable. Compiler IntegrationMost static and all dynamic methods of assurance are not integrated into the compilation process. You can compile and run your program even if it fails tests You can change your program to diverge from your model checker model. Your proofs can diverge from your implementation. Types Because types are integrated into the compiler, they cannot diverge from the source code. This means that type signatures are a kind of machine-checked documentation for your code. Types are the most widely used kind of formal verification in programming today. They are checked automatically by the compiler. They can be extended to encompass properties and proof systems with very high expressivity (covered next week). They are an exhaustive analysis Phantom Types A type parameter is phantom if it does not appear in the right hand side of the type definition. 1newtype Size x = S Int Lets examine each one of the following use cases: We can use this parameter to track what data invariants have been established about a value. We can use this parameter to track information about the representation (e.g. units of measure). We can use this parameter to enforce an ordering of operations performed on these values (type state). ValidationSuppose we have 123data UG -- empty typedata PGdata StudentID x = SID Int We can define a smart constructor that specialises the type parameter: 12sid :: Int -&gt; Either (StudentID UG) (StudentID PG) Define functions: 12enrolInCOMP3141 :: StudentID UG -&gt; IO ()lookupTranscript :: StudentID x -&gt; IO String Units of Measure123456789data Kilometresdata Milesdata Value x = U IntsydneyToMelbourne = (U 877 :: Value Kilometres)losAngelesToSanFran = (U 383 :: Value Miles)-- Note the arguments to area must have the same unitdata Square aarea :: Value m -&gt; Value m -&gt; Value (Square m)area (U x) (U y) = U (x * y) Type State123456789101112{- A Socket can either be ready to recieve data, or busy. If the socket is busy, the user must first use the wait operation, which blocks until the socket is ready. If the socket is ready, the user can use the send operation to send string data, which will make the socket busy again.-}data Busydata Readynewtype Socket s = Socket ...wait :: Socket Busy -&gt; IO (Socket Ready)send :: Socket Ready -&gt; String -&gt; IO (Socket Busy)-- assumption: use the socket in a linear way(only use once) Linearity and Type StateThe previous code assumed that we didn’t re-use old Sockets: 12345678910send2 :: Socket Ready -&gt; String -&gt; String -&gt; IO (Socket Busy)send2 s x y = do s' &lt;- send s x s'' &lt;- wait s' s''' &lt;- send s'' y pure s'''-- But we can just re-use old values to send without waiting:send2' s x y = do _ &lt;- send s x s' &lt;- send s y pure s' Linear type systems can solve this, but not in Haskell (yet) Datatype Promotion123data UGdata PGdata StudentID x = SID Int Defining empty data types for our tags is untyped. We can have StudentID UG, but also StudentID String. The DataKinds language extension lets us use data types as kinds: 1234567891011121314{-# LANGUAGE DataKinds, KindSignatures #-}data Stream = UG | PGdata StudentID (x :: Stream) = SID Intpostgrad :: [Int]postgrad = [3253158]makeStudentID :: Int -&gt; Either (StudentID UG) (StudentID PG)makeStudentID i | i `elem` postgrad = Right (SID i) | otherwise = Left (SID i)enrollInCOMP3141 :: StudentID UG -&gt; IO ()enrollInCOMP3141 (SID x) = putStrLn (show x ++ \" enrolled in COMP3141!\") GADTsUntyped Evaluator 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950data Expr t = BConst Bool | IConst Int | Times (Expr Int) (Expr Int) | Less (Expr Int) (Expr Int) | And (Expr Bool) (Expr Bool) | If (Expr Bool) (Expr t) (Expr t) deriving (Show, Eq)data Value = BVal Bool | IVal Int deriving (Show, Eq)eval :: Expr -&gt; Valueeval (BConst b) = BVal beval (IConst i) = IVal ieval (Times e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; IVal (i1 * i2)eval (Less e1 e2) = case (eval e1, eval e2) of (IVal i1, IVal i2) -&gt; BVal (i1 &lt; i2) eval (And e1 e2) = case (eval e1, eval e2) of (BVal b1, BVal b2) -&gt; BVal (b1 &amp;&amp; b2) eval (If ec et ee) = case eval ec of BVal True -&gt; eval et BVal False -&gt; eval ee-- partial functioneval :: Expr -&gt; Maybe Valueeval (BConst b) = pure (BVal b)eval (IConst i) = pure (IVal i)eval (Times e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (IVal (v1' * v2')) _ -&gt; Nothingeval (Less e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (IVal v1', IVal v2') -&gt; pure (BVal (v1' &lt; v2')) _ -&gt; Nothingeval (And e1 e2) = do v1 &lt;- eval e1 v2 &lt;- eval e2 case (v1,v2) of (BVal v1', BVal v2') -&gt; pure (BVal (v1' &amp;&amp; v2')) _ -&gt; Nothingeval (If ec et ee) = do v1 &lt;- eval ec case v1 of (BVal True) -&gt; eval et (BVal False) -&gt; eval ee GADTs Generalised Algebraic Datatypes (GADTs) is an extension to Haskell that, among other things, allows data types to be specified by writing the types of their constructors. 1234567{-# LANGUAGE GADTs, KindSignatures #-}-- Unary natural numbers, e.g. 3 is S (S (S Z))data Nat = Z | S Nat-- is the same asdata Nat :: * whereZ :: NatS :: Nat -&gt; Nat When combined with the type indexing trick of phantom types, this becomes very powerful! Typed Evaluator There is now only one set of precisely-typed constructors. 12345678910111213141516{-# LANGUAGE GADTs, KindSignatures #-}data Expr :: * -&gt; * where BConst :: Bool -&gt; Expr Bool IConst :: Int -&gt; Expr Int Times :: Expr Int -&gt; Expr Int -&gt; Expr Int Less :: Expr Int -&gt; Expr Int -&gt; Expr Bool And :: Expr Bool -&gt; Expr Bool -&gt; Expr Bool If :: Expr Bool -&gt; Expr a -&gt; Expr a -&gt; Expr aeval :: Expr t -&gt; teval (IConst i) = ieval (BConst b) = beval (Times e1 e2) = eval e1 * eval e2eval (Less e1 e2) = eval e1 &lt; eval e2eval (And e1 e2) = eval e1 &amp;&amp; eval e2eval (If ec et ee) = if eval ec then eval et else eval ee ListsWe could define our own list type using GADT syntax as follows: 123456data List (a :: *) :: * whereNil :: List aCons :: a -&gt; List a -&gt; List a-- head (hd) and tail (tl) functions are partial hd (Cons x xs) = xtl (Cons x xs) = xs We will constrain the domain of these functions by tracking the length of the list on the type level. Vectors123456789101112131415161718192021222324252627282930313233{-# LANGUAGE GADTs, KindSignatures #-}{-# LANGUAGE DataKinds, StandaloneDeriving, TypeFamilies #-}data Nat = Z | S Natplus :: Nat -&gt; Nat -&gt; Nat plus Z n = nplus (S m) n = S (plus m n)type family Plus (m :: Nat) (n :: Nat) :: Nat where Plus Z n = n Plus (S m) n = S (Plus m n)data Vec (a :: *) :: Nat -&gt; * where Nil :: Vec a Z Cons :: a -&gt; Vec a n -&gt; Vec a (S n)deriving instance Show a =&gt; Show (Vec a n)appendV :: Vec a m -&gt; Vec a n -&gt; Vec a (Plus m n)appendV Nil ys = ysappendV (Cons x xs) ys = Cons x (appendV xs ys)-- 0: Z-- 1: S Z-- 2: S (S Z)hd :: Vec a (S n) -&gt; ahd (Cons x xs) = xmapVec :: (a -&gt; b) -&gt; Vec a n -&gt; Vec b nmapVec f Nil = NilmapVec f (Cons x xs) = Cons (f x) (mapVec f xs) TradeoffsThe benefits of this extra static checking are obvious, however: It can be difficult to convince the Haskell type checker that your code is correct, even when it is. Type-level encodings can make types more verbose and programs harder to understand. Sometimes excessively detailed types can make type-checking very slow, hindering productivity Pragmatism We should use type-based encodings only when the assurance advantages outweigh the clarity disadvantages. The typical use case for these richly-typed structures is to eliminate partial functions from our code base. If we never use partial list functions, length-indexed vectors are not particularly useful Theory of TypesLogic We can specify a logical system as a deductive system by providing a set of rules and axioms that describe how to prove various connectives. Natural Deduction A way we can specify logic Each connective typically has introduction and elimination rules. For example, to prove an implication A → B holds, we must show that B holds assuming A. This introduction rule is written as: More rulesImplication also has an elimination rule, that is also called modus ponens:$$\\frac{\\ulcorner \\vdash A\\rightarrow B \\space\\space\\space\\space\\space\\space\\space\\space\\ulcorner \\vdash A}{\\ulcorner \\vdash B}\\rightarrow-E$$Conjunction (and) has an introduction rule that follows our intuition:$$\\frac{\\ulcorner \\vdash A\\space\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner \\vdash B}{\\ulcorner \\vdash A\\land B}\\land-I_1$$It has two elimination rules:$$\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash A}\\land-E_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A\\land B}{\\ulcorner \\vdash B}\\land-E_2$$Disjunction (or) has two introduction rules:$$\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_1\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner \\vdash A}{\\ulcorner \\vdash A\\lor B}\\land-I_2$$Disjunction elimination is a little unusual:$$\\frac{\\ulcorner \\vdash A\\lor B\\space\\space\\space\\space\\space\\space\\space A,\\ulcorner \\vdash P \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner \\vdash P}{\\ulcorner \\vdash P}\\lor-E$$The true literal, written T, has only an introduction:$$\\frac{}{\\ulcorner \\vdash \\top}$$And false, written ⊥, has just elimination (ex falso quodlibet):$$\\frac{\\ulcorner \\vdash \\bot}{\\ulcorner \\vdash P}$$Typically we just define：$$\\neg A \\equiv(A\\rightarrow\\bot)$$ Constructive LogicThe logic we have expressed so far does not admit the law of the excluded middle:$$P\\lor\\neg P$$Or the equivalent double negation elimination:$$(\\neg\\neg P)\\rightarrow P$$This is because it is a constructive logic that does not allow us to do proof by contradiction. Typed Lambda CalculusBoiling Haskell DownThe theoretical properties we will describe also apply to Haskell, but we need a smaller language for demonstration purposes. No user-defined types, just a small set of built-in types. No polymorphism (type variables) Just lambdas (λx.e) to define functions or bind variables. This language is a very minimal functional language, called the simply typed lambda calculus, originally due to Alonzo Church. Our small set of built-in types are intended to be enough to express most of the data types we would otherwise define. We are going to use logical inference rules to specify how expressions are given types (typing rules). Function TypesWe create values of a function type A → B using lambda expressions:$$\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$The typing rule for function application is as follows:$$\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2:: B}$$ Composite Data TypesIn addition to functions, most programming languages feature ways to compose types together to produce new types, such as: Classes, Tuples, Structs, Unions, Records… Product TypesFor simply typed lambda calculus, we will accomplish this with tuples, also called product types. (A, B) We won’t have type declarations, named fields or anything like that. More than two values can be combined by nesting products, for example a three dimensional vector:$$\\text{(Int, (Int, Int))}$$ Constructors and EliminatorsWe can construct a product type the same as Haskell tuples:$$\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}$$The only way to extract each component of the product is to use the fst and snd eliminators:$$\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}$$ Unit TypesCurrently, we have no way to express a type with just one value. This may seem useless at first, but it becomes useful in combination with other types. We’ll introduce the unit type from Haskell, written (), which has exactly one inhabitant, also written ():$$\\frac{}{\\ulcorner\\vdash ():: ()}$$ Disjunctive CompositionWe can’t, with the types we have, express a type with exactly three values. 1data TrafficLight = Red | Amber | Green In general we want to express data that can be one of multiple alternatives, that contain different bits of data. 12345type Length = Inttype Angle = Intdata Shape = Rect Length Length | Circle Length | Point | Triangle Angle Length Length Sum TypesWe’ll build in the Haskell Either type to express the possibility that data may be one of two forms.$$\\text{Either } A \\space B$$These types are also called sum types. Our TrafficLight type can be expressed (grotesquely) as a sum of units:$$\\text{TrafficLight } \\simeq \\text{Either () (Either () ())}$$ Constructors and Eliminators for SumsTo make a value of type Either A B, we invoke one of the two constructors:$$\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}$$We can branch based on which alternative is used using pattern matching:$$\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}$$Example Our traffic light type has three values as required:$$\\begin{align*}\\text{TrafficLight } &amp;\\simeq \\text{Either () (Either () ())}\\\\\\text{Red } &amp;\\simeq \\text{Left ()}\\\\\\text{Amber } &amp;\\simeq \\text{Right (Left ())}\\\\\\text{Green } &amp;\\simeq \\text{Right (Right (Left ()}\\\\\\end{align*}$$ The Empty TypeWe add another type, called Void, that has no inhabitants. Because it is empty, there is no way to construct it. We do have a way to eliminate it, however:$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}$$If I have a variable of the empty type in scope, we must be looking at an expression that will never be evaluated. Therefore, we can assign any type we like to this expression, because it will never be executed. Gathering Rules$$\\frac{\\ulcorner\\vdash e::\\text{Void}}{\\ulcorner\\vdash \\text{absurd }e::P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ():: ()}\\\\\\frac{\\ulcorner\\vdash e:: A}{\\ulcorner\\vdash \\text{Left }e::\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash e:: B}{\\ulcorner\\vdash \\text{Right }e::\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash e::\\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space x::A,\\ulcorner\\vdash e_1::P\\space \\space\\space\\space\\space\\space\\space\\space y::B,\\ulcorner\\vdash e_2::P}{\\ulcorner\\vdash (\\textbf{case }e\\textbf{ of}\\text{ Left x}\\rightarrow e_1; \\text{ Right y}\\rightarrow e_2)::P}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash e_2::B}{\\ulcorner\\vdash(e_1, e_2):: (A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{fst }e::A}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash e:: (A,B)}{\\ulcorner\\vdash \\text{snd }e::B}\\\\\\frac{\\ulcorner\\vdash e_1:: A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash e_2::A}{\\ulcorner\\vdash e_1 e_2::B}\\space\\space\\space\\space\\space\\space\\space\\frac{x :: A, \\ulcorner\\vdash e::B}{\\ulcorner\\vdash\\lambda x.e::A\\rightarrow B}$$ Removing Terms. . .$$\\frac{\\ulcorner\\vdash\\text{Void}}{\\ulcorner\\vdash P}\\space\\space\\space\\space\\space\\space\\frac{}{\\ulcorner\\vdash ()}\\\\\\frac{\\ulcorner\\vdash A}{\\ulcorner\\vdash\\text{Either }A\\space B} \\space\\space\\space\\space \\space\\space\\space\\space \\frac{\\ulcorner\\vdash B}{\\ulcorner\\vdash\\text{Either }A\\space B}\\\\\\frac{\\ulcorner\\vdash \\text{Either }A\\space B\\space\\space\\space \\space \\space\\space\\space\\space A,\\ulcorner\\vdash P\\space \\space\\space\\space\\space\\space\\space\\space B,\\ulcorner\\vdash P}{\\ulcorner\\vdash P}\\\\\\frac{\\ulcorner\\vdash A\\space\\space\\space\\space\\space\\space \\ulcorner\\vdash B}{\\ulcorner\\vdash(A, B)}\\space\\space\\space\\space\\space\\space\\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash A}\\space\\space\\space\\space\\space\\space \\frac{\\ulcorner\\vdash (A,B)}{\\ulcorner\\vdash B}\\\\\\frac{\\ulcorner\\vdash A\\rightarrow B\\space\\space\\space\\space\\space\\space\\ulcorner\\vdash A}{\\ulcorner\\vdash B}\\space\\space\\space\\space\\space\\space\\space\\frac{ A, \\ulcorner\\vdash B}{\\ulcorner\\vdash A\\rightarrow B}$$ This looks exactly like constructive logic! If we can construct a program of a certain type, we have also created a proof of a program The Curry-Howard CorrespondenceThis correspondence goes by many names, but is usually attributed to Haskell Curry and William Howard. It is a very deep result: Programming Logic Types Propositions Programs Proofs Evaluation Proof Simplification It turns out, no matter what logic you want to define, there is always a corresponding λ-calculus, and vice versa. λ-calculus Logic Typed λ-Calculus Constructive Logic Continuations Classical Logic Monads Modal Logic Linear Types, Session Types Linear Logic Region Types Separation Logic TranslatingWe can translate logical connectives to types and back: Types logical connectives Tuples Conjuction($\\land$) Either Disjunction (∨) Functions Implication () True Void False We can also translate our equational reasoning on programs into proof simplification on proofs! Proof SimplificationAssuming A $∧$ B, we want to prove B $∧$ A. We have this unpleasant proof: Translating to types, we get: Assuming x :: (A, B), we want to construct (B, A). We know that$$\\text{(snd x, snd (fst x, fst x)) = (snd x, fst x)}$$Assuming x :: (A, B), we want to construct (B, A). Back to logic: ApplicationsAs mentioned before, in dependently typed languages such as Agda and Idris, the distinction between value-level and type-level languages is removed, allowing us to refer to our program in types (i.e. propositions) and then construct programs of those types (i.e. proofs). Generally, dependent types allow us to use rich types not just for programming, but also for verification via the Curry-Howard correspondence. CaveatsAll functions we define have to be total and terminating. Otherwise we get an inconsistent logic that lets us prove false things. Most common calculi correspond to constructive logic, not classical ones, so principles like the law of excluded middle or double negation elimination do not hold. Algebraic Type IsomorphismSemiring Structure These types we have defined form an algebraic structure called a commutative semiring Laws for Either and Void: Associativity: Either (Either A B) C $\\simeq$ Either A (Either B C) Identity: Either Void A $\\simeq$ A Commutativity: Either A B $\\simeq$ Either B A Laws for tuples and 1: Associativity: ((A, B), C) $\\simeq$ (A,(B, C)) Identity: ((), A) $\\simeq$ A Commutativity: (A, B) $\\simeq$ (B, A) Combining the two: Distributivity: (A, Either B C) $\\simeq$ Either (A, B) (A, C) Absorption: (Void, A) $\\simeq$ Void What does $\\simeq$ mean here? It’s more than logical equivalence. Distinguish more things. Isomorphism Two types A and B are isomorphic, written A $\\simeq$ B, if there exists a bijection between them. This means that for each value in A we can find a unique value in B and vice versa. Example: 12data Switch = On Name Int | Off Name Can be simplified to the isomorphic (Name, Maybe Int). Generic Programming Representing data types generically as sums and products is the foundation for generic programming libraries such as GHC generics. This allows us to define algorithms that work on arbitrary data structures. Polymorphism and ParametrictyType QuantifiersConsider the type of fst: 1fst :: (a,b) -&gt; a This can be written more verbosely as: 1fst :: forall a b. (a,b) -&gt; a Or, in a more mathematical notation:$$\\text{fst}::\\forall a\\space b(a, b)\\rightarrow a$$This kind of quantification over type variables is called parametric polymorphism or just polymorphism for short. (It’s also called generics in some languages, but this terminology is bad) Curry-HowardThe type quantifier ∀ corresponds to a universal quantifier ∀, but it is not the same as the ∀ from first-order logic. What’s the difference? First-order logic quantifiers range over a set of individuals or values, for example the natural numbers:$$\\forall x. x+1 &gt;x$$These quantifiers range over propositions (types) themselves. It is analogous to second-order logic, not first-order:$$\\forall A.\\forall B\\space A\\land B \\rightarrow B\\land A\\\\\\forall A.\\forall B\\space (A, B) \\rightarrow (B, A)$$The first-order quantifier has a type-theoretic analogue too (type indices), but this is not nearly as common as polymorphism. Generality A type A is more general than a type B, often written A $\\sqsubseteq$ B, if type variables in A can be instantiated to give the type B. If we need a function of type Int → Int, a polymorphic function of type ∀a. a → a will do just fine, we can just instantiate the type variable to Int. But the reverse is not true. This gives rise to an ordering. Example$$\\text{Int}\\rightarrow\\text{Int} \\sqsupseteq \\forall z. z\\rightarrow z\\sqsupseteq \\forall x\\space y. x\\rightarrow y\\sqsupseteq\\forall a. a$$ Constraining ImplementationsHow many possible total, terminating implementations are there of a function of the following type? Many$$\\text{Int}\\rightarrow\\text{Int}$$How about this type? 1$$\\forall a. a\\rightarrow a$$ Polymorphic type signatures constrain implementations. Parametricity The principle of parametricity states that the result of polymorphic functions cannot depend on values of an abstracted type. More formally, suppose I have a polymorphic function g that is polymorphic on type a. If run any arbitrary function f :: a → a on all the a values in the input of g, that will give the same results as running g first, then f on all the a values of the output. Example:$$foo :: ∀a. [a] → [a]$$We know that every element of the output occurs in the input. The parametricity theorem we get is, for all f :$$\\text{foo }\\circ(\\text{map f}) = (\\text{map f})\\circ \\text{foo}$$ $$head :: ∀a. [a] → a\\\\\\text{ f (head l) = head (map f l)}$$ $$(++) :: ∀a. [a] → [a] → [a]\\\\\\text{map f (a ++ b) = map f a ++ map f b}$$ $$concat :: ∀a. [[a]] → [a]\\\\\\text{map f (concat ls) = concat (map (map f ) ls)}$$ Higher Order Functions$$filter :: ∀a. (a → Bool) → [a] → [a]\\\\\\text{filter p (map f ls) = map f (filter (p ◦ f ) ls)}$$ Parametricity TheoremsFollow a similar structure. In fact it can be mechanically derived, using the relational parametricity framework invented by John C. Reynolds, and popularised by Wadler in the famous paper, “Theorems for Free!”1 . Upshot: We can ask lambdabot on the Haskell IRC channel for these theorems.","link":"/2020/08/01/Haskell/"}],"tags":[{"name":"Concurrency","slug":"Concurrency","link":"/tags/Concurrency/"},{"name":"spin","slug":"spin","link":"/tags/spin/"},{"name":"concurrency","slug":"concurrency","link":"/tags/concurrency/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Haskell","slug":"Haskell","link":"/tags/Haskell/"},{"name":"Functional Programming","slug":"Functional-Programming","link":"/tags/Functional-Programming/"}],"categories":[{"name":"notes","slug":"notes","link":"/categories/notes/"},{"name":"教程","slug":"教程","link":"/categories/%E6%95%99%E7%A8%8B/"}]}